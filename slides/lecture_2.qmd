---
title: "Statistical Models"
subtitle: "Lecture 2"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Lecture 2: <br>Random samples<br>{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Outline of Lecture 2

1. Probability revision III




# Part 1: <br>Probability revision III {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Probability revision III {.smaller}

- You are expected to be familiar with the main concepts from Y1 module    
**Introduction to Probability & Statistics**

- Self-contained revision material available in <a href="appendix_A.qmd">Appendix A</a> 


**Topics to review**: Sections 6--7 of <a href="appendix_A.qmd">Appendix A</a>
    
::: {.column width="38%"}
    
- Independence of random variables
- Covariance and correlation

:::

::: {.column width="58%"}


:::






## Independence of random variables  {.smaller}



::: Definition
### Independence
$(X,Y)$ random vector with joint pdf or pmf $f_{X,Y}$ and marginal pdfs or pmfs $f_X,f_Y$. We say that $X$ and $Y$ are **independent** random variables if
$$
f_{X,Y}(x,y)  =  f_X(x)f_Y(y) \,, \quad \forall \, (x,y) \in \R^2 
$$

:::




## Independence of random variables {.smaller}
### Conditional distributions and probabilities


If $X$ and $Y$ are **independent** then $X$ gives no information on $Y$ (and vice-versa):

- Conditional distribution: $Y|X$ is same as $Y$
$$
f(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)} = \frac{f_X(x)f_Y(y)}{f_X(x)} = f_Y(y)
$$

- Conditional probabilities: From the above we also obtain
\begin{align*}
P(Y \in A | x) & = \sum_{y \in A} f(y|x) = \sum_{y \in A} f_Y(y) = P(Y \in A)  & \, \text{ discrete rv} \\
P(Y \in A | x) & = \int_{y \in A} f(y|x) \, dy = \int_{y \in A} f_Y(y) \, dy = P(Y \in A)  &  \, \text{ continuous rv}
\end{align*}





## Independence of random variables {.smaller}
### Characterization of independence - Densities

::: Theorem

$(X,Y)$ random vector with joint pdf or pmf $f_{X,Y}$. They are equivalent:

- $X$ and $Y$ are independent random variables
- There exist functions $g(x)$ and $h(y)$ such that
$$
f_{X,Y}(x,y) = g(x)h(y) \,, \quad \forall \, (x,y) \in \R^2
$$

::: 




## Exercise {.smaller}

A student leaves for class between 8 AM and 8:30 AM and takes between 40 and 50 minutes to get there

- Denote by $X$ the time of departure 
    * $X = 0$ corresponds to 8 AM
    * $X = 30$ corresponds to 8:30 AM
- Denote by $Y$ the travel time

- Assume that $X$ and $Y$ are independent and uniformly distributed


**Question:** Find the probability that the student arrives to class before 9 AM





## Solution {.smaller}


- By assumption $X$ is uniform on $(0,30)$. Therefore
$$
f_X(x)  = 
\begin{cases}
\frac{1}{30} & \text{ if } \, x \in (0,30)  \\
0 & \text{ otherwise }
\end{cases}
$$

- By assumption $Y$ is uniform on $(40,50)$. Therefore
$$
f_Y(y) = 
\begin{cases}
\frac{1}{10} & \text{ if } \, y \in (40,50)  \\
0 & \text{ otherwise }
\end{cases}
$$
where we used that $50 - 40 = 10$



## Solution {.smaller}


- Define the rectangle
$$
R = (0,30) \times (40,50)
$$

- Since $X$ and $Y$ are independent, we get

$$
f_{X,Y}(x,y) = f_X(x)f_Y(y) = 
\begin{cases}
\frac{1}{300} & \text{ if } \, (x,y) \in R  \\
0 & \text{ otherwise }
\end{cases}
$$



## Solution {.smaller}

- The arrival time is given by $X + Y$

- Therefore, the student arrives to class before 9 AM iff $X + Y < 60$

- Notice that 
$$
\{X + Y < 60 \}  = \{ (x,y) \in \R^2 \, \colon \, 0 \leq x < 60 - y,  40 \leq y < 50 \} 
$$

## Solution {.smaller}

Therefore, the probability of arriving before 9 AM is

\begin{align*}
P(\text{arrives before 9 AM}) & = P(X + Y < 60) \\
                              & = \int_{\{X+Y < 60\}} f_{X,Y} (x,y) \, dxdy  \\
                              & = \int_{40}^{50} \left( \int_0^{60-y} \frac{1}{300} \, dx 
                              \right) \, dy  \\
                              & = \frac{1}{300}  \int_{40}^{50} (60 - y) \, dy \\
                              & = \frac{1}{300} \ y \left(  60 - \frac{y}{2} \right) \Bigg|_{y=40}^{y=50} \\
                              & = \frac{1}{300} \cdot (1750 - 1600) = \frac12
\end{align*}







## Consequences of independence {.smaller}

::: Theorem

Suppose $X$ and $Y$ are independent random variables. Then

- For any $A,B \subset \R$ we have
$$
P(X \in A, Y \in B) = P(X \in A) P(Y \in B)
$$

- Suppose $g(x)$ is a function of (only) $x$,
$h(y)$ is a function of (only) $y$. Then
$$
\Expect[g(X)h(Y)] = \Expect[g(X)]\Expect[h(Y)]  
$$

::: 





## Application: MGF of sums {.smaller}

::: Theorem

Suppose $X$ and $Y$ are independent random variables and denote by 
$M_X$ and $M_Y$ their MGFs. Then
$$
M_{X + Y} (t) = M_X(t) M_Y(t)
$$

:::


**Proof**: Follows by previous Theorem
\begin{align*}
M_{X + Y} (t) & = \Expect[e^{t(X+Y)}] = \Expect[e^{tX}e^{tY}] \\
              & = \Expect[e^{tX}] \Expect[e^{tY}] \\
              & = M_X(t) M_Y(t)
\end{align*}







## Example - Sum of independent normals {.smaller}

- Suppose $X \sim N (\mu_1, \sigma_1^2)$ and $Y \sim N (\mu_2, \sigma_2^2)$ 
are independent normal random variables

- We have seen in Lecture 1 that for normal distributions
$$
M_X(t) = \exp \left( \mu_1 t + \frac{t^2 \sigma_1^2}{2} \right) \,, \qquad
M_Y(t) = \exp \left( \mu_2 t + \frac{t^2 \sigma_2^2}{2} \right)
$$

- Since $X$ and $Y$ are independent, from previous Theorem we get
\begin{align*}
M_{X+Y}(t) & =  M_{X}(t) M_{Y}(t) =
\exp \left( \mu_1 t + \frac{t^2 \sigma_1^2}{2} \right)
\exp \left( \mu_2 t + \frac{t^2 \sigma_2^2}{2} \right)   \\
& = \exp \left( (\mu_1 + \mu_2) t + \frac{t^2 (\sigma_1^2 + \sigma_2^2)}{2} \right)
\end{align*}





## Example - Sum of independent normals {.smaller}

- Therefore $Z := X + Y$ has moment generating function
$$
M_{Z}(t) = M_{X+Y}(t) = \exp \left( (\mu_1 + \mu_2) t + \frac{t^2 (\sigma_1^2 + \sigma_2^2)}{2} \right)
$$

- The above is the mgf of a **normal distribution** with 
$$
\text{mean }\quad \mu_1 + \mu_2 \quad \text{ and variance} \quad  \sigma_1^2 + \sigma_2^2
$$

- By the Theorem in Slide 68 of Lecture 1 we have
$$
Z \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
$$

- **Sum of independent normals is normal**








## Covariance & Correlation {.smaller}
### Relationship between RV

Given two random variables $X$ and $Y$ we said that

- $X$ and $Y$ are **independent** if
$$
f_{X,Y}(x,y) = f_X(x) g_Y(y)
$$

- In this case there is no relationship between $X$ and $Y$

- This is reflected in the conditional distributions:
$$
X|Y \sim X \qquad \qquad Y|X \sim Y
$$





## Covariance & Correlation {.smaller}
### Relationship between RV


If $X$ and $Y$ are **not independent** then there is a **relationship** between them

::: Question 

How do we measure the strength of such dependence?

:::


**Answer**: By introducing the notions of

- Covariance
- Correlation




## Covariance {.smaller}
### Definition


**Notation**: Given two rv $X$ and $Y$ we denote
\begin{align*}
& \mu_X := \Expect[X]   \qquad & \mu_Y & := \Expect[Y] \\
& \sigma^2_X := \Var[X] \qquad  & \sigma^2_Y & := \Var[Y] 
\end{align*}


::: Definition 

The **covariance** of $X$ and $Y$ is the number
$$
\Cov(X,Y) := \Expect[  (X - \mu_X) (Y - \mu_Y)  ]
$$

:::




## Covariance {.smaller}
### Alternative Formula

::: Theorem 

The covariance of $X$ and $Y$ can be computed via
$$
\Cov(X,Y) = \Expect[XY] - \Expect[X]\Expect[Y]
$$

:::







## Correlation {.smaller}

**Remark**:

- $\Cov(X,Y)$ encodes only **qualitative** information about the relationship between $X$ and $Y$

- To obtain **quantitative** information we introduce the **correlation**



::: Definition 

The **correlation** of $X$ and $Y$ is the number
$$
\rho_{XY} := \frac{\Cov(X,Y)}{\sigma_X \sigma_Y} 
$$

:::




## Correlation {.smaller}

Correlation detects **linear relationships** between $X$ and $Y$

::: Theorem

For any random variables $X$ and $Y$ we have

- $- 1\leq \rho_{XY} \leq 1$
- $|\rho_{XY}|=1$ if and only if there exist $a,b \in \R$ such that
$Y = aX + b$
    * If $\rho_{XY}=1$ then $a>0$  $\qquad \qquad \quad$ (positive linear correlation)
    * If $\rho_{XY}=-1$ then $a<0$ $\qquad \qquad$ (negative linear correlation)

:::


**Proof**: Omitted, see page 172 of [@casella-berger]





## Correlation \& Covariance {.smaller}
### Independent random variables


::: Theorem

If $X$ and $Y$ are independent random variables then
$$
\Cov(X,Y) = 0 \,, \qquad \rho_{XY}=0
$$

:::


**Proof**: 

- If $X$ and $Y$ are independent then $\Expect[XY]=\Expect[X]\Expect[Y]$
- Therefore $\Cov(X,Y)= \Expect[XY]-\Expect[X]\Expect[Y] = 0$
- Moreover $\rho_{XY}=0$ by definition







## Formula for Variance {.smaller}
### Variance is quadratic

::: Theorem

For any two random variables $X$ and $Y$ and $a,b \in \R$
$$
\Var[aX + bY] = a^2 \Var[X] + b^2 \Var[Y] + 2 \Cov(X,Y)
$$
If $X$ and $Y$ are independent then
$$
\Var[aX + bY] = a^2 \Var[X] + b^2 \Var[Y]
$$

:::


**Proof**: Exercise









# Part 5: <br>Multivariate random vectors {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Multivariate Random Vectors {.smaller}
### Recall

- A **Random vector** is a function 
$$
\XX \colon \Omega \to \R^n
$$
- $\XX$ is a **multivariate** random vector if $n \geq 3$
- We denote the components of $\XX$ by
$$
\XX = (X_1,\ldots,X_n) \,, \qquad X_i \colon \Omega \to \R
$$
- We denote the components of a point $\xx \in \R^n$ by
$$
\xx = (x_1,\ldots,x_n)
$$



## Discrete and Continuous Multivariate Random Vectors {.smaller}

Everything we defined for **bivariate** vectors extends to **multivariate** vectors

::: Definition

The random vector $\XX \colon \Omega \to \R^n$ is:

- **continuous** if components $X_i$s are continuous
- **discrete** if components $X_i$ are discrete

:::






## Joint pmf {.smaller}

::: Definition 

The **joint pmf** of a continuous random vector $\XX$ is $f_{\XX} \colon \R^n \to \R$ defined by
$$
f_{\XX} (\xx) = f_{\XX}(x_1,\ldots,x_n) := P(X_1 = x_1 , \ldots , X_n = x_n ) \,, \qquad \forall \, \xx \in \R^n
$$

:::

**Note:** For all $A \subset \R^n$ it holds
$$
P(\XX \in A) = \sum_{\xx \in A} f_{\XX}(\xx)
$$






## Joint pdf {.smaller}

::: Definition 

The **joint pdf** of a continuous random vector $\XX$ is a function $f_{\XX} \colon \R^n \to \R$ such that
$$
P (\XX \in A) := \int_A f_{\XX}(x_1 ,\ldots, x_n) \, dx_1 \ldots dx_n = \int_{A} f_{\XX}(\xx) \, d\xx \,, \quad \forall \, A \subset \R^n
$$

:::

**Note**: $\int_A$ denotes an $n$-fold intergral over all points $\xx \in A$





## Expected Value {.smaller}

::: Definition 

$\XX \colon \Omega \to \R^n$ random vector and $g \colon \R^n \to \R$ function. The **expected value** of the random variable $g(X)$ is
\begin{align*}
\Expect[g(\XX)] & := \sum_{x \in \R^n} g(\xx) f_{\XX} (\xx)  \qquad & (\XX \text{ discrete}) \\
\Expect[g(\XX)] & := \int_{\R^n} g(\xx) f_{\XX} (\xx) \, d\xx \qquad & \qquad  (\XX \text{ continuous})
\end{align*}


:::





## Marginal distributions {.smaller}


- **Marginal pmf or pdf** of any **subset** of the coordinates $(X_1,\ldots,X_n)$ can be computed by integrating or summing the remaining coordinates

- To ease notations, we only define maginals wrt the first $k$ coordinates



::: Definition 

The **marginal pmf** or **marginal pdf** of the random vector $\XX$ with 
respect to the first $k$ coordinates is the function $f \colon \R^k \to \R$ defined by
\begin{align*}
f(x_1,\ldots,x_k) & := \sum_{ (x_{k+1}, \ldots, x_n) \in \R^{n-k} }  
f_{\XX} (x_1 , \ldots , x_n)  \quad & (\XX \text{ discrete}) \\
f(x_1,\ldots,x_k) & := \int_{\R^{n-k}}f_{\XX} (x_1 , \ldots, x_n ) \, dx_{k+1} \ldots dx_{n} \quad & \quad  (\XX \text{ continuous})
\end{align*}


:::





## Marginal distributions {.smaller}


- We use a special notation for **marginal pmf or pdf** wrt a single coordinate


::: Definition 

The **marginal pmf** or **marginal pdf** of the random vector $\XX$ with 
respect to the $i$-th coordinate is the function $f_{X_i} \colon \R \to \R$ defined by
\begin{align*}
f_{X_i}(x_i) & := \sum_{ \tilde{x} \in \R^{n-1} }  
f_{\XX} (x_1, \ldots, x_n)  \quad & (\XX \text{ discrete}) \\
f_{X_i}(x_i) & := \int_{\R^{n-1}}f_{\XX} (x_1, \ldots, x_n) \, d\tilde{x} \quad & \quad  (\XX \text{ continuous})
\end{align*}
where $\tilde{x} \in \R^{n-1}$ denotes the vector $\xx$ with $i$-th component removed
$$
\tilde{x} := (x_1, \ldots, x_{i-1}, x_{i+1},\ldots, x_n)
$$

:::




## Conditional distributions {.smaller}

We now define conditional distributions given the first $k$ coordinates


::: Definition 

Let $\XX$ be a random vector and suppose that the marginal pmf or pdf wrt the first $k$ coordinates satisfies 
$$
f(x_1,\ldots,x_k) > 0 \,, \quad \forall \, (x_1,\ldots,x_k ) \in \R^k
$$ 
The **conditional pmf or pdf** of $(X_{k+1},\ldots,X_n)$ given $X_1 = x_1, \ldots , X_k = x_k$ is the function of $(x_{k+1},\ldots,x_{n})$ defined by
$$
f(x_{k+1},\ldots,x_n | x_1 , \ldots , x_k) := \frac{f_{\XX}(x_1,\ldots,x_n)}{f(x_1,\ldots,x_k)}
$$

:::



## Conditional distributions {.smaller}

Similarly, we can define the conditional distribution given the $i$-th coordinate


::: Definition 

Let $\XX$ be a random vector and suppose that for a given $x_i \in \R$
$$
f_{X_i}(x_i) > 0
$$ 
The **conditional pmf or pdf** of $\tilde{X}$ given $X_i = x_i$ is the function of $\tilde{x}$ defined by
$$
f(\tilde{x} | x_i ) := \frac{f_{\XX}(x_1,\ldots,x_n)}{f_{X_i}(x_i)}
$$
where we denote
$$
\tilde{X} := (X_1, \ldots, X_{i-1}, X_{i+1},\ldots, X_n) \,, \quad 
\tilde{x} := (x_1, \ldots, x_{i-1}, x_{i+1},\ldots, x_n)
$$

:::





## Independence {.smaller}


::: Definition

$\XX=(X_1,\ldots,X_n)$ random vector with joint pmf or pdf $f_{\XX}$ and marginals $f_{X_i}$. We say that the random variables $X_1,\ldots,X_n$ are **mutually independent** if
$$
f_{\XX}(x_1,\ldots,x_n) = f_{X_1}(x_1) \cdot \ldots \cdot f_{X_n}(x_n) = \prod_{i=1}^n f_{X_i}(x_i)
$$

:::


::: Proposition

If $X_1,\ldots,X_n$ are mutually independent then for all $A_i \subset \R$
$$
P(X_1 \in A_1 , \ldots , X_n \in A_n) = \prod_{i=1}^n P(X_i \in A_i)
$$

:::




## Independence {.smaller}
### Characterization result

::: Theorem

$\XX=(X_1,\ldots,X_n)$ random vector with joint pmf or pdf $f_{\XX}$. They are equivalent:

- The random variables $X_1,\ldots,X_n$ are **mutually independent**
- There exist functions $g_i(x_i)$ such that
$$
f_{\XX}(x_1,\ldots,x_n) =  \prod_{i=1}^n g_{i}(x_i)
$$

:::





## Independence {.smaller}
### Expectation of product


::: Theorem

$X_1,\ldots,X_n$ be mutually independent random variables and $g_i(x_i)$ functions. Then
$$
\Expect[ g_1(X_1) \cdot \ldots \cdot g_n(X_n) ] = \prod_{i=1}^n \Expect[g_i(X_i)]
$$

:::



## Independence {.smaller}
### A very useful theorem


::: Theorem

$X_1,\ldots,X_n$ be mutually independent random variables and $g_i(x_i)$ function only of $x_i$. Then the random variables
$$
g_1(X_1) \,, \ldots \,, g_n(X_n)
$$
are mutually independent

:::

**Proof**: Omitted. See [@casella-berger] page 184

**Example**: $X_1,\ldots,X_n \,$ independent $\,\, \implies \,\, X_1^2, \ldots, X_n^2 \,$ independent






## Independence {.smaller}
### MGF of sum

::: Theorem

$X_1,\ldots,X_n$ be mutually independent random variables with mgfs $M_{X_1}(t),\ldots, M_{X_n}(t)$. Define the random variable
$$
Z := X_1 + \ldots + X_n
$$
The mgf of $Z$ satisfies
$$
M_Z(t) = \prod_{i=1}^n M_{X_i}(t)
$$

:::



## Sum of independent Normals {.smaller}


::: Theorem

Let $X_1,\ldots,X_n$ be mutually independent random variables with normal distribution $X_i \sim N (\mu_i,\sigma_i^2)$. Define
$$
Z := X_1 + \ldots + X_n
$$
and
$$
\mu :=\mu_1 + \ldots + \mu_n    \,, \quad \sigma^2 := \sigma_1^2 + \ldots + \sigma_n^2
$$
Then $Z$ is normally distributed with
$$
Z \sim N(\mu,\sigma^2)
$$

:::





## Sum of independent Normals {.smaller}
### Proof of Theorem


- We have seen in Slide 119 in Lecture 1 that if $X_i \sim N(\mu_i,\sigma_i^2)$ then
$$
M_{X_i}(t) = \exp \left( \mu_i t + \frac{t^2 \sigma_i^2}{2} \right)
$$

- Since $X_1,\ldots,X_n$ are mutually independent, from previous Theorem we get
\begin{align*}
M_{Z}(t) & = \prod_{i=1}^n M_{X_i}(t) = 
\prod_{i=1}^n \exp \left( \mu_i t + \frac{t^2 \sigma_i^2}{2} \right) \\
& = \exp \left( (\mu_1 + \ldots + \mu_n) t + \frac{t^2 (\sigma_1^2 + \ldots +\sigma_n^2)}{2} \right) \\
& =  \exp \left( \mu t + \frac{t^2 \sigma^2 }{2} \right) 
\end{align*}


## Sum of independent Normals {.smaller}
### Proof of Theorem

- Therefore $Z$ has moment generating function
$$
M_{Z}(t) = \exp \left( \mu t + \frac{t^2 \sigma^2 }{2} \right) 
$$

- The above is the mgf of a **normal distribution** with 
$$
\text{mean }\quad \mu  \quad \text{ and variance} \quad  \sigma^2 
$$

- Since mgfs characterize distributions (see Theorem in Slide 132 of Lecture 1), we conclude
$$
Z \sim N(\mu, \sigma^2 )
$$




## Sum of independent Gammas {.smaller}

::: Theorem

Let $X_1,\ldots,X_n$ be mutually independent random variables with Gamma distribution $X_i \sim \Gamma (\alpha_i,\beta)$. Define
$$
Z := X_1 + \ldots + X_n
$$
and
$$
\alpha :=\alpha_1 + \ldots + \alpha_n 
$$
Then $Z$ has Gamma distribution
$$
Z \sim \Gamma(\alpha,\beta)
$$

:::








## Sum of independent Gammas {.smaller}
### Proof of Theorem


- We have seen in Slide 126 in Lecture 1 that if $X_i \sim \Gamma(\alpha_i,\beta)$ then
$$
M_{X_i}(t) = \frac{\beta^{\alpha_i}}{(\beta-t)^{\alpha_i}}
$$

- Since $X_1,\ldots,X_n$ are mutually independent we get
\begin{align*}
M_{Z}(t) & = \prod_{i=1}^n M_{X_i}(t) = 
\prod_{i=1}^n \frac{\beta^{\alpha_i}}{(\beta-t)^{\alpha_i}} \\
& = \frac{\beta^{(\alpha_1 + \ldots + \alpha_n)}}{(\beta-t)^{(\alpha_1 + \ldots + \alpha_n)}} \\
& = \frac{\beta^{\alpha}}{(\beta-t)^{\alpha}} 
\end{align*}




## Sum of independent Gammas {.smaller}
### Proof of Theorem

- Therefore $Z$ has moment generating function
$$
M_{Z}(t) = \frac{\beta^{\alpha}}{(\beta-t)^{\alpha}} 
$$

- The above is the mgf of a **Gamma distribution** with 
$$
\text{mean }\quad \alpha  \quad \text{ and variance} \quad  \beta 
$$

- Since mgfs characterize distributions (see Theorem in Slide 132 of Lecture 1), we conclude
$$
Z \sim \Gamma(\alpha, \beta )
$$








## Expectation of sums {.smaller}
### Expectation is linear


::: Theorem

For random variables $X_1,\ldots,X_n$ and scalars $a_1,\ldots,a_n$ we have
$$
\Expect[a_1X_1 + \ldots + a_nX_n] = a_1 \Expect[X_1] + \ldots + a_n \Expect[X_n]
$$

:::




## Variance of sums {.smaller}
### Variance is quadratic


::: Theorem

For random variables $X_1,\ldots,X_n$ and scalars $a_1,\ldots,a_n$ we have
\begin{align*}
\Var[a_1X_1 + \ldots + a_nX_n] = a_1^2 \Var[X_1] & + \ldots + a^2_n \Var[X_n] \\
                                                 & + 2 \sum_{i \neq j} \Cov(X_i,X_j)
\end{align*}
If $X_1,\ldots,X_n$ are mutually independent then
$$
\Var[a_1X_1 + \ldots + a_nX_n] = a_1^2 \Var[X_1] + \ldots + a^2_n \Var[X_n]
$$

:::




## References


