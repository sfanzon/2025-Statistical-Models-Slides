---
title: "Statistical Models"
subtitle: "Lecture 9"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Lecture 9: <br> Practical regression {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Outline of Lecture 9 {.smaller}


1. Plotting variables in R  
    * Cross-check formal statistical results with graphical analyses
    * Important in practical research work

<br>

2. Coefficient of determination $R^2$
    * $R^2$ measures proportion of variability in the data explained by the model
    * $R^2$ close to $1 \implies$ Model exaplains data well
    * Any $R^2$ larger than $0.3$ is potentially interesting



## Outline of Lecture 9 {.smaller}


3. t-test for simple regression
    * Test the significance of the slope parameter

4. t-test for general regression
    * Test the significance of regression parameters

5. F-test for multiple regression
    * Test the overall significance of the parameters


6. Worked Example: The Longley dataset
    * Classic example of highly collinear dataset

7. Model selection
    * Comparison of nested regression models

8. Examples of model selection





# Part 1: <br>Plotting variables in R {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Plotting variables in R {.smaller}


Interested in relationship between 2 variables

- Want to plot the 2 variables together

- Cross-check the results of a formal statistical analysis

- Very important in real project work



## Example: Stock and Gold prices

::: {style="font-size: 0.75em"}
Dataset with $33$ entries for Stock and Gold price pairs
:::


::: {style="font-size: 0.55em"}
```{r}
#| echo: false
#| layout-ncol: 1

data <- read.table("datasets/L3eg1data.txt")

#Add column labels to data
colnames(data) <- c("Stock Price","Gold Price")

# Output markdown table from data
knitr::kable(
  list(data[1:11,], data[12:22,], data[23:33,]),
  row.names = TRUE,
  format = "html", 
  table.attr = 'class="table simple table-striped table-hover"',
) 


```

:::



## Example: Stock and Gold prices {.smaller}


- The data is stored in a ``.txt`` file

- The file can be downloaded here [stock_gold.txt](datasets/stock_gold.txt)

::: {.column width="54%"}

- The text file looks like this
![](images/stock_gold.png){width=73%}

:::

::: {.column width="44%"}

- Remarks:
    * There is a Header
    * 1st column lists *Stock Price*
    * 2nd column lists *Gold Price*

:::



## Reading data into R {.smaller}


To read ``stock_gold.txt`` into R proceed as follows:

1. Download [stock_gold.txt](datasets/stock_gold.txt) and move file to Desktop

2. Open the R Console and change working directory to **Desktop**

```r
# In MacOS type
setwd("~/Desktop")

# In Windows type
setwd("C:/Users/YourUsername/Desktop")
```


## Reading data into R {.smaller}

3. Read ``stock_gold.txt`` into R and store it in data-frame ``prices`` with code

```r
prices = read.table(file = "stock_gold.txt",
                    header = TRUE)
```

<br>

**Note:** We are telling ``read.table()`` that

- ``stock_gold.txt`` has a header
- Headers are *optional*
- Headers are good practice to describe data



## Reading data into R {.smaller}


4. Double check that we loaded the correct data file 


```r
print(prices)
```

```{r}
prices = read.table(file = "datasets/stock_gold.txt",
                    header = TRUE)

print(prices)
```




## Store data into vectors {.smaller}

- We now store Stock and Gold prices in 2 vectors
    * Stock prices are in 1st column of ``prices``
    * Gold prices are in 2nd column of ``prices``

```r
stock.price <- prices[ , 1]
gold.price <- prices[ , 2]
```

<br>

- Alternatively the same can be achieved with

```r
stock.price <- prices$stock_price
gold.price <- prices$gold_price
```



## Plot Stock Price vs Gold Price {.smaller}

::::: {.columns style='display: flex !important; height: 80%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}



```r
plot(stock.price, 
     gold.price, 
     xlab = "Stock Price", 
     ylab = "Gold Price",
     pch = 16
    )
```

:::

::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Create the plot
plot(stock.price,
     gold.price, 
     xlab = "", 
     ylab= "",
     pch = 16)

# Add labels
mtext("Stock Price", side=1, line=3, cex=2.1)
mtext("Gold Price", side=2, line=2.5, cex=2.1)
```

:::
:::::




## Plot Stock Price vs Gold Price {.smaller}

::::: {.columns style='display: flex !important; height: 80%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

- ``xlab`` and ``ylab`` specify axes labels

- ``pch`` specifies type of points

- Scaling is achieved with
    * ``xlim = c(lower, upper)``
    * ``ylim = c(lower, upper)``


:::


::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Create the plot
plot(stock.price,
     gold.price, 
     xlab = "", 
     ylab= "",
     pch = 16)

# Add labels
mtext("Stock Price", side=1, line=3, cex=2.1)
mtext("Gold Price", side=2, line=2.5, cex=2.1)
```

:::
:::::



## Examining the graph {.smaller}

::::: {.columns style='display: flex !important; height: 80%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

- Graph suggests that the 2 variables are negatively correlated

- Need to cross-check with the results of a formal statistical regression analysis


:::


::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Create the plot
plot(stock.price,
     gold.price, 
     xlab = "", 
     ylab= "",
     pch = 16)

# Add labels
mtext("Stock Price", side=1, line=3, cex=2.1)
mtext("Gold Price", side=2, line=2.5, cex=2.1)
```

:::
:::::




# Part 2: <br>Coefficient of <br> determination $R^2${background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Coefficient of determination $R^2$ {.smaller}

- $R^2$ is defined as

$$
 R^2  = 1 - \frac{ \RSS }{ \TSS }
$$

- $R^2$ measures proportion of variability in the data explained by the model

- Recall that $R^2 \leq 1$

::: Important

- $R^2$ is automatically computed by R when using ``lm``
- **High** values of $R^2$ are **better**! (that is, values of $R^2$ close to $1$) 

:::




## Some observations about $R^2$ {.smaller}

::: Warning

$R^2$ increases as more $X$ variables are added to a regression model

:::

This is not necessarily good

- One can add lots of variables and make $R^2 \approx 1$

- This way the model explains the data really well
    $$
    y_i \approx \hat y_i \,, \quad \forall \,\, i = 1 , \ldots, n
    $$ 

- Problem: the model will not make good predictions on new data 

- This is known as **overfitting** and it should be avoided  
(more on this later)




## Some observations about $R^2$ {.smaller}

- For multiple regression $R^2$ lies between $0$ and $1$
    * $R^2 = 0$ model explains nothing
    * $R^2 = 1$ model explains everything

- **Warning:** $R^2$ can be negative for general linear regression (no intercept)


- Generally: the higher the value of $R^2$ the better the model
    * Textbook examples often have high values
    $$
    R^2 \geq 0.7
    $$
    * **Example:** In the *Unemployment* example of Lecture 8 we found
    $$
    R^2 = 0.8655401
    $$




## Some observations about $R^2$ {.smaller}



::: Important

In practice values 
$$
R^2 \geq 0.3
$$ 
imply there is a nontrivial amount of variation in the data explained by the model

:::

    
**Example:** In the *Stock Price* Vs *Gold Price* example we have
$$
R^2 = 0.395325
$$ 

- This shows that *Stock Price* affects *Gold Price*
- Since $R^2$ is not too large, also other factors may affect *Gold Price*





## Regression in R {.smaller}

- The basic R command used to run regression is

::: {.r-stack}

``lm(formula)``

:::

<br>

- ``lm`` stands for **linear model**



## Simple linear regression in R {.smaller}


For simple linear regression

$$
Y_i = \alpha + \beta x_i + \e_i
$$

the command is

::: {.r-stack}

``lm(y ~ x)``

:::

<br>

- Symbol ``y ~ x`` reads as *$y$ modelled as function of $x$*

- ``y`` is vector containing the data $y_1, \ldots, y_n$

- ``x`` is vector containing the data $x_1, \ldots, x_n$




## Multiple linear regression in R {.smaller}


For multiple linear regression

$$
Y_i = \beta_1 + \beta_2 \, x_{i2} + \ldots + \beta_p \, x_{ip} + \e_i
$$

the command is 

::: {.r-stack}

``lm (y ~ x2 + x3 + ... + xp)``

:::

<br>

- ``y`` is vector containing the data $y_1, \ldots, y_n$

- ``xj`` is vector containing the data $x_{1j}, \ldots , x_{jp}$ 



## General commands for regression in R {.smaller}

The best way to run regression is

1. Run the regression analysis and store the results in a variable

```r
fit.model <- lm(formula)
```

<br>

2. Use command ``summary`` to read output of regression

```r
summary(fit.model)
```

**Note:** If you are running the code from ``.R`` file you need to print output

```r
print( summary(fit.model) )
```




## Example: Stock and Gold prices {.smaller}

- Stock price is stored in vector 
    * ``stock.price``

- Gold price is stored in vector 
    * ``gold.price``


- We want to fit the simple linear model
$$
\text{gold.price } = \alpha + \beta \, \times ( \text{ stock.price }) + \text{ error}
$$


```r
# Fit simple linear regression model
fit.model <- lm(gold.price ~ stock.price)

# Print result to screen
summary(fit.model)
```


- The full code can be downloaded here [simple_regression.R](codes/simple_regression.R)




##  {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output.png){width=78%}
:::

::::


- There is a lot of information here!

- We will make sense of most of it in this lecture



## Interesting parts of Output {.smaller}



:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut.png){width=79%}
:::

::::
  

- The MLE are under ``estimate``
    * ``(Intercept)`` refers to the coefficient $\hat \alpha \qquad \implies  \qquad  \hat \alpha = 37.917$
    * ``stock.price`` refers to the coefficient $\hat \beta \qquad \implies  \qquad  \hat \beta = - 6.169$




## Interesting parts of Output {.smaller}


:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut.png){width=79%}
:::

::::


- Other quantities of interest are:


| Coefficient $R^2$ | $\texttt{Multiple R-squared:  0.3953}$ |
|:------------------|:----------                             |
|**t-statistic for** ``stock.price`` | $\texttt{stock.price t-value:  -4.502}$ |
| **F-statistic**      |  $\texttt{F-statistic: 20.27}$           |
: {tbl-colwidths="[50,50]"}




## Plotting the regression line {.smaller}

::::: {.columns style='display: flex !important; height: 80%;'}

::: {.column width="43%" style='display: flex; justify-content: center; align-items: center;'}


```r    
# Data stored in stock.price 
# and gold.price
# Plot the data
plot(stock.price, gold.price, 
     xlab = "Stock Price", 
     ylab= "Gold Price",
     pch = 16)

# Model stored in fit.model
# Plot the regression line
abline(fit.model, 
       col = "red", 
       lwd = 3)
```

:::


::: {.column width="48%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                     header = TRUE
                    )

# Store data into vectors
stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit simple linear regression model
fit.model <- lm(gold.price ~ stock.price)

# Plot the data
plot(stock.price,
     gold.price, 
     xlab = "", 
     ylab= "",
     pch = 16)

# Add labels
mtext("Stock Price", side=1, line=3, cex=2.1)
mtext("Gold Price", side=2, line=2.5, cex=2.1)

# Plot the regression line in red
abline(fit.model, col = "red", lwd = 3)
```

:::

:::::




## Conclusion {.smaller}

- We fit a simple linear model to *Stock Price* Vs *Gold Price*

- We obtained the regression line

$$
\Expect[Y | x] = \hat \alpha + \hat \beta x =  37.917 -  6.169 \times x
$$

- The coefficient of determination is

$$
R^2 = 0.395325 \geq 0.3
$$ 

- Hence the linear model explains the data to a reasonable extent:
    * *Stock Price* affects *Gold Price*
    * Since $R^2$ is not too large, also other factors may affect *Gold Price*






## t-test and F-test for regression {.smaller}

- From ``lm`` we also obtained

| **t-statistic for** ``stock.price`` | $\texttt{stock.price t-value:  -4.502}$ |
|:------------------|:----------                             |
| **F-statistic**      |  $\texttt{F-statistic: 20.27}$           |
: {tbl-colwidths="[50,50]"}

<br>

- t-statistic and F-statistic for regression are mathematically **DIFFICULT** topic

- In the next two parts we explain what they mean
    * We however omit mathematical details
    * If interested check out Section 11.3 of [@casella-berger] and Chapter 11 of [@degroot]




# Part 3: <br>t-test for simple regression {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## t-test for simple regression {.smaller}

- Consider the simple linear regression model

$$
Y_i = \alpha + \beta X_i + \e_i
$$


- We have that 

$$
X \,\, \text{ affects } \,\, Y \qquad \iff \qquad
\beta \neq 0
$$

- $\beta$ is a random quantity which depends on the sample

- To see if $X$ affects $Y$, we can test the hypothesis

\begin{align*}
H_0 \colon  & \beta = 0 \\
H_1 \colon & \beta \neq 0
\end{align*}





## Construction of t-test {.smaller}

- More in general, consider the hypothesis 

\begin{align*}
H_0 \colon  & \beta = b \\
H_1 \colon & \beta \neq b
\end{align*}

- Our best guess for $\beta$ is the ML Estimator
$$
\hat \beta = \frac{ S_{xy} }{ S_{xx} }
$$ 

- To test above hypotheses, we therefore need to
    * Know the **distribution** of $\hat \beta$
    * Construct **t-statistic** involving $\hat \beta$




## Distribution of $\hat \beta$ {.smaller}

::: Theorem

Consider the linear regression model 

$$
Y_i = \alpha + \beta x_i + \e_i  \,, \qquad \e_i \,\, \text{ iid } \,\, N(0, \sigma^2)
$$


The MLE $\hat{\beta}$ is normally distributed

$$
\hat \beta \sim N \left(\beta , \frac{ \sigma^2 }{ S_{xx} } \right)
$$

:::


**Proof:** Quite difficult. If interested see Theorem 11.3.3 in [@casella-berger] 




## Construction of t-statistic for $\hat \beta$ {.smaller}


- From the previous Theorem, we know that 
$$
\hat \beta \sim N \left(\beta , \frac{ \sigma^2 }{ S_{xx} } \right)
$$



- In particular $\hat \beta$ is an unbiased estimator for $\beta$
$$
\Expect[ \hat \beta ] = \beta
$$

- This means $\hat \beta$ is the *Estimate* for the unknown parameter $\beta$

- The t-statistic is therefore
$$
t = \frac{\text{Estimate } - \text{ Hypothesised Value}}{\ese} 
  = \frac{ \hat \beta - \beta }{ \ese }
$$




## Estimated Standard Error for $\hat \beta$ {.smaller}


- From the Theorem in Slide 30, we know that 
$$
\Var [\hat \beta] = \frac{ \sigma^2 }{ S_{xx}} \quad \implies \quad \SD [\hat \beta] = \frac{ \sigma }{ \sqrt{S_{xx}} }
$$

- The standard deviation $\SD$ cannot be used as error, since $\sigma^2$ is unknown  
(Recall that $\sigma^2$ is the unknown variance of the error $\e_i$)

- We however have an estimate for $\sigma^2$
$$
\hat \sigma^2 = \frac{1}{n} \RSS = \frac1n \sum_{i=1}^n (y_i - \hat y_i)^2
$$

- $\hat \sigma^2$ was obtained from maximization of the likelihood function (Lecture 8) 



##  {.smaller}


- It can be shown that
$$
\Expect[ \hat\sigma^2 ] = \frac{n-2}{n} \, \sigma^2
$$
(for a proof, see Section 11.3.4 in [@casella-berger])


- Therefore $\hat\sigma^2$ is not unbiased estimator of $\sigma^2$

- To obtain an unbiased estimator, we rescale $\hat\sigma^2$ and introduce $S^2$
$$
S^2 := \frac{n}{n-2} \, \hat\sigma^2 = \frac{\RSS}{n-2} 
$$

- This way, $S^2$ is unbiased estimator for $\sigma^2$
$$
\Expect[S^2] =  \frac{n}{n-2}  \, \Expect[\hat\sigma^2] =  \frac{n}{n-2} \,\cdot \, \frac{n-2}{n} \, \cdot \, \sigma^2 = \sigma^2
$$


## {.smaller}


- Recall that the standard deviation of $\hat \beta$ is

$$
\SD [\hat \beta] = \frac{ \sigma }{ \sqrt{S_{xx}} }
$$

- We replace the unknown $\sigma$ with its unbiased estimator $S$ 

- This gives the **estimated standard error**

$$
\ese := \frac{S}{\sqrt{S_{xx}}} \,, \qquad S = \sqrt{\frac{\RSS}{n-2}}
$$




## t-statistic for testing $\hat \beta$ {.smaller}

The t-statistic for $\hat \beta$ is then

$$
t = \frac{\text{Estimate } - \text{ Hypothesised Value}}{\ese}
  = \frac{ \hat \beta - \beta }{ S / \sqrt{S_{xx}} } 
$$


::: Theorem 

Consider the linear regression model 

$$
Y_i = \alpha + \beta x_i + \e_i \,, \qquad \e_i \,\, \text{ iid } \,\, N(0, \sigma^2)
$$

The t-statistic for $\hat{\beta}$ follows a t-distribution

$$
t = \frac{ \hat \beta - \beta }{ S / \sqrt{S_{xx}} }  
\, \sim \,
t_{n-2}
$$

:::



## How to prove the Theorem {.smaller}

- Proof of this Theorem is quite difficult and we omit it

- If you are interested in the proof, see Section 11.3.4 in [@casella-berger]

- The main idea is that t-statistic can be rewritten as

$$
t = \frac{ \hat \beta - \beta }{ S / \sqrt{S_{xx}} }  
  = \frac{ U }{ \sqrt{ V/(n-2) } }  
$$

- Here we defined

$$
U := \frac{ \hat \beta - \beta }{ \sigma / \sqrt{S_{xx}} } \,, \qquad \quad V := \frac{ (n-2) S^2 }{ \sigma^2 }
$$


##  {.smaller}

- We know that 
$$
\hat \beta \sim N \left(\beta , \frac{ \sigma^2 }{ S_{xx} } \right)
$$


- Therefore 
$$
U = \frac{ \hat \beta - \beta }{ \sigma / \sqrt{S_{xx}} } \, \sim \, N(0,1)
$$


- Moreover, it can be shown that 
$$
V = \frac{(n-2) S^2}{\sigma^2} \, \sim \, \chi_{n-2}^2
$$

- It can also be shown that $U$ and $V$ are independent



##  {.smaller}

- In summary, we have

$$
t = \frac{ \hat \beta - \beta }{ S / \sqrt{S_{xx}} }  
  = \frac{ U }{ \sqrt{ V/(n-2) } }  
$$

- $U$ and $V$ are independent, with

$$
U \sim N(0,1) \,, \qquad \quad V \sim \chi_{n-2}^2
$$

- From the Theorem on t-distribution in Lecture 2, we conclude

$$
t \sim t_{n-2}
$$




## The t-test for $\beta$ {.smaller}

**Assumption:** Given data points $(x_1, y_1), \ldots, (x_n,y_n)$, consider the simple linear regression model
$$
Y_i = \alpha + \beta x_i + \e_i \,, \qquad \e_i \,\, \text{ iid } \,\, N(0,\sigma^2)
$$


**Goal**: Statistical inference on the slope $\beta$


**Hypotheses**: If $b$ is guess for $\beta$, the two-sided hypothesis is
$$
H_0 \colon \beta = b \,, \quad \qquad 
H_1 \colon \beta \neq b
$$
The one-sided alternative hypotheses are
$$
H_1 \colon \beta < b \quad  \text{ or } \quad 
H_1 \colon \beta > b
$$




## Procedure: 3 Steps {.smaller}

1. **Calculation**: Compute the MLE $\hat{\alpha}, \hat{\beta}$ and the predictions $\hat{y}_i$
$$
\hat \beta = \frac{ S_{xy} }{ S_{xx} }  \,, \qquad 
\hat \alpha = \overline{y} - \hat \beta \overline{x} \,, \qquad \hat{y}_i = \hat{\alpha} + \hat{\beta} x_i
$$
Compute the *Residual Sum of Squares* and the estimator $S^2$ 
$$
\RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2 \,, \qquad  S^2 := \frac{\RSS}{n-2}
$$
Finally, compute the t-statistic
$$
t = \frac{\hat \beta - b }{ \ese }  \,, \qquad \ese = \frac{S}{\sqrt{S_{xx}} }
$$



## {.smaller}

2. **Statistical Tables or R**: Find either
    * Critical value $t^*$ in [Table 1](files/Statistics_Tables.pdf)
    * p-value in R

<br>

3. **Interpretation**: Reject $H_0$ when either
$$
p < 0.05 \qquad \text{ or } \qquad t \in \,\,\text{Rejection Region} 
\qquad \qquad \qquad \qquad
(T \, \sim \, t_{n-2})
$$




| Alternative                      | Rejection Region  | $t^*$             | p-value         |
|----------------------------------|-------------------|-------------------|-----------------|
| $\beta \neq b$               | $|t| > t^*$       | $t_{n-2}(0.025)$| $2P(T > |t|)$   |
| $\beta < b$                  | $t < - t^*$       | $t_{n-2}(0.05)$ | $P(T < t)$      |
| $\beta > b$                  | $t > t^*$         | $t_{n-2}(0.05)$ | $P(T > t)$      |
: {tbl-colwidths="[25,25,25,25]"}







## Worked Example: Stock and Gold prices {.smaller}

- Recall that 
    * $X =$ Stock Price
    * $Y =$ Gold Price

- **Goal:** Test if *Gold Price* affects *Stock Price* at level $0.05$

- **Procedure:** Consider the linear model
$$
Y_i =  \alpha + \beta x_i + \e_i  \,, \quad \e_i \,\, \text{ iid } \,\, N(0,\sigma^2)
$$
Test the hypotheses with two-sided alternative
\begin{align*}
H_0 & \colon \beta = 0 \\
H_1 & \colon \beta \neq 0
\end{align*}




## Testing for $\beta = 0$ {.smaller}

- Recall that Stock Prices and Gold Prices are stored in R vectors
    * ``stock.price``
    * ``gold.price``


- Fit the simple linear model with the following commands

$$
\text{gold.price } = \alpha + \beta \, \times  (\text{ stock.price }) + \text{ error}
$$


```r
# Fit simple linear regression model
fit.model <- lm(gold.price ~ stock.price)

# Print result to screen
summary(fit.model)
```



## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::
  

- $\texttt{(Intercept)}$: 1st row of the table contains information related to $\hat \alpha$

- $\texttt{stock.price}$: 2nd row of the table contains information related to $\hat \beta$

- For larger models, there will be additional rows below the 2nd
    * These will be **informative** about additional regression parameters



## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::
  
- The 2nd row of the table has to be interpreted as follows


| $\texttt{Estimate}$   | $\text{The value of } \hat \beta$                  |
|:------------------    |:----------                                         |
| $\texttt{Std. Error}$ | Estimated standard error $\ese$ for $\hat \beta$        |
| $\texttt{t value}$    | t-statistic $\, t = \dfrac{\hat \beta - 0 }{\ese}$|
| $\texttt{Pr(>|t|)}$   | p-value $\, p = 2 P( t_{n-2} > |t| )$             |
| $\texttt{***}$, $\, \texttt{**}$, $\, \texttt{*}$, $\, \texttt{.}$   | Significance (how large is p-value) -- More stars is better |
: {tbl-colwidths="[30,70]"}




## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::
  
- The above table then gives

$$
\hat \beta = -6.169 \,, \qquad \ese = 1.37 \, , \qquad t = - 4.502 \,, \qquad 
p = 8.9 \times 10^{-5} \
$$


- The $t$-statistic computed by R can also be computed by hand

$$
  t   = \frac{\hat \beta - 0}{ \ese }
      = \frac{-6.169}{1.37} = -4.502 
$$




## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::
  

- The p-value computed by R is $p = 8.9 \times 10^{-5}$

- We can also compute it by hand: The degrees of freedom are

$$
n = \text{No. of data points} = 33 \,, \qquad  \text{df} = n - 2 = 31
$$

```{r}
#| echo: true
t <- - 4.502 
p <- 2 - 2 * pt(abs(t), df = 31)

cat("The p-value is", p)
```



## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::


- In alternative, we can find critical value in [Table 1](files/Statistics_Tables.pdf)

$$
n = \text{No. of data points} = 33 \,, \qquad  \text{df} = n - 2 = 31
$$

- For two-sided test, the critical value is $\, t_{31}(0.025) = 2.040$

$$
|t| = 4.502 > 2.040 = t_{31}(0.025) \quad \implies \quad  p < 0.05
$$


## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::


**Interpretation:** $\, p$ is very small (hence the $\, \texttt{***} \,$ rating)

- Therefore, we reject the null hypothesis $H_0$, and the real parameter is $\beta \neq 0$

- Since $\beta \neq 0$, we have that *Stock Prices* affect *Gold Prices*

- The best estimate for $\beta$ is the MLE $\hat \beta = -6.169$

- $\hat \beta < 0$ and statistically significant:  
$\,\,$ 
As *Stock Prices* increase *Gold Prices* decrease





## Warning {.smaller}

The t-statistic and p-value in the *summary* refer to the two-sided test
$$
H_0 \colon \beta = 0 \,, \qquad 
H_1 \colon \beta \neq 0
$$
In such case the t-statistic and p-value given in *summary are*
$$
t = \frac{\hat \beta - 0}{\ese} \,, \qquad p = 2P(t_{n- 2} > |t|) 
$$


## {.smaller}

If instead you are required to test
$$
H_0 \colon \beta = b \,, \qquad 
H_1 \colon \beta \neq b \,, \quad 
\beta < b\,, \quad  \text{ or } \,\,
\beta > b
$$

1. The t-statistic has to be computed by hand
$$
t = \frac{\hat \beta - b}{\ese}
$$
    * $\, \ese$ is in the 2nd row under $\,\, \texttt{Std. Error}$

2. The p-value must be computed by hand, according to the table

| Alternative                  | p-value         | R command    |
|------------------------------|-----------------|--------------|
| $\beta \neq b$               | $2P(t_{n-2} > |t|)$      | ``2 - 2 * pt(abs(t), df = n - 2)``|
| $\beta < b$                  | $P(t_{n-2} < t)$      | ``pt(t, df = n - 2)``|
| $\beta > b$                  | $P(t_{n-2} > t)$      | ``1 - pt(t, df = n - 2)`` |
: {tbl-colwidths="[20,25,50]"}



# Part 4: <br>t-test for general regression {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Recall: The t-test for simple regression {.smaller}

- In the previous Part, we have considered the simple linear regression model

$$
Y_i = \alpha + \beta x_i + \e_i \,, \qquad \e_i \,\, \text{ iid } \,\, N(0,\sigma^2)
$$

- We have that 

$$
X \,\, \text{ affects } \,\, Y \qquad \iff \qquad
\beta \neq 0
$$

- To see if $X$ affects $Y$, we have developed a t-test for the hypothesis

\begin{align*}
H_0 \colon  & \beta = b \\
H_1 \colon & \beta \neq b
\end{align*}


## The t-test for general regression {.smaller}

- Now, consider the *general linear regression model*

$$
Y_i = \beta_1 z_{i1} + \ldots + \beta_{ip} z_{ip} + \e_i \,, \qquad 
\e_i \, \text{ iid } \, N(0, \sigma^2)
$$

- We have that 

$$
Z_j \,\, \text{ affects } \,\, Y \qquad \iff \qquad
\beta_j \neq 0
$$

- To see if $Z_j$ affects $Y$ we can test the hypothesis

\begin{align*}
H_0 \colon  & \beta_j = 0 \\
H_1 \colon & \beta_j \neq 0
\end{align*}




## Construction of t-test {.smaller}

- More in general, consider the hypothesis 

\begin{align*}
H_0 \colon  & \beta_j = b_j \\
H_1 \colon & \beta_j \neq b_j
\end{align*}

- Our best guess for $\beta_j$ is the ML Estimator
$$
\hat{\beta}_j = (\hat \beta)_j \,, \qquad \hat \beta = (Z^T Z)^{-1} Z^T y
$$
($\beta_j$ is the j-th component of the vector $\beta$)

- To test above hypotheses, we therefore need to
    * Know the **distribution** of $\hat{\beta}_j$
    * Construct **t-statistic** involving $\hat{\beta}_j$



## Distribution of $\hat \beta_j$ {.smaller}

::: {style="font-size: 0.92em"}

::: Theorem

Consider the general linear regression model 
$$
Y_i = \beta_1 z_{i1} + \ldots + \beta_{ip} z_{ip} + \e_i \,, \qquad 
\e_i \, \text{ iid } \, N(0, \sigma^2)
$$
The MLE $\hat{\beta}_j$ is normally distributed
$$
\hat \beta_j \sim N \left( \beta_j ,  \xi_{jj} \sigma^2 \right) \,,
$$
where the numbers $\xi_{jj}$ are the diagonal entries of the $p \times p$ matrix
$$
(Z^T Z)^{-1} =
\left(
\begin{array}{ccc}
\xi_{11} & \ldots & \xi_{1p} \\
\ldots   & \ldots & \ldots \\
\xi_{p1} & \ldots & \xi_{pp} \\
\end{array}
\right)
$$


:::


**Proof:** Quite difficult. If interested, see see Section 11.5 in [@degroot] 

:::




## Construction of the t-statistic for $\hat{\beta}_j$ {.smaller}

- From the previous Theorem, we know that
$$
\hat \beta_j \sim N \left( \beta_j ,  \xi_{jj} \sigma^2 \right)
$$


- In particular, $\hat{\beta}_j$ is an unbiased estimator for $\beta_j$
$$
\Expect[ \hat{\beta}_j ] = \beta_j
$$

- This means $\hat{\beta}_j$ is the *Estimate* for the unknown parameter $\beta$

- The t-statistic is therefore
$$
t = \frac{\text{Estimate } - \text{ Hypothesised Value}}{\ese} 
  = \frac{ \hat{\beta}_j - \beta_j }{ \ese }
$$



## Estimated Standard Error for $\hat{\beta}_j$ {.smaller}


- From the Theorem, we know that 
$$
\Var [\hat{\beta}_j] =  \xi_{jj} \, \sigma^2  \qquad \implies \qquad 
\SD [\hat \beta_j] =  \xi_{jj}^{1/2} \, \sigma
$$

- The standard deviation $\SD$ cannot be used as error, since $\sigma^2$ is unknown


- We however have an estimate for $\sigma^2$
$$
\hat \sigma^2 = \frac{1}{n} \RSS = \frac1n \sum_{i=1}^n (y_i - \hat y_i)^2
$$

- $\hat \sigma^2$ was obtained from maximization of the likelihood function (Lecture 8) 



## {.smaller}


- It can be shown that (see Section 11.5 in [@degroot])
$$
\Expect[ \hat\sigma^2 ] = \frac{n-p}{n} \, \sigma^2
$$


- Therefore $\hat\sigma^2$ is not unbiased estimator of $\sigma^2$

- To obtain unbiased estimator, we rescale $\hat\sigma^2$ and introduce $S^2$
$$
S^2 := \frac{n}{n-p} \, \hat\sigma^2 = \frac{\RSS}{n-p} 
$$

- This way, $S^2$ is unbiased estimator for $\sigma^2$

$$
\Expect[S^2] =  \frac{n}{n-p}  \, \Expect[\hat\sigma^2] =  \frac{n}{n-p} \, \frac{n-p}{n} \, \sigma^2 = \sigma^2
$$


## {.smaller}

- Recall that the standard deviation of $\hat{\beta}_j$ is

$$
\SD [\hat{\beta}_j] = \xi_{jj}^{1/2} \, \sigma
$$

- We replace the unknown $\sigma$ with its unbiased estimator $S$ 

- This gives the **estimated standard error**

$$
\ese =\xi_{jj}^{1/2} \,  S \,, \qquad S = \sqrt{\frac{\RSS}{n-2}}
$$



## t-statistic for testing $\beta_j$ {.smaller}


::: Theorem 

Consider the general linear regression model 

$$
Y_i = \beta_1 z_{i1} + \ldots +\beta_p z_{ip} + \e_i \,, \qquad \e_i \,\, \text{ iid } \,\, N(0, \sigma^2)
$$

The t-statistic for $\hat{\beta}$ follows a t-distribution

$$
t = \frac{\text{Estimate } - \text{ Hypothesised Value}}{\ese} = \frac{ \hat \beta_j - \beta_j }{ \xi_{jj}^{1/2} \, S}  
\, \sim \,
t_{n-p}
$$

:::

**Proof:** See section 11.5 in [@degroot]




## The t-test for $\beta_j$ {.smaller}

**Assumption:** Given data points $(z_{1i}, \ldots, z_{pi}, y_i)$, consider the general model
$$
Y_i = \beta_1 z_{i1} + \ldots + \beta_p z_{ip} + \e_i \,, \qquad \e_i \, \text{ iid } \, N(0,\sigma^2)
$$

**Goal**: Statistical inference on the coefficients $\beta_j$

**Hypotheses**: If $b_j$ is guess for $\beta_j$, the two-sided hypotheses is
$$
H_0 \colon \beta_j = b_j \,, \quad \qquad 
H_1 \colon \beta_j \neq b_j
$$
The one-sided alternative hypotheses are
$$
H_1 \colon \beta_j < b_j \quad  \text{ or } \quad 
H_1 \colon \beta_j > b_j
$$



## Procedure: 3 Steps {.smaller}


1. **Calculation**: Write down the design matrix $Z$ and compute $(Z^TZ)^{-1}$
$$
Z :=
\left(
\begin{array}{ccc}
z_{11} & \ldots & z_{1p} \\
\ldots & \ldots & \ldots \\
z_{n1} & \ldots & z_{np} \\
\end{array}
\right) \,, \qquad
(Z^T Z)^{-1} =
\left(
\begin{array}{ccc}
\xi_{11} & \ldots & \xi_{1p} \\
\ldots   & \ldots & \ldots \\
\xi_{p1} & \ldots & \xi_{pp} \\
\end{array}
\right)
$$
Compute the MLE $\hat{\beta}$, predictions $\hat{y}$, RSS and $S^2$
$$
\hat \beta = (Z^TZ)^{-1} Z^T y  \,, \qquad 
\hat{y} = Z \hat{\beta} \,, \qquad 
\RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2 \,, \qquad  S^2 := \frac{\RSS}{n-2}
$$
Finally, compute the t-statistic
$$
t = \frac{\hat \beta_j - b_j }{ \ese }  \,, \qquad \quad
\ese = \xi_{jj}^{1/2} \, S 
$$





## {.smaller}

2. **Statistical Tables or R**: Find either
    * Critical value $t^*$ in [Table 1](files/Statistics_Tables.pdf)
    * p-value in R

<br>

3. **Interpretation**: Reject $H_0$ when either
$$
p < 0.05 \qquad \text{ or } \qquad t \in \,\,\text{Rejection Region} 
\qquad \qquad \qquad \qquad
(T \, \sim \, t_{n-p})
$$




| Alternative                      | Rejection Region  | $t^*$             | p-value         |
|----------------------------------|-------------------|-------------------|-----------------|
| $\beta_j \neq b_j$               | $|t| > t^*$       | $t_{n-p}(0.025)$| $2P(T > |t|)$   |
| $\beta_j < b_j$                  | $t < - t^*$       | $t_{n-p}(0.05)$ | $P(T < t)$      |
| $\beta_j > b_j$                  | $t > t^*$         | $t_{n-p}(0.05)$ | $P(T > t)$      |
: {tbl-colwidths="[25,25,25,25]"}







## Implementation in R {.smaller}

- First, fit general regression model with ``lm``

- Then read the *summary*

Assume the hypotheses to test are
$$
H_0 \colon \beta_j = 0 \,, \qquad 
H_1 \colon \beta_j \neq 0
$$
In such case, the t-statistic and p-value are explicitly given in the *summary*
$$
t = \frac{\hat \beta_j - 0}{\ese} \,, \qquad p = 2P(t_{n- p} > |t|) 
$$

- Read the t-statistic in $j$-th variable row under $\,\, \texttt{t value}$

- Read the p-value in $j$-th variable row under $\,\, \texttt{Pr(>|t|)}$




## {.smaller}

::: {style="font-size: 0.92em"}

If instead you are required to test
$$
H_0 \colon \beta_j = b_j \,, \qquad 
H_1 \colon \beta_j \neq b_j \,, \quad 
\beta_j < b_j \,, \quad  \text{ or } \,\,
\beta_j > b_j
$$

1. The t-statistic has to be computed by hand
$$
t = \frac{\hat{\beta}_j - b_j}{\ese}
$$
    * $\,\, \hat \beta_j$ is in $j$-th variable row under $\,\, \texttt{Estimate}$
    * $\,\, \ese$ for $\hat \beta_j$ is in $j$-th variable row under $\,\, \texttt{Std. Error}$


2. The p-value must be computed by hand, according to the table

| Alternative                  | p-value         | R command    |
|------------------------------|-----------------|--------------|
| $\beta_j \neq b_j$               | $2P(t_{n-p} > |t|)$      | ``2 - 2 * pt(abs(t), df = n - p)``|
| $\beta_j < b_j$                  | $P(t_{n-p} < t)$      | ``pt(t, df = n - p)``|
| $\beta_j > b_j$                  | $P(t_{n-p} > t)$      | ``1 - pt(t, df = n - p)`` |
: {tbl-colwidths="[20,25,50]"}

:::







## Worked Example: Stock and Gold prices {.smaller}

- Recall that 
    * $X =$ Stock Price
    * $Y =$ Gold Price

- **Goal:** Test if an *intercept is present*, at level $0.05$

- **Procedure:** Consider the linear model
$$
Y_i =  \alpha + \beta x_i + \e_i  \,, \quad \e_i \,\, \text{ iid } \,\, N(0,\sigma^2)
$$
Test the hypotheses with two-sided alternative
\begin{align*}
H_0 & \colon \alpha = 0 \\
H_1 & \colon \alpha \neq 0
\end{align*}




## Testing for $\alpha= 0$ {.smaller}

- Recall that Stock Prices and Gold Prices are stored in R vectors
    * ``stock.price``
    * ``gold.price``


- Fit the simple linear model with the following commands

$$
\text{gold.price } = \alpha + \beta \, \times  (\text{ stock.price }) + \text{ error}
$$


```r
# Fit simple linear regression model
fit.model <- lm(gold.price ~ stock.price)

# Print result to screen
summary(fit.model)
```





## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::
  
- $\texttt{(Intercept)}$: 1st row of the table contains information related to $\hat \alpha$

| $\texttt{Estimate}$   | $\text{The value of } \hat \alpha$                  |
|:------------------    |:----------                                         |
| $\texttt{Std. Error}$ | Estimated standard error $\ese$ for $\hat \alpha$        |
| $\texttt{t value}$    | t-statistic $\, t = \dfrac{\hat \alpha - 0 }{\ese}$|
| $\texttt{Pr(>|t|)}$   | p-value $\, p = 2 P( t_{n-2} > |t| )$             |
| $\texttt{***}$, $\, \texttt{**}$, $\, \texttt{*}$, $\, \texttt{.}$   | Significance (how large is p-value) -- More stars is better |
: {tbl-colwidths="[30,70]"}



## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::
  
- The above table then gives

$$
\hat \alpha = 37.917 \,, \qquad \ese = 4.336 \, , \qquad t =  8.744 \,, \qquad 
p = 7.14 \times 10^{-10} \
$$


- The $t$-statistic computed by R can also be computed by hand

$$
  t   = \frac{\hat \alpha - 0}{ \ese }
      = \frac{37.917}{4.336} = 8.744 
$$




## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::
  
- The p-value computed by R is $p = 7.14 \times 10^{-10}$

- We can also compute it by hand: The degrees of freedom are

$$
n = \text{No. of data points} = 33 \,, \qquad  \text{df} = n - 2 = 31
$$

```{r}
#| echo: true
t <- 8.744
p <- 2 - 2 * pt(abs(t), df = 31)

cat("The p-value is", p)
```




## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::


- In alternative, we can find the critical value in [Table 1](files/Statistics_Tables.pdf)

$$
n = \text{No. of data points} = 33 \,, \qquad  \text{df} = n - 2 = 31
$$

- For two-sided test, the critical value is $\, t_{31}(0.025) = 2.040$

$$
|t| = 8.744 > 2.040 = t_{31}(0.025) \quad \implies \quad  p < 0.05
$$




## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::


**Interpretation:** $\, p$ is very small (hence the $\, \texttt{***} \,$ rating)

- Therefore, we reject the null hypothesis $H_0$, and the real parameter is $\alpha \neq 0$

- Since $\alpha \neq 0$, we deduce that the model has to *include an intercept*

- The best estimate for $\alpha$ is the MLE $\hat \alpha = 37.917$





## Theoretical Example {.smaller}
### Deriving $\ese$ for simple linear regression 

- Consider the simple regression model

$$
Y_i = \alpha + \beta x_i + \e_i
$$


- We have seen in the previous part that the $\ese$ for $\hat{\beta}$ is

$$
\ese = \frac{S}{\sqrt{S_{xx}}} \,, \qquad S = \sqrt{ \frac{\RSS}{n-2} }
$$


- However, we have not computed the $\ese$ for $\hat{\alpha}$


**Goal:** Compute $\ese$ for $\hat{\alpha}$ using the theory developed for general regression




## {.smaller}

**Theory developed so far:**

- Consider the general linear model
$$
Y_i = \beta_1 z_{i1} + \ldots +\beta_p z_{ip} + \e_i \,, \qquad \e_i \,\, \text{ iid } \,\, N(0, \sigma^2)
$$

- We have shown that the estimated standard error for $\hat{\beta}_j$ is
$$
\ese = \xi_{jj}^{1/2} \, S \,, \qquad 
S = \sqrt{ \frac{\RSS}{n-2} }
$$
where $\xi_{jj}$ are the diagonal entries of the matrix
$$
(Z^T Z)^{-1} =
\left(
\begin{array}{ccc}
\xi_{11} & \ldots & \xi_{1p} \\
\ldots   & \ldots & \ldots \\
\xi_{p1} & \ldots & \xi_{pp} \\
\end{array}
\right)
$$


## {.smaller}

- Let us go back to the linear regression model

$$
Y_i = \alpha + \beta x_i + \e_i
$$



- The design matrix is

$$
Z = \left( 
\begin{array}{cc}
1 & x_1 \\
\ldots & \ldots \\
1 & x_n \\
\end{array}
\right)
$$


##  {.smaller}

- We have computed in Lecture 8 that

$$
(Z^T Z)^{-1} = 
\frac{1}{n S_{xx} }
\left(
\begin{array}{cc}
\sum_{i=1}^n x^2_i & -n \overline{x}\\
-n\overline{x} & n
\end{array}
\right)
$$


- Hence the $\ese$ for $\hat \alpha$ and $\hat \beta$ are

\begin{align*}
\ese (\hat \alpha) & = \xi_{11}^{1/2} \, S = \sqrt{ \frac{ \sum_{i=1}^n x_i^2 }{ n S_{xx} } } \, S \\[7pt]
\ese (\hat \beta) & = \xi_{22}^{1/2} \, S = \frac{ S }{ \sqrt{ S_{xx} } }
\end{align*}

- **Note:** $\, \ese(\hat\beta)$ coincides with the $\ese$ in Slide 35




# Part 5: <br>F-test for multiple regression {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## F-test for overall significance {.smaller}

- Want to test the **overall significance** of the model

$$
Y_i = \beta_1  + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \e_i
$$

- This means answering the question:

$$
\text{ Does at least one } X_i \text{ affect } Y \text{ ?}
$$

- How to do this?
    * Could perform a sequence of t-tests on the $\beta_j$
    * For statistical reasons this is not really desirable
    * To assess **overall** significance we can perform **F-test**




## F-test for overall significance {.smaller}

The F-test for overall significance has 3 steps:

1. Define a larger **Full Model** (with more parameters)


2. Define a smaller nested **Reduced Model** (with fewer parameters)


3. Use an F-statistic to decide between larger or smaller model



## Overall significance for multiple regression {.smaller}

- Model 1 is the smaller **Reduced Model**
- Model 2 is the larger **Full Model**

\begin{align*}
\textbf{Model 1:} & \quad Y_i = \beta_1 + \e_i \\[15pt]
\textbf{Model 2:} & \quad Y_i = \beta_1  + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \e_i
\end{align*}


- Choosing the Full Model 2 is equivalent to rejecting $H_0$

\begin{align*}
H_0 & \colon \, \beta_2 = \beta_3 = \ldots = \beta_p = 0 \\
H_1 & \colon \text{ At least one of the } \beta_i \text{ is non-zero}
\end{align*}

**Coefficients $\beta_i$ in both models are intended to be the components of the MLE $\hat \beta$**


## Overall significance for simple regression {.smaller}


- Model 1 is the smaller **Reduced Model**
- Model 2 is the larger **Full Model**

\begin{align*}
\textbf{Model 1:} & \quad Y_i = \alpha + \e_i \\[15pt]
\textbf{Model 2:} & \quad Y_i = \alpha  + \beta x_i  + \e_i
\end{align*}


- Choosing the Full Model 2 is equivalent to rejecting $H_0$

\begin{align*}
H_0 & \colon \beta = 0 \\
H_1 & \colon \beta \neq 0 
\end{align*}

**Coefficients $\alpha, \beta$ in both models are intended to be the MLEs $\hat\alpha, \hat\beta$**



## RSS for the Full Model {.smaller}

- Consider the Full Model with $p$ parameters

$$
\textbf{Model 2:}  \quad Y_i = \beta_1  + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \e_i
$$

- Predictions for the Full Model are

$$
\hat y_i := \beta_1  + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} \,, \quad \forall \, i = 1 , \ldots, n
$$

- Define the *Residual Sum of Squares* for the Full Model

$$
\RSS(p) := \sum_{i=1}^n (y_i - \hat y_i)^2 
$$




## RSS for the Reduced Model {.smaller}

- Consider now the Reduced Model

$$
\textbf{Model 1:} \quad Y_i = \beta_1 + \e_i
$$


- Predictions of the Reduced Model are constant

$$
\hat y_i = \beta_1 \,, \quad \forall \, i = 1 , \ldots, n
$$

- Define the *Residual Sum of Squares* for the Reduced Model

$$
\RSS(1) := \sum_{i=1}^n (y_i - \beta_1)^2
$$



## Property of the RSS {.smaller}

- Recall that $\RSS$ is defined via **minimization**
$$
\RSS(k) := \min_{\beta_1 , \ldots , \beta_k} \ \sum_{i=1}^n ( y_i - \hat y_i)^2 \,, \qquad 
\hat y_i := \beta_1 + \beta_2 x_{i2} + \ldots + \beta_k x_{ik}
$$
with minumum achieved when $\beta$ coincides with the MLE $\hat \beta = (Z^TZ)^{-1}Z^Ty$

- Therefore $\RSS$ cannot increase if we add parameters to the model

$$
\RSS(1) \geq \RSS(p)
$$


- In particular, we have that 

$$
\frac{\RSS(1) - \RSS (p)}{\RSS(p)} \geq 0
$$



## Deciding between the two models {.smaller}

- Suppose the parameters of the Full Model
$$
\beta_2, \ldots, \beta_p
$$
are not important (meaning that they are close to $0$)

- In this case the predictions of full and Reduced Model will be similar

- Therefore the $\RSS$ for the 2 models are similar

$$
\RSS (1) \, \approx \, \RSS(p)
$$




## Deciding between the two models {.smaller}

- Suppose instead that the parameters of the Full Model
$$
\beta_2, \ldots, \beta_p
$$
are important (meaning that they are not close to $0$)


- In this case the predictions of full and Reduced Model will be different

- In particular, the Full Model will give much better predictions

- This means the RSS for the Full Model will be **smaller**

$$
\RSS (1) \gg \RSS(p)
$$






## Conclusion {.smaller}

- To measure how influential the parameters $\beta_2, \ldots, \beta_p$ are, we study

$$
\frac{\RSS(1) - \RSS (p)}{\RSS(p)}
$$


- If parameters $\beta_2, \ldots, \beta_p$ are **not influential**, then 

$$
\RSS(1) \approx \RSS (p) \qquad \implies \qquad  \frac{\RSS(1) - \RSS (p)}{\RSS(p)} \approx 0
$$


- If parameters $\beta_2, \ldots, \beta_p$ are **influential**, then 

$$
\RSS(1) \gg \RSS (p) \qquad \implies \qquad  \frac{\RSS(1) - \RSS (p)}{\RSS(p)} \gg 0
$$





## Construction of F-statistic {.smaller}

- In order to construct a meaningful statistic, we need to rescale the quantity

$$
\frac{\RSS(1) - \RSS (p)}{\RSS(p)}
$$

- Note that the degrees of freedom of Reduced Model (1 parameter) are 

$$
\df (1) = n - 1
$$

- The degrees of freedom of the Full Model (p parameters) are 

$$
\df (p) = n - p
$$



## F-statistic for overall significance {.smaller}


::: Definition

The **F-statistic** for overall significance is

\begin{align*}
F & := \frac{\RSS(1) - \RSS (p)}{ \df(1) - \df (p) } \bigg/ 
\frac{\RSS(p)}{\df(p)}  \\[15pt] 
  & = \frac{\RSS(1) - \RSS (p)}{ p - 1 } \bigg/ 
\frac{\RSS(p)}{n - p}
\end{align*}

:::


**Theorem:** The F-statistic for overall significance has F-distribution

$$
F \, \sim \, F_{p-1,n-p}
$$






## Rewriting the F-statistic {.smaller}

::: Proposition

The F-statistic can be rewritten as

\begin{align*}
F & = \frac{\RSS(1) - \RSS (p)}{ p - 1 } \bigg/ 
\frac{\RSS(p)}{n - p}  \\[15pt]
  & = \frac{R^2}{1 - R^2} \, \cdot \, \frac{n - p}{p - 1} 
\end{align*}

:::



## Proof of Proposition {.smaller}

- Recall the definition of *Total Sum of Squares*
$$
\TSS = \sum_{i=1}^n (y_i - \overline{y})^2
$$

- Notice that the definition of $\TSS$ does not depend on $p$


- Recall the definition of $R^2$ for the Full Model (which has p parameters)
$$
R^2 = 1 - \frac{\RSS (p)}{\TSS} 
$$


- From the above, we obtain

$$
\RSS(p) = (1 - R^2) \TSS
$$




##  {.smaller}

- By definition, we have that 

$$
\RSS(1) = \min_{\beta_1} \ \sum_{i=1}^n (y_i - \beta_1)^2 
$$

- **Exercise:** Check that the unique solution to the above problem is

$$
\beta_1 = \overline{y}
$$

- Therefore, we have

$$
\RSS(1) =  \sum_{i=1}^n (y_i - \overline{y})^2 = \TSS
$$




##  {.smaller}

- We just obtained the two identities

$$
\RSS(p) = (1 - R^2) \TSS \,, \qquad \quad \RSS(1) = \TSS
$$


- From the above, we conclude the proof

\begin{align*}
F & = \frac{\RSS(1) - \RSS (p)}{ p - 1 } \bigg/ 
\frac{\RSS(p)}{n - p}  \\[15pt]
  & = \frac{\TSS - (1 - R^2) \TSS}{ p - 1 } \bigg/ 
\frac{(1 - R^2) \TSS}{n - p}\\[15pt]
  & = \frac{R^2}{1 - R^2} \, \cdot \, \frac{n - p}{p - 1} 
\end{align*}





## F-statistic for simple regression {.smaller}

::: Proposition

1. The $F$-statistic for overall significance in simple regression is
$$
F = t^2 \,, \qquad \quad t = \frac{\hat \beta}{ S / \sqrt{S_{xx}}}
$$
where $t$ is the t-statistic for $\hat \beta$.

2. In particular, the p-values for t-test and F-test coincide
$$
p = 2P( t_{n-2} > |t| ) = P( |t_{n-2}| > |t| )  = P( F_{1,n-2} > F )
$$

:::

**Proof:** Will be left as an exercise




## The F-test for overall significance {.smaller}

**Assumption:** Given data points $(x_{i2},\ldots , x_{ip},  y_i)$, consider the nested models

\begin{align*}
\textbf{Model 1:} & \quad Y_i = \beta_1 + \e_i  & \text{Reduced Model}\\[15pt]
\textbf{Model 2:} & \quad Y_i = \beta_1  + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \e_i & \text{Full Model}
\end{align*}

**Goal:** Testing the *overall significance* of the parameters $\beta_2,\ldots, \beta_p$  
(i.e. decide which model gives better predictions)

**Hypothesis:** Choosing the Full Model 2, is equivalent to rejecting $H_0$

\begin{align*}
H_0 & \colon \, \beta_2 = \beta_3 = \ldots = \beta_p = 0 \\
H_1 & \colon \text{ At least one of the } \beta_i \text{ is non-zero}
\end{align*}



## Procedure: 3 Steps {.smaller}

1. **Calculation**: Write design matrix $Z$, and compute MLE $\hat{\beta}$ and predictions $\hat{y}_i$
$$
Z =
\left(
\begin{array}{cccc}
1 & x_{12} & \ldots & x_{1p} \\
\ldots & \ldots & \ldots & \ldots \\
1 & x_{n2} & \ldots & x_{np} \\
\end{array}
\right) \,, \qquad \hat{\beta} = (Z^TZ)^{-1} Z^T y \,, \qquad \hat{y} = Z \hat{\beta}
$$
Compute the $\RSS$, $\TSS$ and $R^2$ coefficient for the Full Model
$$
\RSS(p) = \sum_{i=1}^n (y_i - \hat{y}_i)^2 \,, \qquad 
\TSS = \sum_{i=1}^n (y_i - \overline{y})^2\,, \qquad 
R^2 = 1 - \frac{\RSS(p)}{\TSS}
$$
Finally, compute the F-statistic for *overall significance*
$$
F = \frac{R^2}{1 - R^2} \, \cdot \, \frac{n - p}{p - 1} \ \sim \ F_{p-1, n - p} 
$$



## {.smaller}

2. **Statistical Tables or R**: Find either
    * Critical value $F^*$ in [Table 3](files/Statistics_Tables.pdf)
    * p-value in R

<br>

3. **Interpretation**: Reject $H_0$ when either
$$
p < 0.05 \qquad \text{ or } \qquad F \in \,\,\text{Rejection Region}
$$




| Alternative                               | Rejection Region  | $F^*$              | p-value              |
|-------------------------------------------|-------------------|--------------------|----------------------|
| At least one of the $\beta_i$ is non-zero | $F > F^*$         | $F_{p-1,n-p}(0.05)$| $P(F_{p-1,n-p} > F)$ |
: {tbl-colwidths="[25,25,25,25]"}



## Implementation in R {.smaller}


1. Fit the Full Model with ``lm``

2. Read the *Summary*
    * F-statistic is listed in the summary
    * p-value is listed in the summary





## Worked Example: Stock and Gold prices {.smaller}

- Recall that 
    * $X =$ Stock Price
    * $Y =$ Gold Price

- Consider the simple linear regression model

$$
Y_i =  \alpha + \beta x_i + \e_i 
$$

- We want to test the **overall significance** of the parameter $\beta$


## {.smaller}

- This means deciding which of the two models below gives better predictions

\begin{align*}
\textbf{Model 1:} & \quad Y_i = \alpha + \e_i  & \text{Reduced Model}\\[15pt]
\textbf{Model 2:} & \quad Y_i = \alpha  + \beta x_{i} + \e_i & \text{Full Model}
\end{align*}

- **Hypothesis:** Choosing the Full Model 2, is equivalent to rejecting $H_0$

\begin{align*}
H_0 & \colon \, \beta = 0 \\
H_1 & \colon \, \beta \neq 0
\end{align*}

- Let us perform the F-test in R





##  {.smaller}

- Recall that Stock Prices and Gold Prices are stored in R vectors
    * ``stock.price``
    * ``gold.price``


- Fit the simple linear model with the following commands

$$
\text{gold.price } = \alpha + \beta \, \times ( \text{ stock.price } ) + \text{ error}
$$


```r
# Fit simple linear regression model
fit.model <- lm(gold.price ~ stock.price)

# Print result to screen
summary(fit.model)
```



## Output: F-statistic and p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_3.png){width=80%}
:::

::::
  

- The computed F-statistic is $F = 20.27$ 

- There are $n = 33$ data points, and $p = 2$ parameters in the Full Model

- Therefore, the degrees of freedom are
$$
{\rm df}_1 = p - 1 = 2 - 1 = 1 \,, \qquad 
{\rm df}_2 = n - p = 33 - 2 = 31
$$
These are listed in the output, after the F-statistic

- In particular, we have that the F-statistic has distribution 
$$
F \ \sim \ F_{{\rm df}_1, {\rm df}_2} = F_{1,31}
$$





## Output: F-statistic and p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_3.png){width=80%}
:::

::::
  

- The computed p-value is 
$$
p = P( F_{1,31} > F ) = 8.904 \times 10^{-5}
$$


- **Conclusion:** Strong evidence ($p=0.000$) that the parameter $\beta$ is significant
    * Going back to the model, this means that *Stock Price* affects *Gold Price*





## Output: F-statistic and p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_3.png){width=80%}
:::

::::
  

We could also compute F-statistic by hand

- From the output we see that $R^2 = 0.3953$

- We have $p = 2$ and $n =33$

- Therefore 

$$
F = \frac{ R^2 }{ 1 - R^2  } \, \cdot \, \frac{n-p}{p-1} = \frac{0.395325}{0.604675} \, \cdot \, \frac{31}{1} = 20.267
$$





## Output: F-statistic and p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_3.png){width=80%}
:::

::::
  

- Similarly, the p-value could be computed by hand

$$
p = P( F_{1,31} > F ) = 8.904 \times 10^{-5}
$$

```{r}
#| echo: true
F <- 20.267    # Input the precise value, rather than the rounded one given in Summary
n <- 33
p <- 2

p.value <- 1 - pf(F, df1 = p - 1, df2 = n - p)

cat("The p-value is", p.value)
``` 




## Output: F-statistic and p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_3.png){width=80%}
:::

::::


- We can also find critical value $F_{1,31} (0.05)$, by finding its closest values in [Table 3](files/Statistics_Tables.pdf)
$$
F_{1, 30} (0.05) = 4.17 \,, \qquad \quad  F_{1, 40} (0.05) = 4.08
$$

- Approximate $F_{1,31} (0.05)$ by averaging the above values
$$
F_{1,31}(0.05) \, \approx  \,  \frac{F_{1, 30} (0.05) +  F_{1, 40} (0.05)}{2} = 4.125
$$

- We reject $H_0$, since
$$
F = 20.267 > F_{1,31}(0.05) = 4.125  \quad \implies \quad  p < 0.05
$$






# Part 6: <br>Worked Example: <br> The Longley dataset {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Longley US macroeconomic dataset {.smaller}
### 7 economical variables, observed yearly 1947--1962 (16 years)


:::: {.columns}

::: {.column width="100%"}
![](images/longley.png){width=100%}
:::

:::


::: footer

<div color="#cc0164">  </div>

:::





##  {.smaller}

::: {style="font-size: 0.92em"}

- The Longley dataset was analyzed by J.W. Longley in his 1967 paper [@longley]

- It is stored in R by default, in the variable ``longley``

- The first line of ``help(longley)`` reads:
    * *A macroeconomic data set which provides a well-known example for a highly collinear regression*

- We will see next time what *highly collinear* means

- For convenience, you can also download the Longley dataset here [longley.txt](datasets/longley.txt)

- Download the file and place it in current working directory

- Let us load the dataset and read the first 3 lines

```{r}
#| echo: true
longley <- read.table(file = "datasets/longley.txt", header = TRUE)

head(longley, n = 3)     # Prints the first 3 rows of dataset
```

:::




## {.smaller}


```{r}
longley <- read.table(file = "datasets/longley.txt", header = TRUE)

head(longley, n = 3)     # Prints the first 3 rows of dataset
```

::: {style="font-size: 0.30em"}

<br>

:::

| #   | Name            | Description |
|-----|----------------|-------------|
| $X_2$  | GNP.deflator   | Coefficient for adjusting GNP for inflation ($1954 = 100$)  (1954 is reference year: other years need adjusting) |
| $X_3$  | GNP           | Yearly Gross National Product (in Billion Dollars)  |
| $X_4$  | Unemployed    | Number of unemployed people (in thousands)  |
| $X_5$  | Armed.Forces  | Number of people in the Armed Forces (in thousands) |
| $X_6$  | Population    | Non-institutionalized Population $\geq$ age 14 (in thousands) (People $\geq 14$ not in Armed Forces or care of institutions) |
| $X_7$  | Year          | The year in which the data is recorded. Range: 1947–1962 |
| $Y$   | Employed      | Number of employed people (in thousands) |
: {tbl-colwidths="[5,10,85]"}




## Analyzing the Longley Dataset {.smaller}

**Goal:** Explain the number of *Employed* people $Y$ in terms of $X_i$ variables

- Longley dataset available here [longley.txt](datasets/longley.txt)

- Download the file and place it in current working directory

```r
# Read data file
longley <- read.table(file = "longley.txt", header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]        # GNP Deflator
x3 <- longley[ , 2]        # GNP
x4 <- longley[ , 3]        # Unemployed
x5 <- longley[ , 4]        # Armed Forces
x6 <- longley[ , 5]        # Population
x7 <- longley[ , 6]        # Year
y <- longley[ , 7]         # Employed
```


## Fitting multiple regression {.smaller}

- We want to fit the multiple regression model

$$
Y = \beta_1 + \beta_2 \, X_2 + \beta_3 \, X_3 + \beta_4 \, X_4 + \beta_5 \, X_5
    + \beta_6 \, X_6 + \beta_7 \, X_7 + \e
$$


```r
# Fit multiple regression model
model <- lm(y ~ x2 + x3 + x4 + x5 + x6 + x7)

# Print summary
summary(model)
```

<br>

- Full code can be downloaded here [longley_regression.R](codes/longley_regression.R)

- Output is in the next slide


##  {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/longley_output.png){width=73%}
:::

::::

::: footer

<div color="#cc0164">  </div>

:::




## Interpreting R output {.smaller}

We are interested in the following information:

1. Coefficient of determination $R^2$

2. Individual t-statistics for the parameters $\beta_i$

3. F-statistic to assess overall significance of parameters $\beta_i$




## 1. Coefficient of determination $R^2$ {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/longley_output_1.png){width=85%}
:::

::::

- We have that $R^2=0.9955$ 

- $R^2$ is very high, which suggests we might have quite a good model

- This means the model explains around 99.6\% of the variability in the data

- There is a chance $R^2$ is too high to be true  
(This means the model is *overfitting* - More on this later)




## 2. Individual t-statistics {.smaller}

- Look at p-values for each variable $X_j$

- Recall: Such p-values refer to the two-sided t-test

$$
H_0 \colon \, \beta_j = 0 \qquad  \quad
H_1 \colon \, \beta_j \neq 0
$$


::: {.column width="48%"}

- Find $X_j$ for which $p < 0.05$
    * For $X_j$ we reject $H_0$
    * Therefore $\beta_j \neq 0$
    * Hence $X_j$ influences $Y$ 
    * $X_j$ is statistically significant

:::

::: {.column width="48%"}

- In dissertations $p < 0.1$ is OK
    * Weak evidence against $H_0$
    * $X_j$ has some weak effect on $Y$

:::






##  {.smaller}

::: {.column width="100%"}
![](images/longley_output_3.png){width=83%}
:::

<br>

**Stars in output help you find the significant p-values**

<br>

| Significance code       | p-value               |  Coefficient        |
|:------------------    |:----------              |:-----------------   |
|  No stars             |  $0.1 \leq p \leq 1$    |  $\beta_j = 0$      |
|  $\texttt{.}$         |  $0.05 \leq p < 0.1$    |  $\beta_j = 0$      |
|  $\texttt{*}$         |  $0.1 \leq p < 0.05$    |  $\beta_j \neq 0$   |
|  $\texttt{**}$        |  $0.001 \leq p < 0.01$  |  $\beta_j \neq 0$   |
|  $\texttt{***}$       |  $p < 0.001$            |  $\beta_j \neq 0$   |
: {tbl-colwidths="[35,40,25]"}



##  {.smaller}

::: {.column width="100%"}
![](images/longley_output_2.png){width=68%}
:::

**Interpretation:** Not all variables are statistically significant

- This is because some variables have no stars 
- Significant variables have at least one star




## {.smaller}

::: {.column width="100%"}
![](images/longley_output_2.png){width=68%}
:::

1. Intercept has two stars $\, \texttt{**}$ and hence is significant
    * The p-value is $p = 0.00356$
    * Since $p < 0.05$, we conclude that the real intercept is $\beta_1 \neq 0$
    * Estimated intercept is $\hat \beta_1 = -3.482 \times 10^{3}$




## {.smaller}

::: {.column width="100%"}
![](images/longley_output_2.png){width=68%}
:::


2. $X_2, X_3, X_6$ have no stars
    * p-values are $p \geq 0.1$ and therefore the real $\beta_2 = \beta_3 = \beta_6 = 0$
    * Notice that the ML Estimators $\hat{\beta}_2, \hat{\beta}_3, \hat{\beta}_6$ are not zero
    * However, these estimates have to be ignored as t-test is not significant
    * $X_2, X_3, X_6$ have no effect on $Y$
    * Hence *GNP Deflator, GNP* and *Population* do not affect *Number of Employed*




##  {.smaller}

::: {.column width="100%"}
![](images/longley_output_2.png){width=68%}
:::


3. $X_4$ has two stars $\, \texttt{**}$, and hence is significant
    * Since $p<0.05$ we have that $\beta_4 \neq 0$
    * Estimated coefficient is $\hat \beta_4 < 0$
    * Since $\hat \beta_4 < 0$, we have that $X_4$ negatively affects $Y$
    * As the *Number of Unemployed* increases, the *Number of Employed* decreases




##  {.smaller}

::: {.column width="100%"}
![](images/longley_output_2.png){width=68%}
:::


4. $X_5$ has three stars $\, \texttt{***}$, and hence is significant
    * Since $p<0.05$, we have that $\beta_5 \neq 0$
    * Estimated coefficient is $\hat \beta_5 < 0$
    * Therefore $X_5$ negatively affects $Y$
    * As the size of *Armed Forces* increases, the *Number of Employed* decreases



##  {.smaller}

::: {.column width="100%"}
![](images/longley_output_2.png){width=68%}
:::


5. $X_7$ has two stars $\, \texttt{**}$, and hence is significant
    * Since $p<0.05$ we have that $\beta_7 \neq 0$
    * Estimated coefficient is $\hat \beta_7 > 0$
    * Therefore $X_7$ positively affects $Y$
    * Remember that $X_7$ is the *Year*
    * The *Number of Employed* is generally increasing from 1947 to 1962




## 3. F-statistic to assess overall significance {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/longley_output_1.png){width=85%}
:::

::::

- The F-statistic is $F = 330.3$, with distribution $F_{6, 9}$

- The p-value for F-test is $p = 4.984 \times 10^{-10} < 0.05$

- Recall: F-statistic and p-value refer to the F-test for

$$
H_0 \colon \, \beta_2 =  \ldots = \beta_6 = 0  \,, \qquad 
H_1 \colon \, \text{At least one } \beta_j \neq 0
$$

- Since $p < 0.05$, we reject $H_0$

- There is evidence that at least one of the $X$-variables affects $Y$



## Conclusions {.smaller}


- From F-test: There is evidence that at least one of the $X$-variables affects $Y$

- From individual t-tests, we have:
    * $X_2, X_3, X_6$ do not affect $Y$
    * $X_4$ and $X_5$ negatively affect $Y$
    * $X_7$ positively affects $Y$

- Coefficient of determination $R^2$ is really high:
    * The model explains around 99.6% of the variability in the data



::: {.content-hidden}

**Question:** Do you think the high $R^2$ statistic is suspicious?




## Observations {.smaller}

- $X_3$ and $X_4$ are *Unemployment Rate* and *Number of Unemployed*
    * These quantities are clearly equivalent 
    * Do we need to include both in the model? 

- $X_3$ and $X_4$ negatively affect the *Number of Employed*
    * Number of Employed and Unemployed are clearly inversely proportional
    * No wonder $X_3, X_4$ and $Y$ are negatively correlated!
    * $X_3$ and $X_4$ provide no meaningful explanation for $Y$!

- $X_6$ is the yearly trend
    * We saw that $X_6$ positively affects the *Number of Employed*
    * This just means the *Number of Employed* is increasing between 1947 and 1962


## Answer {.smaller}

The model does really well because it is **trivial**

- It tells us that the *Number of Employed* is increasing between 1947 and 1962
    * This is clear just by looking at data for $Y$

- It tells us that *Employment* and *Unemployment* are inversely proportional
    * Thanks a lot!

:::







# Part 7: <br> Model selection <br> {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Testing regression parameters {.smaller}

**Summary:** We have seen

- **t-test**
    * Test the significance of individual parameters
    \begin{align*}
    H_0 & \colon \, \beta_j = 0 \\
    H_1 & \colon \, \beta_j \neq 0
    \end{align*}


## Testing regression parameters {.smaller}

- **F-test**
    * Test the overall significance of the model
    * This is done by comparing two **nested** regression models
    \begin{align*}
    \textbf{Model 1:} & \qquad Y_ i= \beta_1 + \e_i \\[10pt]
    \textbf{Model 2:} & \qquad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_p x_{p, i} + \e_i
    \end{align*}
    * The comparison is achieved with F-test for
    \begin{align*}
    H_0 & \colon \, \beta_2 = \beta_3 = \ldots = \beta_p = 0 \\
    H_1 & \colon \text{ At least one of the } \beta_i \text{ is non-zero}
    \end{align*}
    * Choosing the Full Model 2 is equivalent to rejecting $H_0$



## More general nested models {.smaller}

- Consider the more general **nested** models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \beta_{k + 1} x_{k + 1, i} + 
    \ldots + \beta_{p} x_{p, i} + \e_i
\end{align*}


- Model 1 has k parameters

- Model 2 has p parameters with $p > k$

- The two models coincide if

$$
\beta_{k + 1} = \beta_{k + 2} = \ldots = \beta_p = 0 
$$

**Question:** How do we decide which model is better?


## Model selection {.smaller}

- Consider the more general **nested** models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \beta_{k + 1} x_{k + 1, i} + 
    \ldots + \beta_{p} x_{p, i} + \e_i
\end{align*}


- Define the predictions for the two models

\begin{align*}
\hat y_i^1 & := \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} \\[10pt]
\hat y_i^2 & :=  \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \beta_{k + 1} x_{k + 1, i} + 
    \ldots + \beta_{p} x_{p, i}
\end{align*}




## Model selection {.smaller}

- Consider the more general **nested** models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \beta_{k + 1} x_{k + 1, i} + 
    \ldots + \beta_{p} x_{p, i} + \e_i
\end{align*}

- $\RSS$ measures variation between data and prediction

\begin{align*}
    \textbf{Model 1:} & \quad \RSS_1 := \RSS (k) = \sum_{i=1}^n (y_i - \hat y_i^1)^2 \\[10pt]
    \textbf{Model 2:} & \quad \RSS_2 := \RSS (p) = \sum_{i=1}^n (y_i - \hat y_i^2)^2
\end{align*}



## Extra sum of squares {.smaller}

- Consider the more general **nested** models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \beta_{k + 1} x_{k + 1, i} + 
    \ldots + \beta_{p} x_{p, i} + \e_i
\end{align*}


- The **extra sum of squares** is the difference

$$
\RSS_1 - \RSS_2 := \RSS (k) - \RSS (p)
$$



## Construction of F-statistic {.smaller}

**Goal:** Use $\RSS$ to construct statistic to compare the 2 models

- Suppose the extra parameters of Model 2 
$$
\beta_{k+1}, \, \beta_{k+2} , \, \ldots , \, \beta_p
$$
are not important

- Hence the predictions of the 2 models will be similar
$$
\hat y_i^1 \, \approx \, \hat y_i^2
$$

- Therefore the $\RSS$ for the 2 models are similar
$$
\RSS_1 \, \approx \, \RSS_2
$$


## Construction of F-statistic {.smaller}

- Recall that $\RSS$ cannot increase if we increase parameters

$$
k < p \quad \implies \quad \RSS (k) \geq \RSS (p)
$$


- To measure influence of extra parameters 
$$
\beta_{k+1}, \, \beta_{k+2} , \, \ldots , \, \beta_p
$$
we consider the ratio
$$
\frac{ \RSS_1 - \RSS_2 }{ \RSS_2 } = \frac{ \RSS (k) - \RSS (p) }{ \RSS(p) }
$$



## Construction of F-statistic {.smaller}

- We now suitably rescale

$$
\frac{ \RSS_1 - \RSS_2 }{ \RSS_2 }
$$

- Note that the degrees of freedom are
    * Model 1:
    $$
    k \text{ parameters } \quad \implies \quad \df_1 =  n - k
    $$
    * Model 2:
    $$
    p \text{ parameters } \quad \implies \quad \df_2 = n - p
    $$


## F-statistic for model selection {.smaller}

::: Definition

The F-statistic for model selection is

\begin{align*}
F & = \frac{ \RSS_1 - \RSS_2 }{ \df_1 - \df_2  } \bigg/
      \frac{ \RSS_2 }{ \df_2  }  \\[20pt]
 & = \frac{ \RSS(k) - \RSS(p) }{ p - k  } \bigg/
      \frac{ \RSS(p) }{ n - p  }  
\end{align*}

:::

**Theorem:** The F-statistic for model selection has F-distribution

$$
F \, \sim  \, F_{\df_1 - \df_2 , \, \df_2} = F_{p - k, \, n - p}
$$


## Rewriting the F-statistic {.smaller}

- Recall the formulas for sums of squares

$$
\TSS = \ESS(p) + \RSS(p)  \,, \qquad \quad
\TSS = \ESS(k) + \RSS(k)  
$$

- **Note:** $\TSS$ does not depend on numeber of parameters

- Also define the coefficient of determination for the two models

$$
R_1^2 := R^2 (k) := \frac{ \ESS(k) }{ \TSS }
\, , \qquad \quad 
R_2^2 := R^2 (p) := \frac{ \ESS(p) }{ \TSS }
$$



## Rewriting the F-statistic {.smaller}



\begin{align*}
\RSS(k) - \RSS(p) & = \ESS(p) - \ESS(k) \\[10pt]
                  & = \TSS ( R^2(p) - R^2(k) ) \\[10pt]
                  & = \TSS ( R^2_2 - R^2_1 ) \\[20pt]
\RSS(p) & = \TSS - \ESS(p)  \\[10pt]
        & = \TSS  - \TSS \, \cdot \, R^2 (p) \\[10pt]
        & = \TSS (1 - R^2(p)) \\[10pt]
        & = \TSS (1 - R_2^2)
\end{align*}


## Rewriting the F-statistic {.smaller}

Therefore the F-statistic can be rewritten as

\begin{align*}
F & = \frac{ \RSS_1 - \RSS_2 }{ \df_1 - \df_2  } \bigg/
      \frac{ \RSS_2 }{ \df_2 }   \\[20pt]
  & = \frac{ \TSS (R^2_2 - R^2_1) }{\TSS (1 - R^2_2 )}  \, \cdot \,  \frac{n-p}{p-k} \\[20pt]
  & = \frac{ R^2_2 - R^2_1 }{1 - R^2_2}  \, \cdot \,  \frac{n-p}{p-k}
\end{align*}



## F-test for overall significance revisited {.smaller}

- The F-test for overall significance allows to select between models
\begin{align*}
    \textbf{Model 1:} & \qquad Y_ i= \beta_1 + \e_i \\[10pt]
    \textbf{Model 2:} & \qquad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_p x_{p, i} + \e_i
\end{align*}

- Model 1 has $k = 1$ parameters

- F-statistic for model selection coincides with F-statistic for overall significance

$$
F = \frac{ \RSS(1) - \RSS(p) }{ p - 1  } \bigg/
      \frac{ \RSS(p) }{ n - p  }
$$



## Summary: F-test for model selection {.smaller}

**Goal:** Choose one of the nested models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \beta_{k + 1} x_{k + 1, i} + 
    \ldots + \beta_{p} x_{p, i} + \e_i
\end{align*}


**Hypotheses:** Choosing a model is equivalent to testing

\begin{align*}
H_0 \colon & \, \beta_{k+1} = \beta_{k+2} = \ldots  = \beta_p \\[5pt]
H_1 \colon & \, \text{ At least one among } \beta_{k+1}, \ldots, \beta_p \text{ is non-zero}
\end{align*}

- $H_0$ is in favor of Model 1
- $H_1$ is in favor of Model 2



## Summary: F-test for model selection {.smaller}

- The F-statistic is

\begin{align*}
F & = \frac{ \RSS_1 - \RSS_2 }{ \df_1 - \df_2  } \bigg/
      \frac{ \RSS_2 }{ \df_2  }   \\[20pt]
  & = \frac{ R^2_2 - R^2_1 }{1 - R^2_2 }  \, \cdot \,  \frac{n-p}{p-k}
\end{align*}

- Distribution of $F$ is

$$
F \, \sim \, F_{ \df_1 - \df_2 , \, \df_2 } = F_{p-k, \, n-p}
$$



## Summary: F-test for model selection {.smaller}

- The p-value is

$$
p = P(F_{p-k,n-p} > F)
$$

- **F-test for model selection in R:**
    - Fit the two models with $\,\texttt{lm}$
    - Use the command $\, \texttt{anova} \qquad\quad$  (more on this later)


- **Alternative:**
    - Find $R^2_1$ and $R^2_2$ in summary
    - Compute F-statistic and p-value



# Part 8: <br>Examples of <br> model selection {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Examples of model selection {.smaller}

We illustrate F-test for Model Selection with 3 examples:

- Joint significance in Multiple linear Regression
- Polynomial regression 1
- Polynomial regression 2



## Example 1: Multiple linear regression {.smaller}

- Consider again the Longley dataset

```{r}
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

head(longley,n=3)
```


**Goal:** Explain the number of *Employed* people $Y$ in the US in terms of

- $X_2$ *GNP* Gross National Product
- $X_3$ number of *Unemployed*
- $X_4$ number of people in the *Armed Forces*
- $X_5$ *non-institutionalised Population* $\geq$ age 14 (not in care of insitutions)
- $X_6$ *Years* from 1947 to 1962



## Example 1: Multiple linear regression {.smaller}

**Previously:** Using t-test for parameters significance we showed that

- $X_2$ and $X_5$ do not affect $Y$
- $X_3$ and $X_4$ negatively affect $Y$
- $X_6$ positively affects $Y$

**Question:** Since $X_2$ and $X_5$ do not affect $Y$, can we exclude them from the model?


## Two competing models {.smaller}


We therefore want to select between the models:


- **Model 1:** The Reduced Model without $X_2$ and $X_5$

$$
Y = \beta_1 + \beta_3 X_3 + \beta_4 X_4 + 
       \beta_6 X_6 + \e
$$

- **Model 2:** The Full Model

$$
Y = \beta_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + 
       \beta_5 X_5 +  \beta_6 X_6 + \e
$$





## R commands for reading in the data {.smaller}

- We read the data in the same way we did earlier

- Longley dataset available here [longley.txt](datasets/longley.txt)

- Download the file and place it in current working directory

```r
# Read data file
longley <- read.table(file = "longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]
```


## R commands for fitting multiple regression {.smaller}

1. Fit the two multiple regression models

\begin{align*}
\textbf{Model 1:} & \quad Y = \beta_1 + \beta_3 X_3 + \beta_4 X_4 + 
       \beta_6 X_6 + \e \\[10pt]
\textbf{Model 2:} & \quad Y = \beta_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + 
       \beta_5 X_5 +  \beta_6 X_6 + \e
\end{align*}


```r
# Fit Model 1 and Model 2
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)
```

2. F-test for model selection is done using the command $\, \texttt{anova}$

```r
# F-test for model selection
anova(model.1, model.2, test = "F")
```


- Full code can be downloaded here [longley_selection.R](codes/longley_selection.R)




## Anova output {.smaller}


```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 0 and Model 1
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```


**Interpretation:**

- First two lines tell us which models are being compared





## Anova output {.smaller}

```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 0 and Model 1
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```

**Interpretation:**

- $\texttt{Res.Df} \,$ are the degrees of freedom of each model
    * The sample size of longley is 16
    * Model 1 has $k=4$ parameters
    * Model 2 has $p=6$ parameters
    * $\df_1 = n - k = 16 - 4 = 12   \quad \qquad \df_2 = n - p = 16 - 6 = 10$



## Anova output {.smaller}

```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 0 and Model 1
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```

**Interpretation:**

- $\texttt{Df} \,$ is difference in degrees of freedom
    * $\df_1 = 12$
    * $\df_2 = 10$
    * Therefore the difference is
    $$
    \df_1 - \df_2 = 12 - 10 = 2
    $$





## Anova output {.smaller}


```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```

**Interpretation:**

- $\texttt{RSS} \,$ is the residual sum of squares for each model
    * $\RSS_1 = 1.32336$
    * $\RSS_2 = 0.83935$

- $\texttt{Sum of Sq} \,$ is the extra sum of squares
    * $\RSS_1 - \RSS_2 = 0.48401$






## Anova output {.smaller}


```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```

**Interpretation:**

- $\texttt{F} \,$ is the F-statistic for model selection

\begin{align*}
F & = \frac{ \RSS_1 - \RSS_2 }{ \df_1 - \df_2 } \bigg/ 
      \frac{ \RSS_2 }{ \df_2 } \\
  & = \frac{ 1.32336 - 0.83935 }{ 12 - 10 } \bigg/ 
      \frac{ 0.83935 }{ 10 } =  2.8833
\end{align*}




## Anova output {.smaller}


```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```

**Interpretation:**

- $\texttt{Pr(>F)}$ is the p-value for F-test
    * $F \, \sim \, F_{\df_1 - \df_2 , \, \df_2 } = F_{2, 10}$
    * Therefore the p-value is
    $$
    p = P(F_{2,10} > F) = 0.1026
    $$




## Anova output {.smaller}


```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```

**Conclusion:**

- The p-value is $p = 0.1026 > 0.05$
- This means we cannot reject $H_0$
- Therefore the Reduced Model 1 has to be preferred
- This gives statistical evidence that $X_2$ and $X_5$ can be excluded from the model
- *GNP* and *Non-institutionalised* do not affect *Number of Employed*




## Example 2: Motion of falling bodies {.smaller}

Engraving (1546): people believed projectiles follow circular trajectories ([source](https://www.alamy.com/engraving-depicting-the-path-of-a-projectile-shown-as-a-circular-arc-rather-than-a-parabolic-arc-as-was-later-proved-to-be-the-case-by-galileo-galileo-galilei-1564-1642-an-italian-polymath-dated-16th-century-image186386912.html))



![](images/engraving.jpg){width=73%}




## {.smaller}

- 1609: Galileo proved mathematically that projectile trajectories are parabolic
    * His finding was based on empirical data
    * A ball (covered in ink) was released on an inclined plane from *Initial Height*
    * Ink mark on the floor represented the *Horizontal Distance* traveled
    * Unit of measure is *punti*  $\qquad\quad 1 \text{ punto} = 169/180 \, \text{mm}$

![](images/galileo_0.png){width=73%}




## {.smaller}

- We have access to Galileo's original data [@drake_galileo]
- Does a parabolic (quadratic) trajectory really explain the data? 
- Let's fit a polynomial regression model and find out!

![](images/galileo.png){width=73%}




## Plotting the data {.smaller}

<br>

| **Initial Height** | 100  | 200 | 300 | 450 | 600 | 800 | 1000 |
|:------------------ |:---- |:--  |:--- |:--- |:--  |:-   |:-    |
| **Horizontal Distance** | 253  | 337 | 395 | 451 | 495 | 534 | 573 |

<br>

```r
# Enter the data
height <- c(100, 200, 300, 450, 600, 800, 1000)
distance <- c(253, 337, 395, 451, 495, 534, 573)

# Scatter plot of data
plot(height, distance, pch = 16)
```


##  {.smaller}

We clearly see a parabola. 
Therefore we expect a relation of the form

$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3 \, {\rm height }^2
$$

```{r}
#| echo: false
#| fig-asp: 1

# Enter the data
height <- c(100, 200, 300, 450, 600, 800, 1000)
distance <- c(253, 337, 395, 451, 495, 534, 573)

# Scatter plot of data
plot(height, distance, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Initial height", side = 1, line = 3, cex = 2.1)
mtext("Horizontal distance", side = 2, line = 2.5, cex = 2.1)
```


## Fit linear model {.smaller}


$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } 
$$


```r
# Fit linear model
linear <- lm(distance ~ height)
summary(linear)
```

```verbatim

Multiple R-squared:  0.9264,	Adjusted R-squared:  0.9116 


```


- The coefficient of correlation is $R^2 = 0.9264$
- $R^2$ is quite high, showing that a linear model fits reasonably well




## Is a quadratic model better? {.smaller}

$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3  
\, {\rm height }^2
$$

<br>

**Note:** To specify powers we need to type $\,\, \texttt{I}$

```r
# Fit quadratic model
quadratic <- lm(distance ~ height + I( height^2 ))
summary(quadratic)
```

```verbatim

Multiple R-squared:  0.9903,	Adjusted R-squared:  0.9855 


```



- The coefficient of correlation is $R^2 = 0.9903$
- This is higher than the previous score $R^2 = 0.9264$
- The quadratic trajectory explains $99\%$ of variability in the data



## Why not try a cubic model? {.smaller}


$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3  
\, {\rm height }^2 + \beta_4 \, {\rm height }^3 
$$

<br>

```r
# Fit cubic model
cubic <- lm(distance ~ height + I( height^2 ) + I (height^3))
summary(cubic)
```
```verbatim

Multiple R-squared:  0.9994,	Adjusted R-squared:  0.9987 


```


- The coefficient of correlation is $R^2 = 0.9994$
- This is higher than the score of quadratic model $R^2 = 0.9903$
- What is going on?



## Quadratic vs cubic {.smaller}

- Which model is better: quadratic or cubic?

- Let us perform F-test for model selection


```r
# Model selection
anova(quadratic, cubic, test = "F")
```

```verbatim

Analysis of Variance Table

Model 1: distance ~ height + I(height^2)
Model 2: distance ~ height + I(height^2) + I(height^3)
  Res.Df    RSS Df Sum of Sq     F  Pr(>F)   
1      4 744.08                              
2      3  48.25  1    695.82 43.26 0.00715 **
```


## Model selection: quadratic Vs cubic {.smaller}

- The F-test is significant since $p = 0.007 < 0.05$

- This means we should reject the null hypothesis that 

$$
\beta_4 = 0
$$

- Therefore the quadratic model does not describe the data well

- The underlying relationship from Galileo’s data is cubic and not quadratic

- Probably the *inclined plane* introduced drag

- Code can be downloaded here [galileo.R](codes/galileo.R)



## Plot: Quadratic Vs Cubic {.smaller}

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Enter the data
height <- c(100, 200, 300, 450, 600, 800, 1000)
distance <- c(253, 337, 395, 451, 495, 534, 573)

# Scatter plot of data
plot(height, distance, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Initial height", side = 1, line = 3, cex = 2.1)
mtext("Horizontal distance", side = 2, line = 2.5, cex = 2.1)

# Fit quadratic model
quadratic <- lm(distance ~ height + I( height^2 ))

# Fit cubic model
cubic <- lm(distance ~ height + I( height^2 ) + I (height^3))

# Plot quadratic Vs Cubic
polynomial <- Vectorize(function(x, ps) {
  n <- length(ps)
  sum(ps*x^(1:n-1))
}, "x")
curve(polynomial(x, coef(quadratic)), add=TRUE, col = "red", lwd = 2)
curve(polynomial(x, coef(cubic)), add=TRUE, col = "blue", lty = 2, lwd = 2)
legend("topleft", legend = c("quadratic", "cubic"), 
       col = c("red", "blue"), lty = c(1,2), lwd = 2, cex = 2.5)
```


## Why not try higher degree polynomials {.smaller}

$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3  
\, {\rm height }^2 + \beta_4 \, {\rm height }^3 
+ \beta_5 \, {\rm height }^4 
$$

<br>

```r
# Fit quartic model
quartic <- lm(distance ~ height + I( height^2 ) + I (height^3) 
                                                + I (height^4))
summary(quartic)
```
```verbatim

Multiple R-squared:  0.9998,	Adjusted R-squared:  0.9995


```

- We obtain a coefficient $R^2 = 0.9998$
- This is even higher than cubic model coefficient $R^2 =  0.9994$
- Is the quartic model actually better?



## Model selection: cubic Vs quartic {.smaller}


```r
# Model selection
anova(cubic, quartic, test = "F")
```

```verbatim

Analysis of Variance Table

Model 1: distance ~ height + I(height^2) + I(height^3)
Model 2: distance ~ height + I(height^2) + I(height^3) + I(height^4)
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1      3 48.254                           
2      2 12.732  1    35.522 5.5799  0.142


```

- The F-test is not significant since $p = 0.142 > 0.05$

- This means we cannot reject the null hypothesis that $\beta_5 = 0$

- The cubic models does better than quartic, despite higher $R^2$

- The underlying relationship from Galileo’s data is indeed cubic!






## Example 3: Divorces {.smaller}


- Data from **Daily Mirror** gives 
    * Percentage of divorces caused by adultery VS years of marriage

- Original analysis claimed
    * Divorce-risk peaks at year 2 then decreases thereafter

- Is this conclusion misleading?
    * Does a quadratic model offers a better fit than a straight line model?
    


## Divorces dataset {.smaller}
### Percent of divorces caused by adultery by year of marriage

<br>

| **Years of Marriage**              | 1   |  2   |  3  |  4  | 5  |   6  |  7  |
|:---------------------------------- |:--- |:-    |:--  |:--  |:-- |:---  |:--  |
| **\% divorces adultery**        | 3.51| 9.50 | 8.91| 9.35|8.18| 6.43 | 5.31|
: {tbl-colwidths="[30,10,10,10,10,10,10,10]"}


<br>


| **Years of Marriage**              | 8   |  9   | 10  | 15  |20  |  25  | 30  |
|:---------------------------------- |:--- |:-    |:--  |:--  |:-- |:---  |:--  |
| **\% divorces adultery**        | 5.07| 3.65 | 3.80| 2.83|1.51| 1.27 | 0.49|
: {tbl-colwidths="[30,10,10,10,10,10,10,10]"}





## Plot: Years of Marriage Vs Divorce-risk {.smaller}

::: {.column width="48%"}

- Looks like: Divorce-risk is
    * First low, 
    * then peaks at year 2
    * then decreases

- Change of trend suggests: 
    * Higher order model might be good fit
    * Consider quadratic model 


:::

::: {.column width="48%"}
```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Scatter plot of data
plot(year, percent, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Years of marriage", side = 1, line = 3, cex = 2.1)
mtext("Risk of divorce by adultery", side = 2, line = 2.5, cex = 2.1)
```
:::


## Fitting linear model  {.smaller}

```r
# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Fit linear model
linear <- lm(percent ~ year)
summary(linear)
```

```verbatim

            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.88575    0.78667  10.024 3.49e-07 ***
year        -0.27993    0.05846  -4.788 0.000442 ***


```

- t-test for $\beta_2$ is significant since $p = 0.0004 < 0.05$
- Therefore $\beta_2 \neq 0$ and the estimate is $\hat \beta_2 = -0.27993$
- The risk of divorce decreases with years of marriage (because $\hat \beta_2 < 0$)



## Fitting quadratic model  {.smaller}

- Linear model offered a reasonable explanation of the divorce data

- Is quadratic model better?


```r
# Fit quadratic model
quadratic <- lm(percent ~ year + I( year^2 ))
summary(quadratic)
```

```verbatim

             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  8.751048   1.258038   6.956  2.4e-05 ***
year        -0.482252   0.235701  -2.046   0.0654 .  
I(year^2)    0.006794   0.007663   0.887   0.3943 


```

- t-test for $\beta_3$ is not significant since $p = 0.3943 > 0.05$

- Cannot reject null hypothesis $\beta_3 = 0 \quad \implies \quad$ Quadratic term not needed! 

- The original analysis in the **Daily Mirror is probably mistaken**




## Model selection: Linear Vs Quadratic {.smaller}

- We concluded that a linear model is better fit

- To cross check this result we do F-test for model selection

```r
# Model selection
anova(linear, quadratic, test = "F")
```

```verbatim

Model 1: percent ~ year
Model 2: percent ~ year + I(year^2)
  Res.Df    RSS Df Sum of Sq     F Pr(>F)
1     12 42.375                          
2     11 39.549  1     2.826 0.786 0.3943


```


- F-test is not significant since $p = 0.3943 > 0.05$
- We cannot reject the null hypothesis that $\beta_3 = 0$
- Quadratic model is worse than linear model


## Conclusions {.smaller}

- Daily Mirror Claim: Divorce-risk peaks at year 2 then decreases thereafter
    * Claim suggests higher order model needed to explain change in trend


- Analysis conducted: 
    * Fit linear and quadratic regression models 
    * t-test of significance discarded quadratic term
    * F-test for model selection discarded Quadratic model

- Findings: Claims in Daily Mirror are misleading
    * Linear model seems to be better than quadratic
    * This suggests divorce-risk generally decreases over time
    * Peak in year 2 can be explained by unusually low divorce-risk in 1st year

- Code is available here [divorces.R](codes/divorces.R)


## {.smaller}

- Visual confirmation: Linear model is better and divorce-risk is decreasing

- Peak in year 2 should be explained by unusually low divorce-risk in 1st year


```{r} 
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Fit linear model
linear <- lm(percent ~ year)

# Fit quadratic model
quadratic <- lm(percent ~ year + I( year^2 ))

# Scatter plot of data
plot(year, percent, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Years of marriage", side = 1, line = 3, cex = 2.1)
mtext("Risk of divorce by adultery", side = 2, line = 2.5, cex = 2.1)

# Plot Linear Vs Quadratic
polynomial <- Vectorize(function(x, ps) {
  n <- length(ps)
  sum(ps*x^(1:n-1))
}, "x")
curve(polynomial(x, coef(linear)), add=TRUE, col = "red", lwd = 2)
curve(polynomial(x, coef(quadratic)), add=TRUE, col = "blue", lty = 2, lwd = 2)
legend("topright", legend = c("Linear", "Quadratic"), 
       col = c("red", "blue"), lty = c(1,2), cex = 3, lwd = 3)
```



## Why not try higher order polynomials {.smaller}

- Let us compare Linear model with Order 6 Model

```r
# Fit order 6 model
order_6 <- lm(percent ~ year  + I( year^2 ) + I( year^3 ) + 
                              + I( year^4 ) + I( year^5 ) +
                              + I( year^6 ))

# Model selection
anova(linear, order_6)
```

```verbatim

Model 1: percent ~ year
Model 2: percent ~ year + I(year^2) + I(year^3) + I(year^4) + I(year^5) + 
    +I(year^6)
  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   
1     12 42.375                                
2      7  3.724  5    38.651 14.531 0.001404 **
```


## Why not try higher order polynomials {.smaller}


- F-test is significant since $p = 0.001 < 0.05$

- This means we reject the null hypothesis that

$$
\beta_3 = \beta_4 = \beta_5 = \beta_6 = 0
$$

- The Order 6 model is better than the Linear model

- Peak divorce-rate in Year 2 is well explained by order 6 regression

- What is going on? Let us plot the regression functions




## {.smaller}



::: {.column width="45%"}

<br>

- There are more peaks: 
    * Decreasing risk of divorce for 23 years
    * But it gets boring after 27 years!

- **Model overfits**: 
    * Data is very well explained
    * but predictions are not realistic

- Linear model should be preferred

:::


::: {.column width="46%"}

```{r} 
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Fit linear model
linear <- lm(percent ~ year)

# Fit order 6 model
order_6 <- lm(percent ~ year  + I( year^2 ) + I( year^3 ) + 
                                I( year^4 ) + I( year^5 ) +
                                I( year^6 ))

# Scatter plot of data
plot(year, percent, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Years of marriage", side = 1, line = 3, cex = 2.1)
mtext("Risk of divorce by adultery", side = 2, line = 2.5, cex = 2.1)

# Plot Linear Vs Quadratic
polynomial <- Vectorize(function(x, ps) {
  n <- length(ps)
  sum(ps*x^(1:n-1))
}, "x")
curve(polynomial(x, coef(linear)), add=TRUE, col = "red", lwd = 2)
curve(polynomial(x, coef(order_6)), add=TRUE, col = "blue", lty = 2, lwd = 3)
legend("topright", legend = c("Linear", "Order 6"), 
       col = c("red", "blue"), lty = c(1,2), cex = 3, lwd = 3)
```

:::




## References







