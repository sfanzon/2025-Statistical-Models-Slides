---
title: "Statistical Models"
subtitle: "Lecture 10"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Lecture 10: <br> Model Selection & <br> Regression Assumptions {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::






## Outline of Lecture 10 {.smaller}


1. Model Selection
    * Comparison of nested regression models

2. Examples of Model Selection




# Part 1: <br> Model Selection <br> {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Testing regression parameters {.smaller}

**Summary:** In Lecture 10 we have studied t-test and F-test for regression parameters:

1. **t-test:** Consider the *general linear regression model*

$$
Y_i = \beta_1 z_{i1} + \ldots + \beta_{ip} z_{ip} + \e_i \,, \qquad 
\e_i \, \text{ iid } \, N(0, \sigma^2)
$$

- We have that 

$$
Z_j \,\, \text{ affects } \,\, Y \qquad \iff \qquad
\beta_j \neq 0
$$

- To see if $Z_j$ affects $Y$, we introduced **t-test** for the hypothesis

\begin{align*}
H_0 \colon  & \beta_j = 0 \\
H_1 \colon & \beta_j \neq 0
\end{align*}



## Testing regression parameters {.smaller}

2. **F-test:** We considered two **nested** multiple regression models 
    \begin{align*}
    \textbf{Model 1:} & \qquad Y_ i= \beta_1 + \e_i \\[10pt]
    \textbf{Model 2:} & \qquad Y_ i= \beta_1 + \beta_2 x_{i2}+ \ldots + \beta_p x_{ip} + \e_i
    \end{align*}

- We tested the **Overall Significance** of the parameters $\beta_2 , \ldots, \beta_p$
    * This means choosing which model makes better predictions

- The comparison is achieved with **F-test** for the hypothesis

\begin{align*}
H_0 & \colon \, \beta_2 = \beta_3 = \ldots = \beta_p = 0 \\
H_1 & \colon \text{ At least one of the } \beta_i \text{ is non-zero}
\end{align*}




## More general nested models {.smaller}

Consider the more general **nested** multiple regression models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{i2}+ \ldots + \beta_{k} x_{ik} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{i2}+ \ldots + \beta_{k} x_{ik} + \beta_{k + 1} x_{i(k + 1)} + 
    \ldots + \beta_{p} x_{ip} + \e_i
\end{align*}


- Model 1 has k parameters

- Model 2 has p parameters, with $p > k$

- The two models coincide if and only if all the extra parameters are zero

$$
\beta_{k + 1} = \beta_{k + 2} = \ldots = \beta_p = 0 
$$



## Model Selection {.smaller}

Consider the more general **nested** multiple regression models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{i2}+ \ldots + \beta_{k} x_{ik} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{i2}+ \ldots + \beta_{k} x_{ik} + \beta_{k + 1} x_{i(k + 1)} + 
    \ldots + \beta_{p} x_{ip} + \e_i
\end{align*}

**Question:** How do we decide which model is better?

**Answer:** Test hypothesis for *Model Selection*

\begin{align*}
H_0 \colon & \, \beta_{k+1} = \beta_{k+2} = \ldots  = \beta_p = 0 \\[5pt]
H_1 \colon & \, \text{ At least one among } \beta_{k+1}, \ldots, \beta_p \text{ is non-zero}
\end{align*}

- $H_0$ is in favor of Model 1 $\qquad \quad H_1$ is in favor of Model 2





## Comparing the two Models {.smaller}

Consider the more general **nested** multiple regression models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{i2}+ \ldots + \beta_{k} x_{ik} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{i2}+ \ldots + \beta_{k} x_{ik} + \beta_{k + 1} x_{i(k + 1)} + 
    \ldots + \beta_{p} x_{ip} + \e_i
\end{align*}

**Goal:** Formulate a statistic to decide between the 2 Models

- First, calculate ML Estimators $\hat{\beta}^1$ and $\hat{\beta}^2$ for Models 1 and 2, respectively

- With the MLEs, define the predictions for the two models

\begin{align*}
\hat y_i^1 & := \hat{\beta}_1^{1} + \hat{\beta}_2^1 x_{i2}+ \ldots + \hat{\beta}_{k}^1 x_{ik} \\[10pt]
\hat y_i^2 & :=  \hat{\beta}_1^2 + \hat{\beta}_2^2 x_{i2}+ \ldots + \hat{\beta}_{k}^2 x_{ik} + \hat{\beta}_{k + 1}^2 x_{i(k + 1)} + 
    \ldots + \hat{\beta}_{p}^2 x_{ip}
\end{align*}




## Comparing the two Models {.smaller}

Consider the more general **nested** multiple regression models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{i2}+ \ldots + \beta_{k} x_{ik} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{i2}+ \ldots + \beta_{k} x_{ik} + \beta_{k + 1} x_{i(k + 1)} + 
    \ldots + \beta_{p} x_{ip} + \e_i
\end{align*}

- Recall: The $\RSS$ measures variation between data and prediction

- Compute the $\RSS$ for both models

\begin{align*}
    \textbf{Model 1:} & \quad \RSS_1 := \RSS (k) = \sum_{i=1}^n (y_i - \hat y_i^1)^2 \\[10pt]
    \textbf{Model 2:} & \quad \RSS_2 := \RSS (p) = \sum_{i=1}^n (y_i - \hat y_i^2)^2
\end{align*}



## Extra sum of squares {.smaller}

::: Definition 

The **extra sum of squares** is the difference

$$
\RSS_1 - \RSS_2 := \RSS (k) - \RSS (p)
$$


:::

**Recall:** $\RSS$ cannot increase if we increase the number of parameteres
$$
k < p \quad \implies \quad \RSS (k) \geq \RSS (p)
$$
(this is because $\RSS$ is defined via minimization, as already remarked in Lecture 10)

**Remark:** In particular, we deduce that the *Extra Sum of Squares* is non-negative
$$
\RSS_1 - \RSS_2 \geq 0
$$




## Deciding between the two models {.smaller}

::: {style="font-size: 0.95em"}

The hypothesis for deciding between the two model is
\begin{align*}
H_0 \colon & \, \beta_{k+1} = \beta_{k+2} = \ldots  = \beta_p = 0 \\[5pt]
H_1 \colon & \, \text{ At least one among } \beta_{k+1}, \ldots, \beta_p \text{ is non-zero}
\end{align*}

- Suppose the null hypothesis $H_0$ holds

- In this case, the two Models are the same

- Therefore, predictions will be similar

- Hence, the $\RSS$ for the 2 models are similar

$$
\RSS_1 \, \approx \, \RSS_2
$$

:::



## Deciding between the two models {.smaller}

::: {style="font-size: 0.95em"}

The hypothesis for deciding between the two model is
\begin{align*}
H_0 \colon & \, \beta_{k+1} = \beta_{k+2} = \ldots  = \beta_p = 0 \\[5pt]
H_1 \colon & \, \text{ At least one among } \beta_{k+1}, \ldots, \beta_p \text{ is non-zero}
\end{align*}

- Suppose instead that the alternative hypothesis $H_1$ holds


- As already noted, in general it holds that
$$
\RSS_1 \geq \RSS_2
$$


- From $H_1$, we know that some of the extra parameters $\beta_{k+1} , \ldots, \beta_p$ are non-zero

- Thus, Model 2 will give better predictions $\implies \RSS_2$ is **much smaller**
$$
\RSS_1 \gg \RSS_2
$$

:::





## Conclusion: 2 cases {.smaller}

::: {style="font-size: 0.95em"}


1. Choose Model 1:

\begin{align*}
H_0 \,\, \text{ holds } & \, \iff \,  \beta_{k+1} = \beta_{k+2} = \ldots  = \beta_p = 0 \\[15pt]
 & \, \iff \, \RSS_1 \approx \RSS_2 \, \iff \, \frac{\RSS_1 - \RSS_2}{\RSS_2} \approx 0 
 \end{align*}

2. Choose Model 2:

\begin{align*} 
H_1 \,\, \text{ holds } & \, \iff \, \exists \,\, i \in \{ k+1, \ldots, p\} \, \text{ s.t.  } \, \beta_i \neq 0 \\[15pt]
 & \, \iff \, \RSS_1 \gg \RSS_2 \, \iff \, \frac{\RSS_1 - \RSS_2}{\RSS_2} \gg 0
\end{align*}

:::


## Construction of F-statistic {.smaller}

::: {style="font-size: 0.95em"}

- Thus, the quantity below can be used as statistic to decide between the 2 models

$$
\frac{\RSS_1 - \RSS_2}{\RSS_2}
$$


- However, in order to obtain a known distribution, we need to rescale



- To this end, note that the degrees of freedom are:
    * Model 1:
    $$
    k \text{ parameters } \quad \implies \quad \df_1 =  n - k
    $$
    * Model 2:
    $$
    p \text{ parameters } \quad \implies \quad \df_2 = n - p
    $$

:::







## F-statistic for Model Selection {.smaller}

::: Definition

The F-statistic for Model Selection is

\begin{align*}
F & = \frac{ \RSS_1 - \RSS_2 }{ \df_1 - \df_2  } \bigg/
      \frac{ \RSS_2 }{ \df_2  }  \\[20pt]
 & = \frac{ \RSS(k) - \RSS(p) }{ p - k  } \bigg/
      \frac{ \RSS(p) }{ n - p  }  
\end{align*}

:::

**Theorem:** The F-statistic for Model Selection has F-distribution

$$
F \, \sim  \, F_{\df_1 - \df_2 , \, \df_2} = F_{p - k, \, n - p}
$$




## Rewriting the F-statistic {.smaller}

Denote by $R^2_1$ and $R^2_2$ the coefficients of determination of the 2 models

::: Proposition

The F-statistic for Model Selection can be rewritten as

\begin{align*}
F & = \frac{ \RSS_1 - \RSS_2 }{ \df_1 - \df_2  } \bigg/
      \frac{ \RSS_2 }{ \df_2 }   \\[20pt]
  & = \frac{ R^2_2 - R^2_1 }{1 - R^2_2}  \, \cdot \,  \frac{n-p}{p-k}
\end{align*}

:::


## Proof of Proposition {.smaller}


- Recall the definition of coefficient of determination for the two models

$$
R_1^2 := R^2 (k) := 1 - \frac{ \RSS(k) }{ \TSS }
\, , \qquad \quad 
R_2^2 := R^2 (p) := 1 - \frac{ \RSS(p) }{ \TSS }
$$

- **Note:** $\TSS$ does not depend on numeber of parameters


- From the above, we get

$$
\RSS(k) = (1 - R_1^2) \TSS 
\, , \qquad \quad 
\RSS(p) = (1 - R_2^2) \TSS 
$$


- In particular, we obtain

\begin{align*}
\RSS(k) - \RSS(p) = ( R^2_2 - R^2_1 )  \TSS
\end{align*}



##  {.smaller}

- Therefore, the F-statistic can be rewritten as

\begin{align*}
F & = \frac{ \RSS_1 - \RSS_2 }{ \df_1 - \df_2  } \bigg/
      \frac{ \RSS_2 }{ \df_2 }   \\[20pt]
  & = \frac{ \RSS(k) - \RSS(p) }{\RSS(p)}  \, \cdot \,  \frac{n-p}{p-k} \\[20pt]
  & = \frac{ (R^2_2 - R^2_1) \TSS }{ (1 - R^2_2) \TSS} \, \cdot \,  \frac{n-p}{p-k} \\[20pt]
  & = \frac{ R^2_2 - R^2_1 }{1 - R^2_2}  \, \cdot \,  \frac{n-p}{p-k}
\end{align*}

- The proof is concluded






## Test for Model Selection {.smaller}

- **Recall:** We want to decide between the 2 models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{i2}+ \ldots + \beta_{k} x_{ik} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{i2}+ \ldots + \beta_{k} x_{ik} + \beta_{k + 1} x_{i(k + 1)} + 
    \ldots + \beta_{p} x_{ip} + \e_i
\end{align*}


- To make a decision, we have formulated the hypothesis

\begin{align*}
H_0 \colon & \, \beta_{k+1} = \beta_{k+2} = \ldots  = \beta_p = 0 \\[5pt]
H_1 \colon & \, \text{ At least one among } \beta_{k+1}, \ldots, \beta_p \text{ is non-zero}
\end{align*}

- $H_0$ is in favor of Model 1 $\qquad \quad H_1$ is in favor of Model 2




## {.smaller}


::: {style="font-size: 0.93em"}

We use the F-statistic to decide between the two models
$$
F = \frac{ \RSS(k) - \RSS(p) }{ p - k  } \bigg/
      \frac{ \RSS(p) }{ n - p  } 
$$


1. Choose Model 1:
\begin{align*}
H_0 \,\, \text{ holds }  & \, \iff \, \beta_{k+1} = \beta_{k+2} = \ldots  = \beta_p = 0  \\[10pt]
  & \, \iff \, \RSS(k) \approx \RSS (p) \, \iff \, F \approx 0 
 \end{align*}

2. Choose Model 2:
\begin{align*} 
H_1 \,\, \text{ holds } & \, \iff \,\exists \,\, i \in \{ k+1, \ldots, p\} \, \text{ s.t.  } \, \beta_i \neq 0  \\[10pt]
  &  \, \iff \, \RSS(k) \gg \RSS (p) \, \iff \, F \gg 0
\end{align*}


- **Therefore, the test is one-sided: $\,\,$ Reject $H_0 \iff F \gg 0$**


:::





## The F-test for Model Selection {.smaller}

**Assumption:** Given data points $(x_{i2},\ldots , x_{ip},  y_i)$, consider the nested models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{i2}+ \ldots + \beta_{k} x_{ik} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{i2}+ \ldots + \beta_{k} x_{ik} + \beta_{k + 1} x_{i(k + 1)} + 
    \ldots + \beta_{p} x_{ip} + \e_i
\end{align*}

**Hypothesis:** To decide which model gives better predictions

\begin{align*}
H_0 \colon & \, \beta_{k+1} = \beta_{k+2} = \ldots  = \beta_p = 0 \\[5pt]
H_1 \colon & \, \text{ At least one among } \beta_{k+1}, \ldots, \beta_p \text{ is non-zero}
\end{align*}

- $H_0$ is in favor of Model 1 $\qquad \quad H_1$ is in favor of Model 2



## Procedure: 3 Steps {.smaller}

1. **Calculation**: Compute MLEs $\hat{\beta}^1, \hat{\beta}^2$ and predictions $\hat{y}_i^2, \hat{y}_i^2$ for the 2 models
$$
\hat{\beta}^i = (Z_i^TZ_i)^{-1} Z_i^T y \,, \qquad \hat{y}^i = Z_i \hat{\beta}^i \,, \qquad Z_i = \text{design matrix for Model } i
$$
Compute the $\TSS$. Compute $\RSS$ and $R^2$ coefficient for both models
$$
\TSS = \sum_{i=1}^n (y_i - \overline{y})^2\,, \qquad
\RSS_j = \sum_{i=1}^n (y_i - \hat{y}_i^j)^2 \,, \qquad 
R^2_j = 1 - \frac{\RSS_j}{\TSS}
$$
Finally, compute the F-statistic for *Model Selection*
$$
F = \frac{R^2_2 - R_1^2}{1 - R^2_2} \, \cdot \, \frac{n - p}{p - k} \ \sim \ F_{p-k, n - p} 
$$



## {.smaller}

2. **Statistical Tables or R**: Find either
    * Critical value $F^*$ in [Table 3](files/Statistics_Tables.pdf)
    * p-value in R

<br>

3. **Interpretation**: Reject $H_0$ when either
$$
p < 0.05 \qquad \text{ or } \qquad F \in \,\,\text{Rejection Region}
$$




| Alternative                               | Rejection Region  | $F^*$              | p-value              |
|-------------------------------------------|-------------------|--------------------|----------------------|
| $\exists \,\, i \in \{ k+1, \ldots, p\}$ s.t. $\beta_i \neq 0$ | $F > F^*$         | $F_{p-k,n-p}(0.05)$| $P(F_{p-k,n-p} > F)$ |
: {tbl-colwidths="[35,20,20,25]"}



## The F-test for Model Selection in R {.smaller}


1. Fit the two models with ``lm``
2. Compare the models with the command $\, \texttt{anova} \qquad\quad$  (more on this later)

```r
model.1 <- lm(y ~ x1 + ... + xk)
model.2 <- lm(y ~ x1 + ... + xk + x(k+1) + ... + xp)

anova(model.1, model.2)
```

**Alternative:**

- Find $R^2_1$ and $R^2_2$ in each Summary
- Compute F-statistic and p-value by hand, using formulas
    
$$
F = \frac{R^2_2 - R_1^2}{1 - R^2_2} \, \cdot \, \frac{n - p}{p - k} \,, \qquad p = P(F_{p-k,n-p} > F)
$$






## F-test for Overall Significance revisited {.smaller}

- The F-test for Overall Significance allows to select between models
\begin{align*}
    \textbf{Model 1:} & \qquad Y_ i= \beta_1 + \e_i \\[10pt]
    \textbf{Model 2:} & \qquad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_p x_{p, i} + \e_i
\end{align*}

- Model 1 has $k = 1$ parameters

- F-statistic for *Model Selection* coincides with F-statistic for *Overall Significance*

$$
F = \frac{ \RSS(1) - \RSS(p) }{ p - 1  } \bigg/
      \frac{ \RSS(p) }{ n - p  }
$$

**Model Selection and Overall Significance tests coincide in this case**





# Part 2: <br>Examples of <br> Model Selection {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Examples of Model Selection {.smaller}

We illustrate F-test for Model Selection with 3 examples:

- Joint significance in Multiple linear Regression
- Polynomial regression (Two Examples)



## Example 1: Multiple linear regression {.smaller}
### Consider again the Longley dataset

```{r}
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

head(longley,n=3)
```


::: {style="font-size: 0.30em"}

<br>

:::

**Goal:** Explain the number of *Employed* people $Y$ in the US in terms of

- $X_2$ *GNP deflator* to adjust GNP for inflation
- $X_3$ *GNP* Gross National Product
- $X_4$ number of *Unemployed*
- $X_5$ number of people in the *Armed Forces*
- $X_6$ *non-institutionalised Population* $\geq$ age 14 (not in care of insitutions)
- $X_7$ *Years* from 1947 to 1962




##  {.smaller}

**Previous Analysis:** Using t-test for parameters significance, we showed that

- $X_2, X_3$ and $X_6$ do not affect $Y$
- $X_4$ and $X_5$ negatively affect $Y$
- $X_7$ positively affects $Y$

**Question:** Since $X_2, X_3, X_6$ do not affect $Y$, can we exclude them from the model?

**Answer:** Use the F-test for Model Selection on the 2 nested models


- **Model 1:** The Reduced Model without $X_2, X_3, X_6$
$$
Y = \beta_1 +  \beta_4 X_4 +  \beta_5 X_5 + 
       \beta_7 X_7 + \e
$$

- **Model 2:** The Full Model
$$
Y = \beta_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + 
       \beta_5 X_5 +  \beta_6 X_6 +  \beta_7 X_7 + \e
$$





## R commands for reading in the data {.smaller}

- We read the data in the same way we did last time

- Longley dataset available here [longley.txt](datasets/longley.txt)

- Download the file and place it in current working directory

```r
# Read data file
longley <- read.table(file = "longley.txt", header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]        # GNP Deflator
x3 <- longley[ , 2]        # GNP
x4 <- longley[ , 3]        # Unemployed
x5 <- longley[ , 4]        # Armed Forces
x6 <- longley[ , 5]        # Population
x7 <- longley[ , 6]        # Year
y <- longley[ , 7]         # Employed
```


## R commands for F-test of Model Selection {.smaller}

1. Fit the two multiple regression models

\begin{align*}
\textbf{Model 1:} & \quad Y = \beta_1 + \beta_4 X_4 + \beta_5 X_5 + 
       \beta_7 X_7 + \e \\[10pt]
\textbf{Model 2:} & \quad Y = \beta_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + 
       \beta_5 X_5 +  \beta_6 X_6 +  \beta_7 X_7 + \e
\end{align*}


```r
# Fit Model 1 and Model 2
model.1 <- lm(y ~ x4 + x5 + x7)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6 + x7)
```

2. F-test for Model Selection is done using the command $\, \texttt{anova}$

```r
# F-test for Model Selection
anova(model.1, model.2)
```


- Full code can be downloaded here [longley_selection.R](codes/longley_selection.R)




##  {.smaller}

::: {style="font-size: 0.95em"}

```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]        # GNP Deflator
x3 <- longley[ , 2]        # GNP
x4 <- longley[ , 3]        # Unemployed
x5 <- longley[ , 4]        # Armed Forces
x6 <- longley[ , 5]        # Population
x7 <- longley[ , 6]        # Year
y <- longley[ , 7]         # Employed

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x4 + x5 + x7)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6 + x7)

# F-test for Model Selection
anova(model.1, model.2)
```


<br>

- First two lines say that the following 2 models are being compared
\begin{align*}
    \textbf{Model 1:} & \quad Y = \beta_1 + \beta_4 x_{4} + \beta_5 x_{5} + \beta_7 x_{7}  + \e \\[10pt]
    \textbf{Model 2:} &\quad Y =\beta_1 + \beta_2 x_{2}+ \beta_3 x_{3} + \beta_4 x_{4} + \beta_5 x_{5} + \beta_{6} x_{6} + \beta_7 x_{7}  + \e
\end{align*}

- The null hypothesis favors Model 1; The alternative favors Model 2
\begin{align*}
H_0 \colon & \, \beta_{2} = \beta_{3} = \beta_6 = 0 \\[5pt]
H_1 \colon & \, \text{ At least one among } \beta_{2}, \beta_{3}, \beta_6  \text{ is non-zero}
\end{align*}

:::




##  {.smaller}

::: {style="font-size: 0.95em"}

```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]        # GNP Deflator
x3 <- longley[ , 2]        # GNP
x4 <- longley[ , 3]        # Unemployed
x5 <- longley[ , 4]        # Armed Forces
x6 <- longley[ , 5]        # Population
x7 <- longley[ , 6]        # Year
y <- longley[ , 7]         # Employed

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x4 + x5 + x7)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6 + x7)

# F-test for Model Selection
anova(model.1, model.2)
```

<br>

- $\texttt{Res.Df} \,$ are the degrees of freedom of each model
    * The sample size of longley is 16
    * Model 1 has $k=4$ parameters
    * Model 2 has $p=7$ parameters
    * $\df_1 = n - k = 16 - 4 = 12   \quad \qquad \df_2 = n - p = 16 - 7 = 9$


:::


##  {.smaller}

::: {style="font-size: 0.95em"}

```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]        # GNP Deflator
x3 <- longley[ , 2]        # GNP
x4 <- longley[ , 3]        # Unemployed
x5 <- longley[ , 4]        # Armed Forces
x6 <- longley[ , 5]        # Population
x7 <- longley[ , 6]        # Year
y <- longley[ , 7]         # Employed

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x4 + x5 + x7)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6 + x7)

# F-test for Model Selection
anova(model.1, model.2)
```

<br>

- $\texttt{Df} \,$ is difference in degrees of freedom
    * $\df_1 = 12$
    * $\df_2 = 9$
    * Therefore the difference is
    $$
    \df_1 - \df_2 = 12 - 9 = 3
    $$

:::



##  {.smaller}


::: {style="font-size: 0.95em"}

```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]        # GNP Deflator
x3 <- longley[ , 2]        # GNP
x4 <- longley[ , 3]        # Unemployed
x5 <- longley[ , 4]        # Armed Forces
x6 <- longley[ , 5]        # Population
x7 <- longley[ , 6]        # Year
y <- longley[ , 7]         # Employed

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x4 + x5 + x7)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6 + x7)

# F-test for Model Selection
anova(model.1, model.2)
```


<br>

- $\texttt{RSS} \,$ is the residual sum of squares for each model
    * $\RSS_1 = 1.32336$
    * $\RSS_2 = 0.83642$

- $\texttt{Sum of Sq} \,$ is the *Extra Sum of Squares*
    * $\RSS_1 - \RSS_2 = 0.48694$


:::




##  {.smaller}

::: {style="font-size: 0.95em"}

```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]        # GNP Deflator
x3 <- longley[ , 2]        # GNP
x4 <- longley[ , 3]        # Unemployed
x5 <- longley[ , 4]        # Armed Forces
x6 <- longley[ , 5]        # Population
x7 <- longley[ , 6]        # Year
y <- longley[ , 7]         # Employed

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x4 + x5 + x7)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6 + x7)

# F-test for Model Selection
anova(model.1, model.2)
```

<br>

- $\texttt{F} \,$ is the F-statistic for Model Selection

\begin{align*}
F & = \frac{ \RSS_1 - \RSS_2 }{ \df_1 - \df_2 } \bigg/ 
      \frac{ \RSS_2 }{ \df_2 } \\
  & = \frac{ 1.32336 - 0.83642 }{ 12 - 9 } \bigg/ 
      \frac{ 0.83642 }{ 9 } =  1.7465
\end{align*}

:::




##  {.smaller}

::: {style="font-size: 0.95em"}

```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]        # GNP Deflator
x3 <- longley[ , 2]        # GNP
x4 <- longley[ , 3]        # Unemployed
x5 <- longley[ , 4]        # Armed Forces
x6 <- longley[ , 5]        # Population
x7 <- longley[ , 6]        # Year
y <- longley[ , 7]         # Employed

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x4 + x5 + x7)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6 + x7)

# F-test for Model Selection
anova(model.1, model.2)
```

<br>

- $\texttt{Pr(>F)}$ is the p-value for the F-test for Model Selection 
    * $F \, \sim \, F_{\df_1 - \df_2 , \, \df_2 } = F_{3, 9}$
    * Therefore the p-value is
    $$
    p = P(F_{3,9} > F) = 0.227
    $$

:::



##  {.smaller}

::: {style="font-size: 0.95em"}

```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]        # GNP Deflator
x3 <- longley[ , 2]        # GNP
x4 <- longley[ , 3]        # Unemployed
x5 <- longley[ , 4]        # Armed Forces
x6 <- longley[ , 5]        # Population
x7 <- longley[ , 6]        # Year
y <- longley[ , 7]         # Employed

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x4 + x5 + x7)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6 + x7)

# F-test for Model Selection
anova(model.1, model.2)
```

<br>

**Conclusion:** The p-value is $p = 0.227 > 0.05$

- This means we cannot reject $H_0$, which said that $\beta_{2} = \beta_{3} = \beta_6 = 0$ 
- Therefore the Reduced Model 1 has to be preferred
$$
Y = \beta_1 + \beta_4 x_{4} + \beta_5 x_{5} + \beta_7 x_{7}  + \e
$$

- This gives statistical evidence that $X_2, X_3,X_6$ can be excluded from the model
- *GNP Deflator*, *GNP* and *Population* do not affect the *Number of Employed*

:::




## Example 2: Motion of falling bodies {.smaller}

Engraving (1546): people believed projectiles follow circular trajectories ([source](https://www.alamy.com/engraving-depicting-the-path-of-a-projectile-shown-as-a-circular-arc-rather-than-a-parabolic-arc-as-was-later-proved-to-be-the-case-by-galileo-galileo-galilei-1564-1642-an-italian-polymath-dated-16th-century-image186386912.html))



![](images/engraving.jpg){width=73%}




## {.smaller}

- 1609: Galileo proved mathematically that projectile trajectories are parabolic
    * His finding was based on empirical data
    * A ball (covered in ink) was released on an inclined plane from *Initial Height*
    * Ink mark on the floor represented the *Horizontal Distance* traveled
    * Unit of measure is *punti*  $\qquad\quad 1 \text{ punto} = 169/180 \, \text{mm}$

![](images/galileo_0.png){width=73%}




## {.smaller}

- We have access to Galileo's original data [@drake_galileo]
- Does a parabolic (quadratic) trajectory really explain the data? 
- Let's fit a polynomial regression model and find out!

![](images/galileo.png){width=73%}




## Plotting the data {.smaller}

<br>

|                         |      |     |     |     |     |     |      |
|:------------------      |:---- |:--  |:--- |:--- |:--  |:-   |:-    |
| **Initial Height**      | 100  | 200 | 300 | 450 | 600 | 800 | 1000 |
| **Horizontal Distance** | 253  | 337 | 395 | 451 | 495 | 534 | 573  |

<br>

```r
# Enter the data
height <- c(100, 200, 300, 450, 600, 800, 1000)
distance <- c(253, 337, 395, 451, 495, 534, 573)

# Scatter plot of data
plot(height, distance, pch = 16)
```


## Data visualization {.smaller}

::: {.column width="45%"}

```{r}
#| echo: false
#| fig-asp: 1

# Enter the data
height <- c(100, 200, 300, 450, 600, 800, 1000)
distance <- c(253, 337, 395, 451, 495, 534, 573)

# Scatter plot of data
plot(height, distance, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Initial height", side = 1, line = 3, cex = 2.1)
mtext("Horizontal distance", side = 2, line = 2.5, cex = 2.1)
```

:::



::: {.column width="53%"}

::: {style="font-size: 0.93em"}

- The plot shows that a parabola might fit better than a straight line

- We can first fit the usual simple linear regression model
$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \e
$$



- Then, we can try a degree 2 polynomial (linear) regression model
$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3 \, {\rm height }^2 + \e
$$

- We can compare the 2 models with the F-test for *Model Selection*

:::

:::







## Fit simple linear model {.smaller}

$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \e
$$



<br>


```r
# Fit linear model
linear <- lm(distance ~ height)

# Print summary
summary(linear)
```




## {.smaller}

::: {style="font-size: 0.95em"}

```verbatim
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 269.71246   24.31239  11.094 0.000104 ***
height        0.33334    0.04203   7.931 0.000513 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 33.68 on 5 degrees of freedom
Multiple R-squared:  0.9264,	Adjusted R-squared:  0.9116 
F-statistic: 62.91 on 1 and 5 DF,  p-value: 0.0005132

```

<br>


- Individual t-tests show that coefficients for both **Intercept** and **height** are non-zero
     * $\beta_1 \neq 0$ because corresponding p-value is $p = 0.000104 < 0.05$
     * $\beta_2 \neq 0$ because corresponding p-value is $p = 0.000513 < 0.05$
     
- F-test for overall significance coincides with t-test for paramter $\beta_2$
     * Recall: This is always true for simple regression
     * Hence, F-test is redundant in this case

:::




##  {.smaller}

::: {style="font-size: 0.95em"}

```verbatim
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 269.71246   24.31239  11.094 0.000104 ***
height        0.33334    0.04203   7.931 0.000513 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 33.68 on 5 degrees of freedom
Multiple R-squared:  0.9264,	Adjusted R-squared:  0.9116 
F-statistic: 62.91 on 1 and 5 DF,  p-value: 0.0005132

```

<br>

- The coefficient of determination is $R^2 = 0.9264$
    * This is quite high (close to $1$)

- **Conclusion:** Every indicator shows that the linear model fits quite well

:::






## Fit quadratic model {.smaller}

$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3  
\, {\rm height }^2 + \e
$$

<br>

**Note:** To specify powers, we need to type $\,\, \texttt{I}$

<br>

```r
# Fit quadratic model
quadratic <- lm(distance ~ height + I( height^2 ))

# Print summary
summary(quadratic)
```




## {.smaller}

::: {style="font-size: 0.95em"}

```verbatim
Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.999e+02  1.676e+01  11.928 0.000283 ***
height       7.083e-01  7.482e-02   9.467 0.000695 ***
I(height^2) -3.437e-04  6.678e-05  -5.147 0.006760 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 13.64 on 4 degrees of freedom
Multiple R-squared:  0.9903,	Adjusted R-squared:  0.9855 
F-statistic:   205 on 2 and 4 DF,  p-value: 9.333e-05
```

<br>


- Individual t-tests show that all coefficients are non-zero
     * $\beta_1 \neq 0$ because corresponding p-value is $p = 0.000283 < 0.05$
     * $\beta_2 \neq 0$ because corresponding p-value is $p = 0.000695 < 0.05$
     * $\beta_3 \neq 0$ because corresponding p-value is $p = 0.006760 < 0.05$

- In particular, this shows that the *extra quadratic term* is significant


:::


## {.smaller}

::: {style="font-size: 0.95em"}

```verbatim
Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.999e+02  1.676e+01  11.928 0.000283 ***
height       7.083e-01  7.482e-02   9.467 0.000695 ***
I(height^2) -3.437e-04  6.678e-05  -5.147 0.006760 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 13.64 on 4 degrees of freedom
Multiple R-squared:  0.9903,	Adjusted R-squared:  0.9855 
F-statistic:   205 on 2 and 4 DF,  p-value: 9.333e-05
```

<br>

- F-test for overall significance gives a p-value of $p = 9.333 \times 10^{-05} < 0.05$
    * This means we reject the null hypothesis that
    $$
    \beta_2 = \beta_3 = 0
    $$
    * Hence, we have confirmation that at least one of the parameters is significant

:::




##  {.smaller}

::: {style="font-size: 0.95em"}

```verbatim
Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.999e+02  1.676e+01  11.928 0.000283 ***
height       7.083e-01  7.482e-02   9.467 0.000695 ***
I(height^2) -3.437e-04  6.678e-05  -5.147 0.006760 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 13.64 on 4 degrees of freedom
Multiple R-squared:  0.9903,	Adjusted R-squared:  0.9855 
F-statistic:   205 on 2 and 4 DF,  p-value: 9.333e-05
```

<br>


- The coefficient of determination is $R^2 = 0.9903$
    * This is quite high (close to $1$)
    * The quadratic trajectory explains $99\%$ of variability in the data

- **Conclusion:** Every indicator shows that the quadratic model fits quite well

:::




## Linear Vs Quadratic {.smaller}

- We have seen that both linear and quadratic models fit well

- In particular, the coefficients of determination are
$$
R^2 \text{ for quadratic model} = 0.9903 > R^2 \text{ for linear model} = 0.9264
$$

- This confirms that the Quadratic model offers a better fit

- However, need to confirm that Quadratic model makes **better predictions**
    * Can be done with *F-test for Model Selection*


::: {style="font-size: 0.25em"}

<br>

:::

```r
# F-test for Model Selection
anova(linear, quadratic)
```



##  {.smaller}

::: {style="font-size: 0.95em"}

```verbatim
Analysis of Variance Table

Model 1: distance ~ height
Model 2: distance ~ height + I(height^2)
  Res.Df    RSS Df Sum of Sq      F  Pr(>F)   
1      5 5671.2                               
2      4  744.1  1    4927.1 26.487 0.00676 **
```

<br>

- First two lines say that the following 2 models are being compared
\begin{align*}
    \textbf{Model 1:} & \quad {\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \e \\[10pt]
    \textbf{Model 2:} & \quad {\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3  
\, {\rm height }^2 + \e
\end{align*}

- The null hypothesis favors Model 1; The alternative favors Model 2
\begin{align*}
H_0 \colon & \,  \beta_{3}  = 0 \\[5pt]
H_1 \colon & \,  \beta_{3} \neq 0 
\end{align*}

:::


##  {.smaller}

::: {style="font-size: 0.95em"}

```verbatim
Analysis of Variance Table

Model 1: distance ~ height
Model 2: distance ~ height + I(height^2)
  Res.Df    RSS Df Sum of Sq      F  Pr(>F)   
1      5 5671.2                               
2      4  744.1  1    4927.1 26.487 0.00676 **
```

<br>


- **Note:** Hypothesis for Model Selection coincides with hypothesis of significance of $\beta_3$
\begin{align*}
H_0 \colon & \,  \beta_{3}  = 0 \\[5pt]
H_1 \colon & \,  \beta_{3} \neq 0 
\end{align*}

- In this case, t-test for significance of $\beta_3$ and F-test for Model Selection coincide

- They both give a p-value of $p = 0.00676 < 0.05$

- This means we reject $H_0 \quad \implies$  the Quadratic term is needed

- **Quadratic model has to be preferred to the linear one**

:::





## Why not try a Cubic model? {.smaller}


<br>

$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3  
\, {\rm height }^2 + \beta_4 \, {\rm height }^3 + \e
$$

<br>

```r
# Fit cubic model
cubic <- lm(distance ~ height + I( height^2 ) + I (height^3))

# Print Summary
summary(cubic)
```




## {.smaller}

::: {style="font-size: 0.93em"}

```verbatim
Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.558e+02  8.326e+00  18.710 0.000333 ***
height       1.115e+00  6.567e-02  16.983 0.000445 ***
I(height^2) -1.245e-03  1.384e-04  -8.994 0.002902 ** 
I(height^3)  5.477e-07  8.327e-08   6.577 0.007150 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 4.011 on 3 degrees of freedom
Multiple R-squared:  0.9994,	Adjusted R-squared:  0.9987 
F-statistic:  1595 on 3 and 3 DF,  p-value: 2.662e-05
```

<br>


- Individual t-tests show that all coefficients are non-zero (p-values $< 0.05$)
    * In particular, this shows that the *extra cubic term* is significant

- F-test for overall significance gives a p-value of $p = 2.662 \times 10^{-05} < 0.05$
    * We have confirmation that at least one of the parameters is non-zero

:::




##  {.smaller}

::: {style="font-size: 0.93em"}

```verbatim
Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.558e+02  8.326e+00  18.710 0.000333 ***
height       1.115e+00  6.567e-02  16.983 0.000445 ***
I(height^2) -1.245e-03  1.384e-04  -8.994 0.002902 ** 
I(height^3)  5.477e-07  8.327e-08   6.577 0.007150 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 4.011 on 3 degrees of freedom
Multiple R-squared:  0.9994,	Adjusted R-squared:  0.9987 
F-statistic:  1595 on 3 and 3 DF,  p-value: 2.662e-05
```

<br>


- The coefficient of determination is $R^2 = 0.9994$
    * This is quite high (close to $1$)

- **Conclusion:** Every indicator shows that the Cubic model fits quite well

:::





## Quadratic vs Cubic {.smaller}

- Both Quadratic and Cubic models fit well

- In particular, the coefficients of determination are
$$
R^2 \text{ for Cubic model} = 0.9994 > R^2 \text{ for Quadratic model} =  0.9903
$$

- This confirms that the Cubic model offers a better fit
    * Falling bodies follow cubic trajectories???
    * What is going on? 

- Need to confirm that Cubic model makes **better predictions**
    * Can be done with *F-test for Model Selection*


::: {style="font-size: 0.25em"}

<br>

:::

```r
# F-test for Model Selection
anova(quadratic, cubic)
```




##  {.smaller}

::: {style="font-size: 0.93em"}

```verbatim

Analysis of Variance Table

Model 1: distance ~ height + I(height^2)
Model 2: distance ~ height + I(height^2) + I(height^3)
  Res.Df    RSS Df Sum of Sq     F  Pr(>F)   
1      4 744.08                              
2      3  48.25  1    695.82 43.26 0.00715 **
```


<br>

- First two lines say that the following 2 models are being compared
\begin{align*}
    \textbf{Model 1:} & \quad {\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \e \\[10pt]
    \textbf{Model 2:} & \quad {\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3  
\, {\rm height }^2  + \beta_4  
\, {\rm height }^3 + \e
\end{align*}

- The null hypothesis favors Model 1; The alternative favors Model 2
\begin{align*}
H_0 \colon & \,  \beta_{4}  = 0 \\[5pt]
H_1 \colon & \,  \beta_{4} \neq 0 
\end{align*}

:::





##  {.smaller}

::: {style="font-size: 0.93em"}

```verbatim

Analysis of Variance Table

Model 1: distance ~ height + I(height^2)
Model 2: distance ~ height + I(height^2) + I(height^3)
  Res.Df    RSS Df Sum of Sq     F  Pr(>F)   
1      4 744.08                              
2      3  48.25  1    695.82 43.26 0.00715 **
```

<br>

- **Note:** Hypothesis for Model Selection coincides with hypothesis of significance of $\beta_4$
\begin{align*}
H_0 \colon & \,  \beta_{4}  = 0 \\[5pt]
H_1 \colon & \,  \beta_{4} \neq 0 
\end{align*}

- In this case, t-test for significance of $\beta_4$ and F-test for Model Selection coincide

- They both give a p-value of $p = 0.00715 < 0.05$

- This means we reject $H_0 \quad \implies$ the Cubic term is needed

- **Cubic model has to be preferred to the Quadratic one**

:::



## Conclusion {.smaller}


::: {.column width="48%"}

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Enter the data
height <- c(100, 200, 300, 450, 600, 800, 1000)
distance <- c(253, 337, 395, 451, 495, 534, 573)

# Scatter plot of data
plot(height, distance, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Initial height", side = 1, line = 3, cex = 2.1)
mtext("Horizontal distance", side = 2, line = 2.5, cex = 2.1)

# Fit quadratic model
quadratic <- lm(distance ~ height + I( height^2 ))

# Fit cubic model
cubic <- lm(distance ~ height + I( height^2 ) + I (height^3))

# Plot quadratic Vs Cubic
polynomial <- Vectorize(function(x, ps) {
  n <- length(ps)
  sum(ps*x^(1:n-1))
}, "x")
curve(polynomial(x, coef(quadratic)), add=TRUE, col = "red", lwd = 2)
curve(polynomial(x, coef(cubic)), add=TRUE, col = "blue", lty = 2, lwd = 2)
legend("topleft", legend = c("quadratic", "cubic"), 
       col = c("red", "blue"), lty = c(1,2), lwd = 2, cex = 2.5)
```


:::


::: {.column width="50%"}

::: {style="font-size: 0.95em"}

- Cubic model works better than Quadratic model

- The underlying relationship from Galileo’s data is Cubic and not Quadratic

- Probably the *inclined plane* introduced *drag*

- Code can be downloaded here [galileo.R](codes/galileo.R)

:::

:::





## Why not try higher degree polynomials? {.smaller}


<br>


$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3  
\, {\rm height }^2 + \beta_4 \, {\rm height }^3 
+ \beta_5 \, {\rm height }^4 + \e
$$

<br>

```r
# Fit Quartic model
quartic <- lm(distance ~ height + I( height^2 ) + I (height^3) 
                                                + I (height^4))
# Print summary
summary(quartic)
```





## {.smaller}

::: {style="font-size: 0.93em"}

```verbatim
Coefficients:
              Estimate Std. Error t value Pr(>|t|)   
(Intercept)  1.383e+02  9.066e+00  15.254  0.00427 **
height       1.346e+00  1.061e-01  12.690  0.00615 **
I(height^2) -2.117e-03  3.793e-04  -5.582  0.03063 * 
I(height^3)  1.766e-06  5.186e-07   3.406  0.07644 . 
I(height^4) -5.610e-10  2.375e-10  -2.362  0.14201   
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.523 on 2 degrees of freedom
Multiple R-squared:  0.9998,	Adjusted R-squared:  0.9995 
F-statistic:  3024 on 4 and 2 DF,  p-value: 0.0003306
```

<br>


- Individual t-tests show that:
    * Coefficients for **Intercept**, **Linear** and **Quadratic** terms are non-zero ($p < 0.05$)
    * Coefficients for **Cubic** and **Quartic** term are zero ($p > 0.05$)

- F-test for overall significance gives a p-value of $p = 0.0003306 < 0.05$
    * We have confirmation that at least one of the coefficients is non-zero

:::




##  {.smaller}

::: {style="font-size: 0.93em"}

```verbatim
Coefficients:
              Estimate Std. Error t value Pr(>|t|)   
(Intercept)  1.383e+02  9.066e+00  15.254  0.00427 **
height       1.346e+00  1.061e-01  12.690  0.00615 **
I(height^2) -2.117e-03  3.793e-04  -5.582  0.03063 * 
I(height^3)  1.766e-06  5.186e-07   3.406  0.07644 . 
I(height^4) -5.610e-10  2.375e-10  -2.362  0.14201   
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.523 on 2 degrees of freedom
Multiple R-squared:  0.9998,	Adjusted R-squared:  0.9995 
F-statistic:  3024 on 4 and 2 DF,  p-value: 0.0003306
```

<br>


- The coefficient of determination is $R^2 = 0.9998$
    * This is almost $1$

- **Conclusion:** There are signs that the model is overfitting
    * Coefficient $R^2$ extremely high
    * However, some terms (Cubic and Quartic) are not significant

:::





## Cubic vs Quartic {.smaller}

::: {style="font-size: 0.96em"}

- We saw that the Cubic model fits data well

- Quartic model shows some issues:
    * Extremely high $R^2$ coefficient
    * However, some paramteres are non-significant

- Nevertheless, the coefficients of determination are
$$
R^2 \text{ for Quartic model} = 0.9998 > R^2 \text{ for Cubic model} =  0.9994
$$

- Maybe the Quartic model is actually better?


- To compare predictions, use F-test for Model Selection


::: {style="font-size: 0.25em"}

<br>

:::

```r
# F-test for Model Selection
anova(cubic, quartic)
```

:::




##  {.smaller}

::: {style="font-size: 0.93em"}

```verbatim

Analysis of Variance Table

Model 1: distance ~ height + I(height^2) + I(height^3)
Model 2: distance ~ height + I(height^2) + I(height^3) + I(height^4)
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1      3 48.254                           
2      2 12.732  1    35.522 5.5799  0.142
```


<br>


- The null hypothesis favors Model 1; The alternative favors Model 2
\begin{align*}
H_0 \colon & \,  \beta_{5}  = 0 \\[5pt]
H_1 \colon & \,  \beta_{5} \neq 0 
\end{align*}

- The p-value is $p = 0.142 > 0.05 \quad \implies \quad$ do not reject $H_0$

- This means $\beta_5 = 0$, showing that the Quartic term is not improving predictions

:::



##  {.smaller}

::: {style="font-size: 0.93em"}

```verbatim

Analysis of Variance Table

Model 1: distance ~ height + I(height^2) + I(height^3)
Model 2: distance ~ height + I(height^2) + I(height^3) + I(height^4)
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1      3 48.254                           
2      2 12.732  1    35.522 5.5799  0.142
```


<br>

**Conclusion:** Cubic model has to be preferred to the Quartic one

- The Cubic model makes better predictions, despite lower $R^2$ coefficient

- The underlying relationship from Galileo’s data is indeed cubic!


:::






## Example 3: Debunking fake news {.smaller}

Screenshot of a Daily Mirror article [(link)](https://www.mirror.co.uk/news/uk-news/second-year-of-marriage-is-peak-time-337551?int_source=amp_continue_reading&int_medium=amp&int_campaign=continue_reading_button#amp-readmore-target)


![](images/mirror.png){width=100%}



## {.smaller}

- All the hallmarks of fake news are present
    * Bold claim
    * No precise references
    * No actual data


- The original "study" on [Divorce-Online.co.uk](https://www.divorce-online.co.uk) seems unavailable at present (2025)

- However, the Daily Mirror article [(link)](https://www.mirror.co.uk/news/uk-news/second-year-of-marriage-is-peak-time-337551?int_source=amp_continue_reading&int_medium=amp&int_campaign=continue_reading_button#amp-readmore-target) states the following:
    * *The poll [from Divorce-Online.co.uk] backs Office for National Statistics figures*

- The dataset in question can be found on the ONS Website [(Link to the Dataset)](https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/divorce/datasets/divorcesinenglandandwalesageatmarriagedurationofmarriageandcohortanalyses)





## The divorces dataset {.smaller}


::: {.column width="48%"}
```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Scatter plot of data
plot(year, percent, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Years of marriage", side = 1, line = 3, cex = 2.1)
mtext("Risk of divorce", side = 2, line = 2.5, cex = 2.1)
```
:::



::: {.column width="48%"}

::: {style="font-size: 0.72em"}

| **Years of Marriage**  |  **\% divorces** | 
|:---------------------- |:---              |
| 1                      | 3.51             |
| 2                      | 9.50             |
| 3                      | 8.91             |
| 4                      | 9.35             |
| 5                      | 8.18             |
| 6                      | 6.43             |
| 7                      | 5.31             |
| 8                      | 5.07             |
| 9                      | 3.65             |
| 10                     | 3.80             |
| 15                     | 2.83             |
| 20                     | 1.51             |
| 25                     | 1.27             |
| 30                     | 0.49             |
: {tbl-colwidths="[30,20]"}

:::


:::




## The divorces dataset {.smaller}

::: {.column width="48%"}
```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Scatter plot of data
plot(year, percent, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Years of marriage", side = 1, line = 3, cex = 2.1)
mtext("Risk of divorce", side = 2, line = 2.5, cex = 2.1)
```
:::



::: {.column width="48%"}

::: {style="font-size: 0.95em"}


- From the plot: Divorce-risk is
    * First low 
    * then peaks at year 2
    * then decreases

- *Daily Mirror* claimed:
    * Divorce-risk peaks at year 2, then decreases thereafter
    * From the plot, the claim seems credible


:::

:::




## The divorces dataset {.smaller}

::: {.column width="48%"}
```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Scatter plot of data
plot(year, percent, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Years of marriage", side = 1, line = 3, cex = 2.1)
mtext("Risk of divorce", side = 2, line = 2.5, cex = 2.1)
```
:::



::: {.column width="48%"}

::: {style="font-size: 0.95em"}

- We can fact check the claim with regression:
    * A peak in the data means there is a change of trend
    * This can only be explained by a polynomial model
    * Fit quadratic model 

- If claim is to be believed:
    * quadratic model should do better than a linear one


:::


:::




## Fitting linear model  {.smaller}

<br>

```r
# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Fit linear model
linear <- lm(percent ~ year)

# Plot summary
summary(linear)
```


## {.smaller}

::: {style="font-size: 0.95em"}

```verbatim
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.88575    0.78667  10.024 3.49e-07 ***
year        -0.27993    0.05846  -4.788 0.000442 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.879 on 12 degrees of freedom
Multiple R-squared:  0.6564,	Adjusted R-squared:  0.6278 
F-statistic: 22.93 on 1 and 12 DF,  p-value: 0.0004422
```

<br>

- t-test for $\beta_2$ is significant, since $p = 0.0004 < 0.05$
- Therefore $\beta_2 \neq 0$, and the estimate is $\hat \beta_2 = -0.27993$
- The risk of divorce decreases with years of marriage (because $\hat \beta_2 < 0$)
- Coefficient of determination is $R^2 = 0.6564$, which is reasonably high

- **Conclusion:** The linear model provides a good fit

:::


## Fitting quadratic model  {.smaller}

- Linear model offered a reasonable explanation of the divorce data

- Is a quadratic model better?

<br>

```r
# Fit quadratic model
quadratic <- lm(percent ~ year + I( year^2 ))

# Print summary
summary(quadratic)
```



## {.smaller}

::: {style="font-size: 0.95em"}

```verbatim
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  8.751048   1.258038   6.956  2.4e-05 ***
year        -0.482252   0.235701  -2.046   0.0654 .  
I(year^2)    0.006794   0.007663   0.887   0.3943    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.896 on 11 degrees of freedom
Multiple R-squared:  0.6794,	Adjusted R-squared:  0.6211 
F-statistic: 11.65 on 2 and 11 DF,  p-value: 0.001919
```

<br>

- t-test for $\beta_3$ is not significant, since $p = 0.3943 > 0.05$

- Cannot reject null hypothesis that $\beta_3 = 0 \quad \implies \quad$ Quadratic term not needed! 

- **Note:** No need to run Model Selection to compare linear and quadratic models
    * This is because, when comparing linear and quadratic models, the extra term corresponds to the parameter $\beta_3$
    * Therefore, F-test for Model Selection would give same p-value as t-test for $\beta_3$

:::




## Conclusions {.smaller}

::: {style="font-size: 0.95em"}

- Daily Mirror's Claim: Divorce-risk peaks at year 2 then decreases thereafter
    * Claim suggests higher order model needed to explain change in trend


- Analysis conducted by us: 
    * Fit linear and quadratic regression models 
    * t-test of significance discarded quadratic term
    * This is equivalent to F-test for Model Selection: Quadratic model is discarded


- Our Findings: Claim in Daily Mirror article is **misleading**
    * Linear model is clearly better than quadratic model
    * This suggests divorce-risk generally decreases over time
    * Peak in year 2 can be explained by unusually low divorce-risk in 1st year
    * This means data for 1st year is *outlier*


:::


## Visual confirmation {.smaller}


::: {.column width="48%"}

```{r} 
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Fit linear model
linear <- lm(percent ~ year)

# Fit quadratic model
quadratic <- lm(percent ~ year + I( year^2 ))

# Scatter plot of data
plot(year, percent, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Years of marriage", side = 1, line = 3, cex = 2.1)
mtext("Risk of divorce", side = 2, line = 2.5, cex = 2.1)

# Plot Linear Vs Quadratic
polynomial <- Vectorize(function(x, ps) {
  n <- length(ps)
  sum(ps*x^(1:n-1))
}, "x")
curve(polynomial(x, coef(linear)), add=TRUE, col = "red", lwd = 2)
curve(polynomial(x, coef(quadratic)), add=TRUE, col = "blue", lty = 2, lwd = 2)
legend("topright", legend = c("Linear", "Quadratic"), 
       col = c("red", "blue"), lty = c(1,2), cex = 3, lwd = 3)
```

:::




::: {.column width="48%"}

<br>
<br>

::: {style="font-size: 0.95em"}

- Linear model is evidently better at explaining decreasing divorce-risk

- Peak in year 2 should be explained by unusually low divorce-risk in 1st year

- This means Year 1 data is *outlier*

- Code is available here [divorces.R](codes/divorces.R)

:::

:::








## Why not try higher order polynomials? {.smaller}

- We can try fitting higher-order polynomial models to explain *peak in Year 2*

- The lowest-order polynomial that provides a good fit is degree 6 (try it yourself!)

- Let us compare the *linear model* with the *degree 6 polynomial model*

<br>

```r
# Fit order 6 model
degree.6 <- lm(percent ~ year + I( year^2 ) + I( year^3 ) + 
                              + I( year^4 ) + I( year^5 ) +
                              + I( year^6 ))

# F-test for Model Selection
anova(linear, degree.6)
```



## {.smaller}

```verbatim
Model 1: percent ~ year
Model 2: percent ~ year + I(year^2) + I(year^3) + I(year^4) + I(year^5) + 
    +I(year^6)
  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   
1     12 42.375                                
2      7  3.724  5    38.651 14.531 0.001404 **
```

<br>

- F-test for Model Selection is significant, since $p = 0.001 < 0.05$

- This means we reject the null hypothesis that
$$
\beta_3 = \beta_4 = \beta_5 = \beta_6 = 0
$$

- The degree 6 model is better than the Linear model

- Peak divorce-rate in Year 2 is well explained by order 6 regression

- What is going on? Was the Daily Mirror right?
    * Let us plot the fitted regression functions




## {.smaller}




::: {.column width="49%"}

```{r} 
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Fit linear model
linear <- lm(percent ~ year)

# Fit degree 6 model
degree.6 <- lm(percent ~ year  + I( year^2 ) + I( year^3 ) + 
                                I( year^4 ) + I( year^5 ) +
                                I( year^6 ))

# Scatter plot of data
plot(year, percent, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Years of marriage", side = 1, line = 3, cex = 2.1)
mtext("Risk of divorce", side = 2, line = 2.5, cex = 2.1)

# Plot Linear Vs Quadratic
polynomial <- Vectorize(function(x, ps) {
  n <- length(ps)
  sum(ps*x^(1:n-1))
}, "x")
curve(polynomial(x, coef(linear)), add=TRUE, col = "red", lwd = 2)
curve(polynomial(x, coef(degree.6)), add=TRUE, col = "blue", lty = 2, lwd = 3)
legend("topright", legend = c("Linear", "Degree 6"), 
       col = c("red", "blue"), lty = c(1,2), cex = 3, lwd = 3)
```

:::




::: {.column width="49%"}

::: {style="font-size: 0.92em"}


- Degree 6 model explains Peak at Year 2

- However, the model introduces new peaks! 
    * We observe a decreasing risk of divorce for 23 years
    * But marriage gets boring after 27 years!

- **Degree 6 Model overfits**:
    * No, the Daily Mirror is not right 
    * Data is very well explained
    * but predictions are not realistic

- Linear model should be preferred
    * We will justify this rigorously with **Stepwise Regression**

:::

:::




## References
