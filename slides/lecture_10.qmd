---
title: "Statistical Models"
subtitle: "Lecture 10"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Lecture 10: <br>Practical regression{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::






## Outline of Lecture 10 {.smaller}


1. Model selection
    * Comparison of nested regression models

2. Examples of model selection




# Part 1: <br> Model selection <br> {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Testing regression parameters {.smaller}

**Summary:** We have seen

- **t-test**
    * Test the significance of individual parameters
    \begin{align*}
    H_0 & \colon \, \beta_j = 0 \\
    H_1 & \colon \, \beta_j \neq 0
    \end{align*}


## Testing regression parameters {.smaller}

- **F-test**
    * Test the overall significance of the model
    * This is done by comparing two **nested** regression models
    \begin{align*}
    \textbf{Model 1:} & \qquad Y_ i= \beta_1 + \e_i \\[10pt]
    \textbf{Model 2:} & \qquad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_p x_{p, i} + \e_i
    \end{align*}
    * The comparison is achieved with F-test for
    \begin{align*}
    H_0 & \colon \, \beta_2 = \beta_3 = \ldots = \beta_p = 0 \\
    H_1 & \colon \text{ At least one of the } \beta_i \text{ is non-zero}
    \end{align*}
    * Choosing the Full Model 2 is equivalent to rejecting $H_0$



## More general nested models {.smaller}

- Consider the more general **nested** models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \beta_{k + 1} x_{k + 1, i} + 
    \ldots + \beta_{p} x_{p, i} + \e_i
\end{align*}


- Model 1 has k parameters

- Model 2 has p parameters with $p > k$

- The two models coincide if

$$
\beta_{k + 1} = \beta_{k + 2} = \ldots = \beta_p = 0 
$$

**Question:** How do we decide which model is better?


## Model selection {.smaller}

- Consider the more general **nested** models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \beta_{k + 1} x_{k + 1, i} + 
    \ldots + \beta_{p} x_{p, i} + \e_i
\end{align*}


- Define the predictions for the two models

\begin{align*}
\hat y_i^1 & := \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} \\[10pt]
\hat y_i^2 & :=  \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \beta_{k + 1} x_{k + 1, i} + 
    \ldots + \beta_{p} x_{p, i}
\end{align*}




## Model selection {.smaller}

- Consider the more general **nested** models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \beta_{k + 1} x_{k + 1, i} + 
    \ldots + \beta_{p} x_{p, i} + \e_i
\end{align*}

- $\RSS$ measures variation between data and prediction

\begin{align*}
    \textbf{Model 1:} & \quad \RSS_1 := \RSS (k) = \sum_{i=1}^n (y_i - \hat y_i^1)^2 \\[10pt]
    \textbf{Model 2:} & \quad \RSS_2 := \RSS (p) = \sum_{i=1}^n (y_i - \hat y_i^2)^2
\end{align*}



## Extra sum of squares {.smaller}

- Consider the more general **nested** models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \beta_{k + 1} x_{k + 1, i} + 
    \ldots + \beta_{p} x_{p, i} + \e_i
\end{align*}


- The **extra sum of squares** is the difference

$$
\RSS_1 - \RSS_2 := \RSS (k) - \RSS (p)
$$



## Construction of F-statistic {.smaller}

**Goal:** Use $\RSS$ to construct statistic to compare the 2 models

- Suppose the extra parameters of Model 2 
$$
\beta_{k+1}, \, \beta_{k+2} , \, \ldots , \, \beta_p
$$
are not important

- Hence the predictions of the 2 models will be similar
$$
\hat y_i^1 \, \approx \, \hat y_i^2
$$

- Therefore the $\RSS$ for the 2 models are similar
$$
\RSS_1 \, \approx \, \RSS_2
$$


## Construction of F-statistic {.smaller}

- Recall that $\RSS$ cannot increase if we increase parameters

$$
k < p \quad \implies \quad \RSS (k) \geq \RSS (p)
$$


- To measure influence of extra parameters 
$$
\beta_{k+1}, \, \beta_{k+2} , \, \ldots , \, \beta_p
$$
we consider the ratio
$$
\frac{ \RSS_1 - \RSS_2 }{ \RSS_2 } = \frac{ \RSS (k) - \RSS (p) }{ \RSS(p) }
$$



## Construction of F-statistic {.smaller}

- We now suitably rescale

$$
\frac{ \RSS_1 - \RSS_2 }{ \RSS_2 }
$$

- Note that the degrees of freedom are
    * Model 1:
    $$
    k \text{ parameters } \quad \implies \quad \df_1 =  n - k
    $$
    * Model 2:
    $$
    p \text{ parameters } \quad \implies \quad \df_2 = n - p
    $$


## F-statistic for model selection {.smaller}

::: Definition

The F-statistic for model selection is

\begin{align*}
F & = \frac{ \RSS_1 - \RSS_2 }{ \df_1 - \df_2  } \bigg/
      \frac{ \RSS_2 }{ \df_2  }  \\[20pt]
 & = \frac{ \RSS(k) - \RSS(p) }{ p - k  } \bigg/
      \frac{ \RSS(p) }{ n - p  }  
\end{align*}

:::

**Theorem:** The F-statistic for model selection has F-distribution

$$
F \, \sim  \, F_{\df_1 - \df_2 , \, \df_2} = F_{p - k, \, n - p}
$$


## Rewriting the F-statistic {.smaller}

- Recall the formulas for sums of squares

$$
\TSS = \ESS(p) + \RSS(p)  \,, \qquad \quad
\TSS = \ESS(k) + \RSS(k)  
$$

- **Note:** $\TSS$ does not depend on numeber of parameters

- Also define the coefficient of determination for the two models

$$
R_1^2 := R^2 (k) := \frac{ \ESS(k) }{ \TSS }
\, , \qquad \quad 
R_2^2 := R^2 (p) := \frac{ \ESS(p) }{ \TSS }
$$



## Rewriting the F-statistic {.smaller}



\begin{align*}
\RSS(k) - \RSS(p) & = \ESS(p) - \ESS(k) \\[10pt]
                  & = \TSS ( R^2(p) - R^2(k) ) \\[10pt]
                  & = \TSS ( R^2_2 - R^2_1 ) \\[20pt]
\RSS(p) & = \TSS - \ESS(p)  \\[10pt]
        & = \TSS  - \TSS \, \cdot \, R^2 (p) \\[10pt]
        & = \TSS (1 - R^2(p)) \\[10pt]
        & = \TSS (1 - R_2^2)
\end{align*}


## Rewriting the F-statistic {.smaller}

Therefore the F-statistic can be rewritten as

\begin{align*}
F & = \frac{ \RSS_1 - \RSS_2 }{ \df_1 - \df_2  } \bigg/
      \frac{ \RSS_2 }{ \df_2 }   \\[20pt]
  & = \frac{ \TSS (R^2_2 - R^2_1) }{\TSS (1 - R^2_2 )}  \, \cdot \,  \frac{n-p}{p-k} \\[20pt]
  & = \frac{ R^2_2 - R^2_1 }{1 - R^2_2}  \, \cdot \,  \frac{n-p}{p-k}
\end{align*}



## F-test for overall significance revisited {.smaller}

- The F-test for overall significance allows to select between models
\begin{align*}
    \textbf{Model 1:} & \qquad Y_ i= \beta_1 + \e_i \\[10pt]
    \textbf{Model 2:} & \qquad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_p x_{p, i} + \e_i
\end{align*}

- Model 1 has $k = 1$ parameters

- F-statistic for model selection coincides with F-statistic for overall significance

$$
F = \frac{ \RSS(1) - \RSS(p) }{ p - 1  } \bigg/
      \frac{ \RSS(p) }{ n - p  }
$$



## Summary: F-test for model selection {.smaller}

**Goal:** Choose one of the nested models

\begin{align*}
    \textbf{Model 1:} & \quad Y_ i =\beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \e_i \\[10pt]
    \textbf{Model 2:} & \quad Y_ i= \beta_1 + \beta_2 x_{2, i}+ \ldots + \beta_{k} x_{k, i} + \beta_{k + 1} x_{k + 1, i} + 
    \ldots + \beta_{p} x_{p, i} + \e_i
\end{align*}


**Hypotheses:** Choosing a model is equivalent to testing

\begin{align*}
H_0 \colon & \, \beta_{k+1} = \beta_{k+2} = \ldots  = \beta_p \\[5pt]
H_1 \colon & \, \text{ At least one among } \beta_{k+1}, \ldots, \beta_p \text{ is non-zero}
\end{align*}

- $H_0$ is in favor of Model 1
- $H_1$ is in favor of Model 2



## Summary: F-test for model selection {.smaller}

- The F-statistic is

\begin{align*}
F & = \frac{ \RSS_1 - \RSS_2 }{ \df_1 - \df_2  } \bigg/
      \frac{ \RSS_2 }{ \df_2  }   \\[20pt]
  & = \frac{ R^2_2 - R^2_1 }{1 - R^2_2 }  \, \cdot \,  \frac{n-p}{p-k}
\end{align*}

- Distribution of $F$ is

$$
F \, \sim \, F_{ \df_1 - \df_2 , \, \df_2 } = F_{p-k, \, n-p}
$$



## Summary: F-test for model selection {.smaller}

- The p-value is

$$
p = P(F_{p-k,n-p} > F)
$$

- **F-test for model selection in R:**
    - Fit the two models with $\,\texttt{lm}$
    - Use the command $\, \texttt{anova} \qquad\quad$  (more on this later)


- **Alternative:**
    - Find $R^2_1$ and $R^2_2$ in summary
    - Compute F-statistic and p-value



# Part 2: <br>Examples of <br> model selection {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Examples of model selection {.smaller}

We illustrate F-test for Model Selection with 3 examples:

- Joint significance in Multiple linear Regression
- Polynomial regression 1
- Polynomial regression 2



## Example 1: Multiple linear regression {.smaller}

- Consider again the Longley dataset

```{r}
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

head(longley,n=3)
```


**Goal:** Explain the number of *Employed* people $Y$ in the US in terms of

- $X_2$ *GNP* Gross National Product
- $X_3$ number of *Unemployed*
- $X_4$ number of people in the *Armed Forces*
- $X_5$ *non-institutionalised Population* $\geq$ age 14 (not in care of insitutions)
- $X_6$ *Years* from 1947 to 1962



## Example 1: Multiple linear regression {.smaller}

**Previously:** Using t-test for parameters significance we showed that

- $X_2$ and $X_5$ do not affect $Y$
- $X_3$ and $X_4$ negatively affect $Y$
- $X_6$ positively affects $Y$

**Question:** Since $X_2$ and $X_5$ do not affect $Y$, can we exclude them from the model?


## Two competing models {.smaller}


We therefore want to select between the models:


- **Model 1:** The Reduced Model without $X_2$ and $X_5$

$$
Y = \beta_1 + \beta_3 X_3 + \beta_4 X_4 + 
       \beta_6 X_6 + \e
$$

- **Model 2:** The Full Model

$$
Y = \beta_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + 
       \beta_5 X_5 +  \beta_6 X_6 + \e
$$





## R commands for reading in the data {.smaller}

- We read the data in the same way we did earlier

- Longley dataset available here [longley.txt](datasets/longley.txt)

- Download the file and place it in current working directory

```r
# Read data file
longley <- read.table(file = "longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]
```


## R commands for fitting multiple regression {.smaller}

1. Fit the two multiple regression models

\begin{align*}
\textbf{Model 1:} & \quad Y = \beta_1 + \beta_3 X_3 + \beta_4 X_4 + 
       \beta_6 X_6 + \e \\[10pt]
\textbf{Model 2:} & \quad Y = \beta_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + 
       \beta_5 X_5 +  \beta_6 X_6 + \e
\end{align*}


```r
# Fit Model 1 and Model 2
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)
```

2. F-test for model selection is done using the command $\, \texttt{anova}$

```r
# F-test for model selection
anova(model.1, model.2, test = "F")
```


- Full code can be downloaded here [longley_selection.R](codes/longley_selection.R)




## Anova output {.smaller}


```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 0 and Model 1
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```


**Interpretation:**

- First two lines tell us which models are being compared





## Anova output {.smaller}

```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 0 and Model 1
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```

**Interpretation:**

- $\texttt{Res.Df} \,$ are the degrees of freedom of each model
    * The sample size of longley is 16
    * Model 1 has $k=4$ parameters
    * Model 2 has $p=6$ parameters
    * $\df_1 = n - k = 16 - 4 = 12   \quad \qquad \df_2 = n - p = 16 - 6 = 10$



## Anova output {.smaller}

```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 0 and Model 1
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```

**Interpretation:**

- $\texttt{Df} \,$ is difference in degrees of freedom
    * $\df_1 = 12$
    * $\df_2 = 10$
    * Therefore the difference is
    $$
    \df_1 - \df_2 = 12 - 10 = 2
    $$





## Anova output {.smaller}


```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```

**Interpretation:**

- $\texttt{RSS} \,$ is the residual sum of squares for each model
    * $\RSS_1 = 1.32336$
    * $\RSS_2 = 0.83935$

- $\texttt{Sum of Sq} \,$ is the extra sum of squares
    * $\RSS_1 - \RSS_2 = 0.48401$






## Anova output {.smaller}


```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```

**Interpretation:**

- $\texttt{F} \,$ is the F-statistic for model selection

\begin{align*}
F & = \frac{ \RSS_1 - \RSS_2 }{ \df_1 - \df_2 } \bigg/ 
      \frac{ \RSS_2 }{ \df_2 } \\
  & = \frac{ 1.32336 - 0.83935 }{ 12 - 10 } \bigg/ 
      \frac{ 0.83935 }{ 10 } =  2.8833
\end{align*}




## Anova output {.smaller}


```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```

**Interpretation:**

- $\texttt{Pr(>F)}$ is the p-value for F-test
    * $F \, \sim \, F_{\df_1 - \df_2 , \, \df_2 } = F_{2, 10}$
    * Therefore the p-value is
    $$
    p = P(F_{2,10} > F) = 0.1026
    $$




## Anova output {.smaller}


```{r}
# Read data file
longley <- read.table(file = "datasets/longley.txt",
                      header = TRUE)

# Store columns in vectors
x2 <- longley[ , 1]
x3 <- longley[ , 2]
x4 <- longley[ , 3]
x5 <- longley[ , 4]
x6 <- longley[ , 5]
y <- longley[ , 6]

# Fit Model 1 and Model 2
model.1 <- lm(y ~ x3 + x4 + x6)
model.2 <- lm(y ~ x2 + x3 + x4 + x5 + x6)

# F-test for model selection
anova(model.1, model.2, test = "F")
```

**Conclusion:**

- The p-value is $p = 0.1026 > 0.05$
- This means we cannot reject $H_0$
- Therefore the Reduced Model 1 has to be preferred
- This gives statistical evidence that $X_2$ and $X_5$ can be excluded from the model
- *GNP* and *Non-institutionalised* do not affect *Number of Employed*




## Example 2: Motion of falling bodies {.smaller}

Engraving (1546): people believed projectiles follow circular trajectories ([source](https://www.alamy.com/engraving-depicting-the-path-of-a-projectile-shown-as-a-circular-arc-rather-than-a-parabolic-arc-as-was-later-proved-to-be-the-case-by-galileo-galileo-galilei-1564-1642-an-italian-polymath-dated-16th-century-image186386912.html))



![](images/engraving.jpg){width=73%}




## {.smaller}

- 1609: Galileo proved mathematically that projectile trajectories are parabolic
    * His finding was based on empirical data
    * A ball (covered in ink) was released on an inclined plane from *Initial Height*
    * Ink mark on the floor represented the *Horizontal Distance* traveled
    * Unit of measure is *punti*  $\qquad\quad 1 \text{ punto} = 169/180 \, \text{mm}$

![](images/galileo_0.png){width=73%}




## {.smaller}

- We have access to Galileo's original data [@drake_galileo]
- Does a parabolic (quadratic) trajectory really explain the data? 
- Let's fit a polynomial regression model and find out!

![](images/galileo.png){width=73%}




## Plotting the data {.smaller}

<br>

| **Initial Height** | 100  | 200 | 300 | 450 | 600 | 800 | 1000 |
|:------------------ |:---- |:--  |:--- |:--- |:--  |:-   |:-    |
| **Horizontal Distance** | 253  | 337 | 395 | 451 | 495 | 534 | 573 |

<br>

```r
# Enter the data
height <- c(100, 200, 300, 450, 600, 800, 1000)
distance <- c(253, 337, 395, 451, 495, 534, 573)

# Scatter plot of data
plot(height, distance, pch = 16)
```


##  {.smaller}

We clearly see a parabola. 
Therefore we expect a relation of the form

$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3 \, {\rm height }^2
$$

```{r}
#| echo: false
#| fig-asp: 1

# Enter the data
height <- c(100, 200, 300, 450, 600, 800, 1000)
distance <- c(253, 337, 395, 451, 495, 534, 573)

# Scatter plot of data
plot(height, distance, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Initial height", side = 1, line = 3, cex = 2.1)
mtext("Horizontal distance", side = 2, line = 2.5, cex = 2.1)
```


## Fit linear model {.smaller}


$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } 
$$


```r
# Fit linear model
linear <- lm(distance ~ height)
summary(linear)
```

```verbatim

Multiple R-squared:  0.9264,	Adjusted R-squared:  0.9116 


```


- The coefficient of correlation is $R^2 = 0.9264$
- $R^2$ is quite high, showing that a linear model fits reasonably well




## Is a quadratic model better? {.smaller}

$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3  
\, {\rm height }^2
$$

<br>

**Note:** To specify powers we need to type $\,\, \texttt{I}$

```r
# Fit quadratic model
quadratic <- lm(distance ~ height + I( height^2 ))
summary(quadratic)
```

```verbatim

Multiple R-squared:  0.9903,	Adjusted R-squared:  0.9855 


```



- The coefficient of correlation is $R^2 = 0.9903$
- This is higher than the previous score $R^2 = 0.9264$
- The quadratic trajectory explains $99\%$ of variability in the data



## Why not try a cubic model? {.smaller}


$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3  
\, {\rm height }^2 + \beta_4 \, {\rm height }^3 
$$

<br>

```r
# Fit cubic model
cubic <- lm(distance ~ height + I( height^2 ) + I (height^3))
summary(cubic)
```
```verbatim

Multiple R-squared:  0.9994,	Adjusted R-squared:  0.9987 


```


- The coefficient of correlation is $R^2 = 0.9994$
- This is higher than the score of quadratic model $R^2 = 0.9903$
- What is going on?



## Quadratic vs cubic {.smaller}

- Which model is better: quadratic or cubic?

- Let us perform F-test for model selection


```r
# Model selection
anova(quadratic, cubic, test = "F")
```

```verbatim

Analysis of Variance Table

Model 1: distance ~ height + I(height^2)
Model 2: distance ~ height + I(height^2) + I(height^3)
  Res.Df    RSS Df Sum of Sq     F  Pr(>F)   
1      4 744.08                              
2      3  48.25  1    695.82 43.26 0.00715 **
```


## Model selection: quadratic Vs cubic {.smaller}

- The F-test is significant since $p = 0.007 < 0.05$

- This means we should reject the null hypothesis that 

$$
\beta_4 = 0
$$

- Therefore the quadratic model does not describe the data well

- The underlying relationship from Galileo’s data is cubic and not quadratic

- Probably the *inclined plane* introduced drag

- Code can be downloaded here [galileo.R](codes/galileo.R)



## Plot: Quadratic Vs Cubic {.smaller}

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Enter the data
height <- c(100, 200, 300, 450, 600, 800, 1000)
distance <- c(253, 337, 395, 451, 495, 534, 573)

# Scatter plot of data
plot(height, distance, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Initial height", side = 1, line = 3, cex = 2.1)
mtext("Horizontal distance", side = 2, line = 2.5, cex = 2.1)

# Fit quadratic model
quadratic <- lm(distance ~ height + I( height^2 ))

# Fit cubic model
cubic <- lm(distance ~ height + I( height^2 ) + I (height^3))

# Plot quadratic Vs Cubic
polynomial <- Vectorize(function(x, ps) {
  n <- length(ps)
  sum(ps*x^(1:n-1))
}, "x")
curve(polynomial(x, coef(quadratic)), add=TRUE, col = "red", lwd = 2)
curve(polynomial(x, coef(cubic)), add=TRUE, col = "blue", lty = 2, lwd = 2)
legend("topleft", legend = c("quadratic", "cubic"), 
       col = c("red", "blue"), lty = c(1,2), lwd = 2, cex = 2.5)
```


## Why not try higher degree polynomials {.smaller}

$$
{\rm distance} = \beta_1 + \beta_2 \, {\rm height } + \beta_3  
\, {\rm height }^2 + \beta_4 \, {\rm height }^3 
+ \beta_5 \, {\rm height }^4 
$$

<br>

```r
# Fit quartic model
quartic <- lm(distance ~ height + I( height^2 ) + I (height^3) 
                                                + I (height^4))
summary(quartic)
```
```verbatim

Multiple R-squared:  0.9998,	Adjusted R-squared:  0.9995


```

- We obtain a coefficient $R^2 = 0.9998$
- This is even higher than cubic model coefficient $R^2 =  0.9994$
- Is the quartic model actually better?



## Model selection: cubic Vs quartic {.smaller}


```r
# Model selection
anova(cubic, quartic, test = "F")
```

```verbatim

Analysis of Variance Table

Model 1: distance ~ height + I(height^2) + I(height^3)
Model 2: distance ~ height + I(height^2) + I(height^3) + I(height^4)
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1      3 48.254                           
2      2 12.732  1    35.522 5.5799  0.142


```

- The F-test is not significant since $p = 0.142 > 0.05$

- This means we cannot reject the null hypothesis that $\beta_5 = 0$

- The cubic models does better than quartic, despite higher $R^2$

- The underlying relationship from Galileo’s data is indeed cubic!






## Example 3: Divorces {.smaller}


- Data from **Daily Mirror** gives 
    * Percentage of divorces caused by adultery VS years of marriage

- Original analysis claimed
    * Divorce-risk peaks at year 2 then decreases thereafter

- Is this conclusion misleading?
    * Does a quadratic model offers a better fit than a straight line model?
    


## Divorces dataset {.smaller}
### Percent of divorces caused by adultery by year of marriage

<br>

| **Years of Marriage**              | 1   |  2   |  3  |  4  | 5  |   6  |  7  |
|:---------------------------------- |:--- |:-    |:--  |:--  |:-- |:---  |:--  |
| **\% divorces adultery**        | 3.51| 9.50 | 8.91| 9.35|8.18| 6.43 | 5.31|
: {tbl-colwidths="[30,10,10,10,10,10,10,10]"}


<br>


| **Years of Marriage**              | 8   |  9   | 10  | 15  |20  |  25  | 30  |
|:---------------------------------- |:--- |:-    |:--  |:--  |:-- |:---  |:--  |
| **\% divorces adultery**        | 5.07| 3.65 | 3.80| 2.83|1.51| 1.27 | 0.49|
: {tbl-colwidths="[30,10,10,10,10,10,10,10]"}





## Plot: Years of Marriage Vs Divorce-risk {.smaller}

::: {.column width="48%"}

- Looks like: Divorce-risk is
    * First low, 
    * then peaks at year 2
    * then decreases

- Change of trend suggests: 
    * Higher order model might be good fit
    * Consider quadratic model 


:::

::: {.column width="48%"}
```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Scatter plot of data
plot(year, percent, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Years of marriage", side = 1, line = 3, cex = 2.1)
mtext("Risk of divorce by adultery", side = 2, line = 2.5, cex = 2.1)
```
:::


## Fitting linear model  {.smaller}

```r
# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Fit linear model
linear <- lm(percent ~ year)
summary(linear)
```

```verbatim

            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.88575    0.78667  10.024 3.49e-07 ***
year        -0.27993    0.05846  -4.788 0.000442 ***


```

- t-test for $\beta_2$ is significant since $p = 0.0004 < 0.05$
- Therefore $\beta_2 \neq 0$ and the estimate is $\hat \beta_2 = -0.27993$
- The risk of divorce decreases with years of marriage (because $\hat \beta_2 < 0$)



## Fitting quadratic model  {.smaller}

- Linear model offered a reasonable explanation of the divorce data

- Is quadratic model better?


```r
# Fit quadratic model
quadratic <- lm(percent ~ year + I( year^2 ))
summary(quadratic)
```

```verbatim

             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  8.751048   1.258038   6.956  2.4e-05 ***
year        -0.482252   0.235701  -2.046   0.0654 .  
I(year^2)    0.006794   0.007663   0.887   0.3943 


```

- t-test for $\beta_3$ is not significant since $p = 0.3943 > 0.05$

- Cannot reject null hypothesis $\beta_3 = 0 \quad \implies \quad$ Quadratic term not needed! 

- The original analysis in the **Daily Mirror is probably mistaken**




## Model selection: Linear Vs Quadratic {.smaller}

- We concluded that a linear model is better fit

- To cross check this result we do F-test for model selection

```r
# Model selection
anova(linear, quadratic, test = "F")
```

```verbatim

Model 1: percent ~ year
Model 2: percent ~ year + I(year^2)
  Res.Df    RSS Df Sum of Sq     F Pr(>F)
1     12 42.375                          
2     11 39.549  1     2.826 0.786 0.3943


```


- F-test is not significant since $p = 0.3943 > 0.05$
- We cannot reject the null hypothesis that $\beta_3 = 0$
- Quadratic model is worse than linear model


## Conclusions {.smaller}

- Daily Mirror Claim: Divorce-risk peaks at year 2 then decreases thereafter
    * Claim suggests higher order model needed to explain change in trend


- Analysis conducted: 
    * Fit linear and quadratic regression models 
    * t-test of significance discarded quadratic term
    * F-test for model selection discarded Quadratic model

- Findings: Claims in Daily Mirror are misleading
    * Linear model seems to be better than quadratic
    * This suggests divorce-risk generally decreases over time
    * Peak in year 2 can be explained by unusually low divorce-risk in 1st year

- Code is available here [divorces.R](codes/divorces.R)


## {.smaller}

- Visual confirmation: Linear model is better and divorce-risk is decreasing

- Peak in year 2 should be explained by unusually low divorce-risk in 1st year


```{r} 
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Fit linear model
linear <- lm(percent ~ year)

# Fit quadratic model
quadratic <- lm(percent ~ year + I( year^2 ))

# Scatter plot of data
plot(year, percent, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Years of marriage", side = 1, line = 3, cex = 2.1)
mtext("Risk of divorce by adultery", side = 2, line = 2.5, cex = 2.1)

# Plot Linear Vs Quadratic
polynomial <- Vectorize(function(x, ps) {
  n <- length(ps)
  sum(ps*x^(1:n-1))
}, "x")
curve(polynomial(x, coef(linear)), add=TRUE, col = "red", lwd = 2)
curve(polynomial(x, coef(quadratic)), add=TRUE, col = "blue", lty = 2, lwd = 2)
legend("topright", legend = c("Linear", "Quadratic"), 
       col = c("red", "blue"), lty = c(1,2), cex = 3, lwd = 3)
```



## Why not try higher order polynomials {.smaller}

- Let us compare Linear model with Order 6 Model

```r
# Fit order 6 model
order_6 <- lm(percent ~ year  + I( year^2 ) + I( year^3 ) + 
                              + I( year^4 ) + I( year^5 ) +
                              + I( year^6 ))

# Model selection
anova(linear, order_6)
```

```verbatim

Model 1: percent ~ year
Model 2: percent ~ year + I(year^2) + I(year^3) + I(year^4) + I(year^5) + 
    +I(year^6)
  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   
1     12 42.375                                
2      7  3.724  5    38.651 14.531 0.001404 **
```


## Why not try higher order polynomials {.smaller}


- F-test is significant since $p = 0.001 < 0.05$

- This means we reject the null hypothesis that

$$
\beta_3 = \beta_4 = \beta_5 = \beta_6 = 0
$$

- The Order 6 model is better than the Linear model

- Peak divorce-rate in Year 2 is well explained by order 6 regression

- What is going on? Let us plot the regression functions




## {.smaller}



::: {.column width="45%"}

<br>

- There are more peaks: 
    * Decreasing risk of divorce for 23 years
    * But it gets boring after 27 years!

- **Model overfits**: 
    * Data is very well explained
    * but predictions are not realistic

- Linear model should be preferred

:::


::: {.column width="46%"}

```{r} 
#| echo: true
#| code-fold: true
#| code-summary: "Click here to show the full code"
#| fig-asp: 1

# Divorces data
year <- c(1, 2, 3, 4, 5, 6,7, 8, 9, 10, 15, 20, 25, 30)
percent <- c(3.51, 9.5, 8.91, 9.35, 8.18, 6.43, 5.31, 
             5.07, 3.65, 3.8, 2.83, 1.51, 1.27, 0.49)

# Fit linear model
linear <- lm(percent ~ year)

# Fit order 6 model
order_6 <- lm(percent ~ year  + I( year^2 ) + I( year^3 ) + 
                                I( year^4 ) + I( year^5 ) +
                                I( year^6 ))

# Scatter plot of data
plot(year, percent, xlab = "", ylab = "", pch = 16, cex = 2)

# Add labels
mtext("Years of marriage", side = 1, line = 3, cex = 2.1)
mtext("Risk of divorce by adultery", side = 2, line = 2.5, cex = 2.1)

# Plot Linear Vs Quadratic
polynomial <- Vectorize(function(x, ps) {
  n <- length(ps)
  sum(ps*x^(1:n-1))
}, "x")
curve(polynomial(x, coef(linear)), add=TRUE, col = "red", lwd = 2)
curve(polynomial(x, coef(order_6)), add=TRUE, col = "blue", lty = 2, lwd = 3)
legend("topright", legend = c("Linear", "Order 6"), 
       col = c("red", "blue"), lty = c(1,2), cex = 3, lwd = 3)
```

:::



## References
