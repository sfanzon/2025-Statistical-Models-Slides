---
title: "Statistical Models"
subtitle: "Lecture 2"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Lecture 2: <br>Random samples<br>{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Outline of Lecture 2

1. Probability revision III
2. Multivariate random vectors
3. Random samples
4. Unbiased estimators
5. Chi-squared distribution
6. Sampling from normal distribution
7. t-distribution



# Part 1: <br>Probability revision III {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Probability revision III {.smaller}

- You are expected to be familiar with the main concepts from Y1 module    
**Introduction to Probability & Statistics**

- Self-contained revision material available in <a href="appendix_A.qmd">Appendix A</a> 


**Topics to review**: Sections 6--7 of <a href="appendix_A.qmd">Appendix A</a>
    
::: {.column width="38%"}
    
- Independence of random variables
- Covariance and correlation

:::

::: {.column width="58%"}


:::






## Independence of random variables  {.smaller}



::: Definition
### Independence
$(X,Y)$ random vector with joint pdf or pmf $f_{X,Y}$ and marginal pdfs or pmfs $f_X,f_Y$. We say that $X$ and $Y$ are **independent** random variables if
$$
f_{X,Y}(x,y)  =  f_X(x)f_Y(y) \,, \quad \forall \, (x,y) \in \R^2 
$$

:::




## Independence of random variables {.smaller}
### Conditional distributions and probabilities


If $X$ and $Y$ are **independent** then $X$ gives no information on $Y$ (and vice-versa):

- Conditional distribution: $Y|X$ is same as $Y$
$$
f(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)} = \frac{f_X(x)f_Y(y)}{f_X(x)} = f_Y(y)
$$

- Conditional probabilities: From the above we also obtain
\begin{align*}
P(Y \in A | x) & = \sum_{y \in A} f(y|x) = \sum_{y \in A} f_Y(y) = P(Y \in A)  & \, \text{ discrete rv} \\
P(Y \in A | x) & = \int_{y \in A} f(y|x) \, dy = \int_{y \in A} f_Y(y) \, dy = P(Y \in A)  &  \, \text{ continuous rv}
\end{align*}





## Independence of random variables {.smaller}
### Characterization of independence - Densities

::: Theorem

$(X,Y)$ random vector with joint pdf or pmf $f_{X,Y}$. They are equivalent:

- $X$ and $Y$ are independent random variables
- There exist functions $g(x)$ and $h(y)$ such that
$$
f_{X,Y}(x,y) = g(x)h(y) \,, \quad \forall \, (x,y) \in \R^2
$$

::: 




## Exercise {.smaller}

A student leaves for class between 8 AM and 8:30 AM and takes between 40 and 50 minutes to get there

- Denote by $X$ the time of departure 
    * $X = 0$ corresponds to 8 AM
    * $X = 30$ corresponds to 8:30 AM
- Denote by $Y$ the travel time

- Assume that $X$ and $Y$ are independent and uniformly distributed


**Question:** Find the probability that the student arrives to class before 9 AM





## Solution {.smaller}


- By assumption $X$ is uniform on $(0,30)$. Therefore
$$
f_X(x)  = 
\begin{cases}
\frac{1}{30} & \text{ if } \, x \in (0,30)  \\
0 & \text{ otherwise }
\end{cases}
$$

- By assumption $Y$ is uniform on $(40,50)$. Therefore
$$
f_Y(y) = 
\begin{cases}
\frac{1}{10} & \text{ if } \, y \in (40,50)  \\
0 & \text{ otherwise }
\end{cases}
$$
where we used that $50 - 40 = 10$



## Solution {.smaller}


- Define the rectangle
$$
R = (0,30) \times (40,50)
$$

- Since $X$ and $Y$ are independent, we get

$$
f_{X,Y}(x,y) = f_X(x)f_Y(y) = 
\begin{cases}
\frac{1}{300} & \text{ if } \, (x,y) \in R  \\
0 & \text{ otherwise }
\end{cases}
$$



## Solution {.smaller}

- The arrival time is given by $X + Y$

- Therefore, the student arrives to class before 9 AM iff $X + Y < 60$

- Notice that 
$$
\{X + Y < 60 \}  = \{ (x,y) \in \R^2 \, \colon \, 0 \leq x < 60 - y,  40 \leq y < 50 \} 
$$

## Solution {.smaller}

Therefore, the probability of arriving before 9 AM is

\begin{align*}
P(\text{arrives before 9 AM}) & = P(X + Y < 60) \\
                              & = \int_{\{X+Y < 60\}} f_{X,Y} (x,y) \, dxdy  \\
                              & = \int_{40}^{50} \left( \int_0^{60-y} \frac{1}{300} \, dx 
                              \right) \, dy  \\
                              & = \frac{1}{300}  \int_{40}^{50} (60 - y) \, dy \\
                              & = \frac{1}{300} \ y \left(  60 - \frac{y}{2} \right) \Bigg|_{y=40}^{y=50} \\
                              & = \frac{1}{300} \cdot (1750 - 1600) = \frac12
\end{align*}







## Consequences of independence {.smaller}

::: Theorem

Suppose $X$ and $Y$ are independent random variables. Then

- For any $A,B \subset \R$ we have
$$
P(X \in A, Y \in B) = P(X \in A) P(Y \in B)
$$

- Suppose $g(x)$ is a function of (only) $x$,
$h(y)$ is a function of (only) $y$. Then
$$
\Expect[g(X)h(Y)] = \Expect[g(X)]\Expect[h(Y)]  
$$

::: 





## Application: MGF of sums {.smaller}

::: Theorem

Suppose $X$ and $Y$ are independent random variables and denote by 
$M_X$ and $M_Y$ their MGFs. Then
$$
M_{X + Y} (t) = M_X(t) M_Y(t)
$$

:::


**Proof**: Follows by previous Theorem
\begin{align*}
M_{X + Y} (t) & = \Expect[e^{t(X+Y)}] = \Expect[e^{tX}e^{tY}] \\
              & = \Expect[e^{tX}] \Expect[e^{tY}] \\
              & = M_X(t) M_Y(t)
\end{align*}







## Example - Sum of independent normals {.smaller}

- Suppose $X \sim N (\mu_1, \sigma_1^2)$ and $Y \sim N (\mu_2, \sigma_2^2)$ 
are independent normal random variables

- We have seen in Lecture 1 that for normal distributions
$$
M_X(t) = \exp \left( \mu_1 t + \frac{t^2 \sigma_1^2}{2} \right) \,, \qquad
M_Y(t) = \exp \left( \mu_2 t + \frac{t^2 \sigma_2^2}{2} \right)
$$

- Since $X$ and $Y$ are independent, from previous Theorem we get
\begin{align*}
M_{X+Y}(t) & =  M_{X}(t) M_{Y}(t) =
\exp \left( \mu_1 t + \frac{t^2 \sigma_1^2}{2} \right)
\exp \left( \mu_2 t + \frac{t^2 \sigma_2^2}{2} \right)   \\
& = \exp \left( (\mu_1 + \mu_2) t + \frac{t^2 (\sigma_1^2 + \sigma_2^2)}{2} \right)
\end{align*}





## Example - Sum of independent normals {.smaller}

- Therefore $Z := X + Y$ has moment generating function
$$
M_{Z}(t) = M_{X+Y}(t) = \exp \left( (\mu_1 + \mu_2) t + \frac{t^2 (\sigma_1^2 + \sigma_2^2)}{2} \right)
$$

- The above is the mgf of a **normal distribution** with 
$$
\text{mean }\quad \mu_1 + \mu_2 \quad \text{ and variance} \quad  \sigma_1^2 + \sigma_2^2
$$

- By the Theorem in Slide 68 of Lecture 1 we have
$$
Z \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
$$

- **Sum of independent normals is normal**








## Covariance & Correlation {.smaller}
### Relationship between RV

Given two random variables $X$ and $Y$ we said that

- $X$ and $Y$ are **independent** if
$$
f_{X,Y}(x,y) = f_X(x) g_Y(y)
$$

- In this case there is no relationship between $X$ and $Y$

- This is reflected in the conditional distributions:
$$
X|Y \sim X \qquad \qquad Y|X \sim Y
$$





## Covariance & Correlation {.smaller}
### Relationship between RV


If $X$ and $Y$ are **not independent** then there is a **relationship** between them

::: Question 

How do we measure the strength of such dependence?

:::


**Answer**: By introducing the notions of

- Covariance
- Correlation




## Covariance {.smaller}
### Definition


**Notation**: Given two rv $X$ and $Y$ we denote
\begin{align*}
& \mu_X := \Expect[X]   \qquad & \mu_Y & := \Expect[Y] \\
& \sigma^2_X := \Var[X] \qquad  & \sigma^2_Y & := \Var[Y] 
\end{align*}


::: Definition 

The **covariance** of $X$ and $Y$ is the number
$$
\Cov(X,Y) := \Expect[  (X - \mu_X) (Y - \mu_Y)  ]
$$

:::




## Covariance {.smaller}
### Alternative Formula

::: Theorem 

The covariance of $X$ and $Y$ can be computed via
$$
\Cov(X,Y) = \Expect[XY] - \Expect[X]\Expect[Y]
$$

:::







## Correlation {.smaller}

**Remark**:

- $\Cov(X,Y)$ encodes only **qualitative** information about the relationship between $X$ and $Y$

- To obtain **quantitative** information we introduce the **correlation**



::: Definition 

The **correlation** of $X$ and $Y$ is the number
$$
\rho_{XY} := \frac{\Cov(X,Y)}{\sigma_X \sigma_Y} 
$$

:::




## Correlation detects linear relationships  {.smaller}


::: Theorem

For any random variables $X$ and $Y$ we have

- $- 1\leq \rho_{XY} \leq 1$
- $|\rho_{XY}|=1$ if and only if there exist $a,b \in \R$ such that
$$
P(Y = aX + b) = 1
$$
    * If $\rho_{XY}=1$ then $a>0$  $\qquad \qquad \quad$ (positive linear correlation)
    * If $\rho_{XY}=-1$ then $a<0$ $\qquad \qquad$ (negative linear correlation)

:::


**Proof**: Omitted, see page 172 of [@casella-berger]





## Correlation \& Covariance {.smaller}
### Independent random variables


::: Theorem

If $X$ and $Y$ are independent random variables then
$$
\Cov(X,Y) = 0 \,, \qquad \rho_{XY}=0
$$

:::


**Proof**: 

- If $X$ and $Y$ are independent then $\Expect[XY]=\Expect[X]\Expect[Y]$
- Therefore $\Cov(X,Y)= \Expect[XY]-\Expect[X]\Expect[Y] = 0$
- Moreover $\rho_{XY}=0$ by definition



## Formula for Variance {.smaller}
### Variance is quadratic

::: Theorem

For any two random variables $X$ and $Y$ and $a,b \in \R$
$$
\Var[aX + bY] = a^2 \Var[X] + b^2 \Var[Y] + 2 \Cov(X,Y)
$$
If $X$ and $Y$ are independent then
$$
\Var[aX + bY] = a^2 \Var[X] + b^2 \Var[Y]
$$

:::


**Proof**: Exercise






## Example 1 {.smaller}

- Assume $X$ and $Z$ are independent, and
$$
X  \sim {\rm uniform} \left( 0,1 \right)  \,, \qquad 
Z  \sim {\rm uniform} \left( 0, \frac{1}{10} \right) 
$$


- Consider the random variable 
$$
Y = X + Z
$$

- Since $X$ and $Z$ are independent, and $Z$ is uniform, we have that
$$
Y | X = x  \, \sim \,  {\rm uniform} \left( x, x + \frac{1}{10} \right) 
$$
(adding $x$ to $Z$ simply shifts the uniform distribution of $Z$ by $x$)

- **Question:** Is the correlation $\rho_{XY}$ between $X$ and $Y$ high or low?



## Example 1 {.smaller}

- As $Y | X  \, \sim \,  {\rm uniform} \left( X, X + \frac{1}{10} \right)$, the conditional pdf of $Y$ given $X = x$ is
$$
f(y|x) = 
\begin{cases}
10 & \text{ if } \, y \in \left( x , x + \frac{1}{10} \right) \\
0  & \text{ otherwise}
\end{cases}
$$

- As $X  \sim {\rm uniform} (0,1)$, its pdf is
$$
f_X(x) = 
\begin{cases}
1 & \text{ if } \, x \in \left( 0 , 1  \right) \\
0  & \text{ otherwise}
\end{cases}
$$

- Therefore, the joint distribution of $(X,Y)$ is
$$
f_{X,Y}(x,y) = f(y|x)f_X(x) = \begin{cases}
10 & \text{ if } \, x \in (0,1) \, \text{ and } \, y \in \left( x , x + \frac{1}{10} \right) \\
0  & \text{ otherwise}
\end{cases}
$$



## Example 1 {.smaller}

In gray: the region where $f_{X,Y}(x,y)>0$

- When $X$ increases, $Y$ increases linearly (not surprising, since $Y = X + Z$)
- We expect the correlation $\rho_{XY}$ to be close to $1$

```{r}
# Load necessary library
library(ggplot2)

# Define the function to check where f_{X,Y}(x,y) is nonzero
f_XY <- function(x, y) {
  if (x >= 0 & x <= 1 & y >= x & y <= x + 1/10) {
    return(1)  # Nonzero region (1 means presence)
  } else {
    return(0)  # Zero region (0 means absence)
  }
}

# Generate a finer grid of (x, y) values
x_values <- seq(0, 1, length.out = 300)  
y_values <- seq(0, 1.1, length.out = 300)  

# Create a matrix to store function values
z_matrix <- outer(x_values, y_values, Vectorize(f_XY))

# Convert to a data frame for ggplot2
df <- expand.grid(x = x_values, y = y_values)
df$z <- as.vector(z_matrix)

# Plot the filled region with 1:1 aspect ratio
ggplot(df, aes(x, y, fill = factor(z))) +
  geom_tile() +  
  scale_fill_manual(values = c("white", "gray30")) +  # White for 0, dark gray for nonzero
  labs(x = "X", y = "Y") +  # Remove title
  theme_minimal() +
  theme(legend.position = "none",  # Remove legend for clarity
        aspect.ratio = 1)  # Set aspect ratio to 1:1

```



## Example 1 -- Computing $\rho_{XY}${.smaller}

- For a random variable $W \sim {\rm uniform} (a,b)$, we have
$$
\Expect[W] = \frac{a+b}{2} \,, \qquad 
\Var[W] = \frac{(b-a)^2}{12}
$$

- Since $X \sim {\rm uniform} (0,1)$ and $Z \sim {\rm uniform} (0,1/10)$, we have
$$
\Expect[X] = \frac12 \,, \qquad 
\Var[X] = \frac{1}{12} \,, \qquad 
\Expect[Z] = \frac{1}{20} \,, \qquad 
\Var[Z] = \frac{1}{1200}
$$


- Since $X$ and $Z$ are independent, we also have
$$
\Var[Y] = \Var[X + Z] =  \Var[X] + \Var[Z] = \frac{1}{12} + \frac{1}{1200} 
$$





## Example 1 -- Computing $\rho_{XY}${.smaller}

- Since $X$ and $Z$ are independent, we have
$$
\Expect[XZ] = \Expect[X]\Expect[Z]
$$

- We conclude that
\begin{align*}
\Cov(X,Y) & = \Expect[XY] -  \Expect[X] \Expect[Y] \\
          & = \Expect[X(X + Z)] -  \Expect[X] \Expect[X + Z] \\
          & = \Expect[X(X + Z)] -  \Expect[X] \Expect[X + Z] \\
          & = \Expect[X^2] - \Expect[X]^2 + \Expect[XZ] - \Expect[X]\Expect[Z] \\
          & = \Var[X] = \frac{1}{12}
\end{align*}



## Example 1 -- Computing $\rho_{XY}${.smaller}


- The correlation between $X$ and $Y$ is
\begin{align*}
\rho_{XY} & = \frac{\Cov(X,Y)}{\sqrt{\Var[X]}\sqrt{\Var[Y]}} \\
          & = \frac{\frac{1}{12}}{\sqrt{\frac{1}{12}}  \sqrt{ \frac{1}{12} + \frac{1}{1200}} } = \sqrt{\frac{100}{101}}
\end{align*}

- As expected, we have very high correlation $\rho_{XY} \approx 1$

- This confirms a very strong **linear** relationship between $X$ and $Y$







## Example 2 {.smaller}

- Assume $X$ and $Z$ are independent, and
$$
X  \sim {\rm uniform} \left( -1,1 \right)  \,, \qquad 
Z  \sim {\rm uniform} \left( 0, \frac{1}{10} \right) 
$$


- Define the random variable 
$$
Y = X^2 + Z
$$

- Since $X$ and $Z$ are independent, and $Z$ is uniform, we have that
$$
Y | X = x  \, \sim \,  {\rm uniform} \left( x^2, x^2 + \frac{1}{10} \right) 
$$
(adding $x$ to $Z$ simply shifts the uniform distribution of $Z$ by $x$)

- **Question:** Is the correlation $\rho_{XY}$ between $X$ and $Y$ high or low?



## Example 2 {.smaller}

- As $Y | X  \, \sim \,  {\rm uniform} \left( X^2, X^2 + \frac{1}{10} \right)$, the conditional pdf of $Y$ given $X = x$ is
$$
f(y|x) = 
\begin{cases}
10 & \text{ if } \, y \in \left( x^2 , x^2 + \frac{1}{10} \right) \\
0  & \text{ otherwise}
\end{cases}
$$

- As $X  \sim {\rm uniform} (-1,1)$, its pdf is
$$
f_X(x) = 
\begin{cases}
\frac12 & \text{ if } \, x \in \left( -1 , 1  \right) \\
0  & \text{ otherwise}
\end{cases}
$$

- Therefore, the joint distribution of $(X,Y)$ is
$$
f_{X,Y}(x,y) = f(y|x)f_X(x) = \begin{cases}
10 & \text{ if } \, x \in (-1,1) \, \text{ and } \, y \in \left( x^2 , x^2 + \frac{1}{10} \right) \\
0  & \text{ otherwise}
\end{cases}
$$



## Example 2 {.smaller}

In gray: the region where $f_{X,Y}(x,y)>0$

- When $X$ increases, $Y$ increases quadratically (not surprising, as $Y = X^2 + Z$)
- There is no linear relationship between $X$ and $Y$ $\,\, \implies \,\,$ we expect $\, \rho_{XY} \approx 0$

```{r}
# Load necessary library
library(ggplot2)

# Define the function to check where f_{X,Y}(x,y) is nonzero
f_XY <- function(x, y) {
  if (x > -1 & x < 1 & y > x^2 & y < x^2 + 1/10) {
    return(1)  # Nonzero region (1 means presence)
  } else {
    return(0)  # Zero region (0 means absence)
  }
}

# Generate a finer grid of (x, y) values
x_values <- seq(-1, 1, length.out = 300)  # x in (-1,1)
y_values <- seq(0, 1.1, length.out = 300)  # y in a reasonable range

# Create a matrix to store function values
z_matrix <- outer(x_values, y_values, Vectorize(f_XY))

# Convert to a data frame for ggplot2
df <- expand.grid(x = x_values, y = y_values)
df$z <- as.vector(z_matrix)

# Plot the filled region with 1:1 aspect ratio
ggplot(df, aes(x, y, fill = factor(z))) +
  geom_tile() +  
  scale_fill_manual(values = c("white", "gray30")) +  # White for 0, dark gray for nonzero
  labs(x = "X", y = "Y") +
  theme_minimal() +
  theme(legend.position = "none",  # Remove legend for clarity
        aspect.ratio = 1)  # Set aspect ratio to 1:1

```



## Example 2 -- Computing $\rho_{XY}${.smaller}

- Since $X \sim {\rm uniform} (-1,1)$, we can compute that
$$
\Expect[X] = \Expect[X^3] = 0
$$


- Since $X$ and $Z$ are independent, we have
$$
\Expect[XZ] = \Expect[X]\Expect[Z] = 0
$$




## Example 2 -- Computing $\rho_{XY}${.smaller}

- Compute the covariance
\begin{align*}
\Cov(X,Y) & = \Expect[XY] -  \Expect[X] \Expect[Y] \\
          & = \Expect[XY] \\
          & = \Expect[X(X^2 + Z)]  \\
          & = \Expect[X^3] + \Expect[XZ] = 0
\end{align*}


- The correlation between $X$ and $Y$ is
$$
\rho_{XY}  = \frac{\Cov(X,Y)}{\sqrt{\Var[X]}\sqrt{\Var[Y]}} = 0 
$$


- This confirms there is **no linear** relationship between $X$ and $Y$









# Part 2: <br>Multivariate random vectors {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Multivariate Random Vectors {.smaller}
### Recall

- A **Random vector** is a function 
$$
\XX \colon \Omega \to \R^n
$$
- $\XX$ is a **multivariate** random vector if $n \geq 3$
- We denote the components of $\XX$ by
$$
\XX = (X_1,\ldots,X_n) \,, \qquad X_i \colon \Omega \to \R
$$
- We denote the components of a point $\xx \in \R^n$ by
$$
\xx = (x_1,\ldots,x_n)
$$



## Discrete and Continuous Multivariate Random Vectors {.smaller}

Everything we defined for **bivariate** vectors extends to **multivariate** vectors

::: Definition

The random vector $\XX \colon \Omega \to \R^n$ is:

- **continuous** if components $X_i$s are continuous
- **discrete** if components $X_i$ are discrete

:::






## Joint pmf {.smaller}

::: Definition 

The **joint pmf** of a continuous random vector $\XX$ is $f_{\XX} \colon \R^n \to \R$ defined by
$$
f_{\XX} (\xx) = f_{\XX}(x_1,\ldots,x_n) := P(X_1 = x_1 , \ldots , X_n = x_n ) \,, \qquad \forall \, \xx \in \R^n
$$

:::

**Note:** For all $A \subset \R^n$ it holds
$$
P(\XX \in A) = \sum_{\xx \in A} f_{\XX}(\xx)
$$






## Joint pdf {.smaller}

::: Definition 

The **joint pdf** of a continuous random vector $\XX$ is a function $f_{\XX} \colon \R^n \to \R$ such that
$$
P (\XX \in A) := \int_A f_{\XX}(x_1 ,\ldots, x_n) \, dx_1 \ldots dx_n = \int_{A} f_{\XX}(\xx) \, d\xx \,, \quad \forall \, A \subset \R^n
$$

:::

**Note**: $\int_A$ denotes an $n$-fold intergral over all points $\xx \in A$





## Expected Value {.smaller}

::: Definition 

$\XX \colon \Omega \to \R^n$ random vector and $g \colon \R^n \to \R$ function. The **expected value** of the random variable $g(X)$ is
\begin{align*}
\Expect[g(\XX)] & := \sum_{x \in \R^n} g(\xx) f_{\XX} (\xx)  \qquad & (\XX \text{ discrete}) \\
\Expect[g(\XX)] & := \int_{\R^n} g(\xx) f_{\XX} (\xx) \, d\xx \qquad & \qquad  (\XX \text{ continuous})
\end{align*}


:::





## Marginal distributions {.smaller}


- **Marginal pmf or pdf** of any **subset** of the coordinates $(X_1,\ldots,X_n)$ can be computed by integrating or summing the remaining coordinates

- To ease notations, we only define maginals wrt the first $k$ coordinates



::: Definition 

The **marginal pmf** or **marginal pdf** of the random vector $\XX$ with 
respect to the first $k$ coordinates is the function $f \colon \R^k \to \R$ defined by
\begin{align*}
f(x_1,\ldots,x_k) & := \sum_{ (x_{k+1}, \ldots, x_n) \in \R^{n-k} }  
f_{\XX} (x_1 , \ldots , x_n)  \quad & (\XX \text{ discrete}) \\
f(x_1,\ldots,x_k) & := \int_{\R^{n-k}}f_{\XX} (x_1 , \ldots, x_n ) \, dx_{k+1} \ldots dx_{n} \quad & \quad  (\XX \text{ continuous})
\end{align*}


:::





## Marginal distributions {.smaller}


We use a special notation for **marginal pmf or pdf** wrt a single coordinate


::: Definition 

The **marginal pmf** or **pdf** of the random vector $\XX$ with 
respect to the $i$-th coordinate is the function $f_{X_i} \colon \R \to \R$ defined by
\begin{align*}
f_{X_i}(x_i) & := \sum_{ \tilde{x} \in \R^{n-1} }  
f_{\XX} (x_1, \ldots, x_n)  \quad & (\XX \text{ discrete}) \\
f_{X_i}(x_i) & := \int_{\R^{n-1}}f_{\XX} (x_1, \ldots, x_n) \, d\tilde{x} \quad & \quad  (\XX \text{ continuous})
\end{align*}
where $\tilde{x} \in \R^{n-1}$ denotes the vector $\xx$ with $i$-th component removed
$$
\tilde{x} := (x_1, \ldots, x_{i-1}, x_{i+1},\ldots, x_n)
$$

:::




## Conditional distributions {.smaller}

We now define conditional distributions given the first $k$ coordinates


::: Definition 

Let $\XX$ be a random vector and suppose that the marginal pmf or pdf wrt the first $k$ coordinates satisfies 
$$
f(x_1,\ldots,x_k) > 0 \,, \quad \forall \, (x_1,\ldots,x_k ) \in \R^k
$$ 
The **conditional pmf** or **pdf** of $(X_{k+1},\ldots,X_n)$ given $X_1 = x_1, \ldots , X_k = x_k$ is the function of $(x_{k+1},\ldots,x_{n})$ defined by
$$
f(x_{k+1},\ldots,x_n | x_1 , \ldots , x_k) := \frac{f_{\XX}(x_1,\ldots,x_n)}{f(x_1,\ldots,x_k)}
$$

:::



## Conditional distributions {.smaller}

Similarly, we can define the conditional distribution given the $i$-th coordinate


::: Definition 

Let $\XX$ be a random vector and suppose that for a given $x_i \in \R$
$$
f_{X_i}(x_i) > 0
$$ 
The **conditional pmf or pdf** of $\tilde{X}$ given $X_i = x_i$ is the function of $\tilde{x}$ defined by
$$
f(\tilde{x} | x_i ) := \frac{f_{\XX}(x_1,\ldots,x_n)}{f_{X_i}(x_i)}
$$
where we denote
$$
\tilde{X} := (X_1, \ldots, X_{i-1}, X_{i+1},\ldots, X_n) \,, \quad 
\tilde{x} := (x_1, \ldots, x_{i-1}, x_{i+1},\ldots, x_n)
$$

:::





## Independence {.smaller}


::: Definition

$\XX=(X_1,\ldots,X_n)$ random vector with joint pmf or pdf $f_{\XX}$ and marginals $f_{X_i}$. We say that the random variables $X_1,\ldots,X_n$ are **mutually independent** if
$$
f_{\XX}(x_1,\ldots,x_n) = f_{X_1}(x_1) \cdot \ldots \cdot f_{X_n}(x_n) = \prod_{i=1}^n f_{X_i}(x_i)
$$

:::


::: Proposition

If $X_1,\ldots,X_n$ are mutually independent then for all $A_i \subset \R$
$$
P(X_1 \in A_1 , \ldots , X_n \in A_n) = \prod_{i=1}^n P(X_i \in A_i)
$$

:::




## Independence {.smaller}
### Characterization result

::: Theorem

$\XX=(X_1,\ldots,X_n)$ random vector with joint pmf or pdf $f_{\XX}$. They are equivalent:

- The random variables $X_1,\ldots,X_n$ are **mutually independent**
- There exist functions $g_i(x_i)$ such that
$$
f_{\XX}(x_1,\ldots,x_n) =  \prod_{i=1}^n g_{i}(x_i)
$$

:::




## Independence {.smaller}
### A very useful theorem


::: Theorem

Let $X_1,\ldots,X_n$ be mutually independent random variables and $g_i(x_i)$ function only of $x_i$. Then the random variables
$$
g_1(X_1) \,, \ldots \,, g_n(X_n)
$$
are mutually independent

:::

**Proof**: Omitted. See [@casella-berger] page 184

**Example**: $X_1,\ldots,X_n \,$ independent $\,\, \implies \,\, X_1^2, \ldots, X_n^2 \,$ independent





## Independence {.smaller}
### Expectation of product


::: Theorem

Let $X_1,\ldots,X_n$ be mutually independent random variables and $g_i(x_i)$ functions. Then
$$
\Expect[ g_1(X_1) \cdot \ldots \cdot g_n(X_n) ] = \prod_{i=1}^n \Expect[g_i(X_i)]
$$

:::






## Application: MGF of sums {.smaller}

::: Theorem

Let $X_1,\ldots,X_n$ be mutually independent random variables, with mgfs $M_{X_1}(t),\ldots, M_{X_n}(t)$. Define the random variable
$$
Z := X_1 + \ldots + X_n
$$
The mgf of $Z$ satisfies
$$
M_Z(t) = \prod_{i=1}^n M_{X_i}(t)
$$

:::



## Application: MGF of sums {.smaller}
### Proof of Theorem

Follows by the previous Theorem

\begin{align*}
M_{Z} (t) & = \Expect[e^{tZ}] \\
          & = \Expect[\exp( t X_1 + \ldots + tX_n)] \\
          & = \Expect\left[  e^{t X_1} \cdot \ldots \cdot e^{ t X_n} \right] \\
          & = \prod_{i=1}^n \Expect[e^{tX_i}] \\
          & = \prod_{i=1}^n M_{X_i}(t)
\end{align*}




## Example -- Sum of independent Normals {.smaller}

::: Theorem

Let $X_1,\ldots,X_n$ be mutually independent random variables with normal distribution $X_i \sim N (\mu_i,\sigma_i^2)$. Define
$$
Z := X_1 + \ldots + X_n
$$
and
$$
\mu :=\mu_1 + \ldots + \mu_n    \,, \quad \sigma^2 := \sigma_1^2 + \ldots + \sigma_n^2
$$
Then $Z$ is normally distributed with
$$
Z \sim N(\mu,\sigma^2)
$$

:::





## Example -- Sum of independent Normals {.smaller}
### Proof of Theorem

- We have seen in Lecture 1 that
$$
X_i \sim N(\mu_i,\sigma_i^2) \quad \implies \quad
M_{X_i}(t) = \exp \left( \mu_i t + \frac{t^2 \sigma_i^2}{2} \right)
$$

- As $X_1,\ldots,X_n$ are mutually independent, from the  Theorem in Slide 47, we get
\begin{align*}
M_{Z}(t) & = \prod_{i=1}^n M_{X_i}(t) = 
\prod_{i=1}^n \exp \left( \mu_i t + \frac{t^2 \sigma_i^2}{2} \right) \\
& = \exp \left( (\mu_1 + \ldots + \mu_n) t + \frac{t^2 (\sigma_1^2 + \ldots +\sigma_n^2)}{2} \right) \\
& =  \exp \left( \mu t + \frac{t^2 \sigma^2 }{2} \right) 
\end{align*}



## Example -- Sum of independent Normals {.smaller}
### Proof of Theorem

- Therefore $Z$ has moment generating function
$$
M_{Z}(t) = \exp \left( \mu t + \frac{t^2 \sigma^2 }{2} \right) 
$$

- The above is the mgf of a **normal distribution** with 
$$
\text{mean }\quad \mu  \quad \text{ and variance} \quad  \sigma^2 
$$

- Since mgfs characterize distributions (see Theorem in Slide 71 of Lecture 1), we conclude
$$
Z \sim N(\mu, \sigma^2 )
$$




## Example -- Sum of independent Gammas {.smaller}

::: Theorem

Let $X_1,\ldots,X_n$ be mutually independent random variables with Gamma distribution $X_i \sim \Gamma (\alpha_i,\beta)$. Define
$$
Z := X_1 + \ldots + X_n
$$
and
$$
\alpha :=\alpha_1 + \ldots + \alpha_n 
$$
Then $Z$ has Gamma distribution
$$
Z \sim \Gamma(\alpha,\beta)
$$

:::




## Example -- Sum of independent Gammas {.smaller}
### Proof of Theorem


- We have seen in Lecture 1 that
$$
X_i \sim \Gamma(\alpha_i,\beta)
\qquad \implies \qquad 
M_{X_i}(t) = \frac{\beta^{\alpha_i}}{(\beta-t)^{\alpha_i}}
$$

- As $X_1,\ldots,X_n$ are mutually independent, from the  Theorem in Slide 47, we get
\begin{align*}
M_{Z}(t) & = \prod_{i=1}^n M_{X_i}(t) = 
\prod_{i=1}^n \frac{\beta^{\alpha_i}}{(\beta-t)^{\alpha_i}} \\
& = \frac{\beta^{(\alpha_1 + \ldots + \alpha_n)}}{(\beta-t)^{(\alpha_1 + \ldots + \alpha_n)}} \\
& = \frac{\beta^{\alpha}}{(\beta-t)^{\alpha}} 
\end{align*}




## Example -- Sum of independent Gammas {.smaller}
### Proof of Theorem

- Therefore $Z$ has moment generating function
$$
M_{Z}(t) = \frac{\beta^{\alpha}}{(\beta-t)^{\alpha}} 
$$

- The above is the mgf of a **Gamma distribution** with parameters $\alpha$ and $\beta$

- Since mgfs characterize distributions (see Theorem in Slide 71 of Lecture 1), we conclude
$$
Z \sim \Gamma(\alpha, \beta )
$$








## Expectation of sums {.smaller}
### Expectation is linear


::: Theorem

For random variables $X_1,\ldots,X_n$ and scalars $a_1,\ldots,a_n$ we have
$$
\Expect[a_1X_1 + \ldots + a_nX_n] = a_1 \Expect[X_1] + \ldots + a_n \Expect[X_n]
$$

:::




## Variance of sums {.smaller}
### Variance is quadratic


::: Theorem

For random variables $X_1,\ldots,X_n$ and scalars $a_1,\ldots,a_n$ we have
\begin{align*}
\Var[a_1X_1 + \ldots + a_nX_n] = a_1^2 \Var[X_1] & + \ldots + a^2_n \Var[X_n] \\
                                                 & + 2 \sum_{i \neq j} \Cov(X_i,X_j)
\end{align*}
If $X_1,\ldots,X_n$ are mutually independent then
$$
\Var[a_1X_1 + \ldots + a_nX_n] = a_1^2 \Var[X_1] + \ldots + a^2_n \Var[X_n]
$$

:::





# Part 3: <br>Random samples {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## iid random variables {.smaller}


::: Definition

The random variables $X_1,\ldots,X_n$ are **independent identically distributed** or **iid** with pdf or pmf $f(x)$ if

- $X_1,\ldots,X_n$ are mutually independent
- The marginal pdf or pmf of each $X_i$ satisfies
$$
f_{X_i}(x) = f(x) \,, \quad \forall \, x \in \R
$$

:::




## Random sample {.smaller}

- Suppose the data in an experiment consists of **observations** on a **population**
- Suppose the **population** has distribution $f(x)$
- Each observation is labelled $X_i$
- We always assume that the population is **infinite**
- Therefore each $X_i$ has distribution $f(x)$
- We also assume the observations are **independent**

::: Definition

The random variables $X_1,\ldots,X_n$ are a **random sample** of size $n$ from the population $f(x)$ if $X_1,\ldots,X_n$ are iid with pdf or pmf $f(x)$

:::



## Random sample {.smaller}


**Remark**: Let $X_1,\ldots,X_n$ be a random sample of size $n$ from the population $f(x)$. The joint distribution of $\XX = (X_1,\ldots,X_n)$ is
$$
f_{\XX}(x_1,\ldots,x_n) = f(x_1) \cdot \ldots \cdot f(x_n) = \prod_{i=1}^n f(x_i)
$$
(since the $X_is$ are mutually independent with distribution $f$)

::: Definition

We call $f_{\XX}$ the **joint sample distribution**

:::




## Random sample {.smaller}


**Notation**: 

- When the population distribution $f(x)$ depends on a parameter $\theta$ we write
$$
f = f(x|\theta)
$$

- In this case the joint sample distribution is 
$$
f_{\XX}(x_1,\ldots,x_n | \theta) =  \prod_{i=1}^n f(x_i | \theta)
$$



## Example {.smaller}

- Suppose a population has $\exponential(\beta)$ distribution
$$
f(x|\beta) = \frac{1}{\beta} e^{-x/\beta} \,, \qquad \text{ if } \,\, x > 0 
$$
- Suppose $X_1,\ldots,X_n$ is random sample from the population $f(x|\beta)$
- The joint sample distribution is then
\begin{align*}
f_{\XX}(x_1,\ldots,x_n | \beta) & = \prod_{i=1}^n f(x_i|\beta) \\
                                & = \prod_{i=1}^n \frac{1}{\beta} e^{-x_i/\beta} \\
                                & = \frac{1}{\beta^n} e^{-(x_1 + \ldots + x_n)/\beta}
\end{align*}




## Example {.smaller}

- We have 
$$
P(X_1 > 2) = \int_{2}^\infty f(x|\beta) \, dx 
           = \int_{2}^\infty \frac{1}{\beta} e^{-x/\beta} \, dx = e^{-2/\beta}
$$

- Thanks to iid assumption we can easily compute
\begin{align*}
P(X_1 > 2 , \ldots, X_n > 2) & = \prod_{i=1}^n P(X_i > 2) \\
                             & = \prod_{i=1}^n P(X_1 > 2) \\
                             & = P(X_1 > 2)^n \\
                             & = e^{-2n/\beta}
\end{align*}





# Part 4: <br>Unbiased estimators {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Point estimation {.smaller}

**Usual situation**: Suppose a population has distribution
$$
f(x|\theta)
$$

- In general, the parameter $\theta$ is **unknown**
- Suppose that knowing $\theta$ is sufficient to characterize $f(x|\theta)$

**Example**: A population could be normally distributed
$$
f(x|\mu,\sigma^2)  = \frac{1}{\sqrt{2\pi\sigma}} \, \exp\left( -\frac{(x-\mu)^2}{2\sigma^2}\right) \,, \quad x \in \R
$$

- Here $\mu$ is the mean and $\sigma^2$ the variance
- Knowing $\mu$ and $\sigma^2$ completely characterizes the normal distribution





## Point estimation {.smaller}


**Goal:** We want to make predictions about the population

- In order to do that, we need to know the population distribution 
$$
f(x|\theta)
$$

- It is therefore desirable to determine $\theta$, with reasonable certanty


**Definitions:**

- **Point estimation** is the procedure of estimating $\theta$ from random sample
- A **point estimator** is any function of a random sample
$$
W(X_1,\ldots,X_n)
$$

- Point estimators are also called **statistics**





## Unbiased estimator {.smaller}

::: Definition

Suppose $W$ is a point estimator of a parameter $\theta$

- The **bias** of $W$ is the quantity $\rm{Bias}_{\theta} := \Expect[W] - \theta$

- $W$ is an **unbiased estimator** if $\rm{Bias}_{\theta} = 0$, that is,
$$
\Expect[W] = \theta
$$

:::


**Note**: A point estimator 
$$
W = W(X_1, \ldots, X_n)
$$
is itself a random variable. Thus $\Expect[W]$ is the mean of such random variable





## Next goal {.smaller}

- We want to estimate mean and variance of a population


- Unbiased estimators for such quantities are:
    * Sample mean
    * Sample variance




## Estimating the population mean {.smaller}

::: Problem

Suppose to have a population with distribution
$$
f(x|\theta)
$$
We want to estimate the **population mean**
$$
\mu := \int_{\R} x f(x|\theta) \, dx
$$

:::





## Sample mean {.smaller}


::: Definition

The **sample mean** of a random sample $X_1,\ldots,X_n$ is the statistic
$$
W(X_1,\ldots,X_n) := \overline{X} := \frac{1}{n} \sum_{i=1}^n X_i
$$

:::




## Sample mean {.smaller}
### Sample mean is unbiased estimator of mean

::: Theorem

The sample mean $\overline{X}$ is an unbiased estimator of the population mean $\mu$, that is,
$$
\Expect[\overline{X}] = \mu
$$

:::



## Sample mean {.smaller}
### Proof of theorem


- $X_1,\ldots,X_n$ is a random sample from $f(x|\theta)$
- Therefore $X_i \sim f(x|\theta)$ and
$$
\Expect[X_i] = \int_{\R} x f(x|\theta) \, dx = \mu
$$
- By linearity of expectation we have
$$
\Expect[\overline{X}] = \frac{1}{n} \sum_{i=1}^n \Expect[X_i] 
                      = \frac{1}{n} \sum_{i=1}^n \mu 
                      = \mu
$$

- This shows $\overline{X}$ is an unbiased estimator of $\mu$



## Variance of Sample mean {.smaller}

For reasons clear later, it is useful to compute the variance of the sample mean $\overline{X}$

::: Lemma

$X_1,\ldots,X_n$ random sample from population with mean $\mu$ and 
variance $\sigma^2$. Then
$$
\Var[\overline{X}] = \frac{\sigma^2}{n}
$$
:::




## Variance of Sample mean {.smaller}
### Proof of Lemma

- By assumption,the population has mean $\mu$ and 
variance $\sigma^2$

- Since $X_i$ is sampled from the population, we have
$$
\Expect[X_i] = \mu \,, \quad \Var[X_i] = \sigma^2
$$

- Since the variance is quadratic, and the $X_is$ are independent,
\begin{align*}
\Var[\overline{X}] & = \Var \left[ \frac{1}{n} \sum_{i=1}^n X_i \right] 
                     = \frac{1}{n^2} \sum_{i=1}^n \Var[X_i] \\
                   & = \frac{1}{n^2} \cdot n \sigma^2
                     = \frac{\sigma^2}{n} 
\end{align*}





## Estimating the population variance {.smaller}

::: Problem

Suppose to have a population 
$$
f(x|\theta)
$$
with mean $\mu$ and variance $\sigma^2$. We want to estimate the **population variance**

:::




## Sample variance {.smaller}

::: Definition

The **sample variance** of a random sample $X_1,\ldots,X_n$ is the statistic
$$
S^2 := \frac{1}{n-1} \sum_{i=1}^n \left( X_i - \overline{X}  \right)^2 
$$
where $\overline{X}$ is the sample mean
$$
\overline{X} := \frac{1}{n} \sum_{i=1}^n X_i
$$

:::



## Sample variance {.smaller}
### Equivalent formulation


::: Proposition

It holds that
$$
S^2 := \frac{ \sum_{i=1}^n \left( X_i - \overline{X}  \right)^2}{n-1}  = 
\frac{ \sum_{i=1}^n  X_i^2  - n\overline{X}^2  }{n-1}
$$

:::



## Sample variance {.smaller}
### Proof of Proposition

- We have
\begin{align*}
\sum_{i=1}^n \left( X_i - \overline{X}  \right)^2  & =
\sum_{i=1}^n \left(X_i^2 + \overline{X}^2 - 2 X_i \overline{X}  \right) 
 = \sum_{i=1}^n X_i^2 + n\overline{X}^2 - 2  \overline{X}  \sum_{i=1}^n X_i \\ 
& = \sum_{i=1}^n X_i^2 + n\overline{X}^2 - 2 n \overline{X}^2 
 = \sum_{i=1}^n X_i^2 -n \overline{X}^2 
\end{align*}


- Dividing by $n-1$ yields the desired identity
$$
S^2 = \frac{ \sum_{i=1}^n X_i^2 -n \overline{X}^2 }{n-1}
$$





## Sample variance {.smaller}
### Sample variance is unbiased estimator of variance

::: Theorem

The sample variance $S^2$ is an unbiased estimator of the population variance $\sigma^2$, that is,
$$
\Expect[S^2] = \sigma^2
$$

:::






## Sample variance {.smaller}
### Proof of theorem


- By linearity of expectation we infer
$$
\Expect[(n-1)S^2]  = \Expect \left[  \sum_{i=1}^n X_i^2 - n\overline{X}^2  \right] 
                   = \sum_{i=1}^n \Expect[X_i^2] - n \Expect[\overline{X}^2] 
$$



- Since $X_i \sim f(x|\theta)$, we have
$$
\Expect[X_i] = \mu \,, \quad \Var[X_i] = \sigma^2
$$

- Therefore by definition of variance, we infer
$$
\Expect[X_i^2] = \Var[X_i] + \Expect[X]^2 = \sigma^2 + \mu^2
$$





## Sample variance {.smaller}
### Proof of theorem


- Also recall that
$$
\Expect[\overline{X}] = \mu \,, \quad \Var[\overline{X}] = \frac{\sigma^2}{n} 
$$


- By definition of variance, we get
$$
\Expect[\overline{X}^2] = \Var[\overline{X}] + \Expect[\overline{X}]^2
                        = \frac{\sigma^2}{n} + \mu^2
$$




## Sample variance {.smaller}
### Proof of theorem


- Hence
\begin{align*}
\Expect[(n-1)S^2] & = \sum_{i=1}^n \Expect[X_i^2] - n \Expect[\overline{X}^2] \\
                  & = \sum_{i=1}^n \left( \mu^2 + \sigma^2 \right) - 
                      n \left(  \mu^2 + \frac{\sigma^2}{n}  \right)  \\
                  & = n\mu^2 + n\sigma^2 - 
                      n \mu^2 - \sigma^2   \\
                  & = (n-1) \sigma^2
\end{align*}

- Dividing both sides by $(n-1)$ yields the thesis
$$
\Expect[S^2] = \sigma^2
$$






## Additional note {.smaller}

- The sample variance is defined by
$$
S^2=\frac{\sum_{i=1}{n}(X_i-\overline{X})^2}{n-1}=\frac{\sum_{i=1}^n X_i^2-n{\overline{X}^2}}{n-1}
$$

- Where does the $n-1$ factor in the denominator come from?  
(It would look more natural to divide by $n$, instead that by $n-1$)

- The $n-1$ factor 
is caused by a loss of precision:
    * Ideally, the sample variance $S^2$ should contain the population mean $\mu$
    * Since $\mu$ is not available, we estimate it with the sample mean $\overline{X}$
    * This leads to the loss of 1 degree of freedom


## Additional note {.smaller}

- General statistical rule:
$$
\text{Lose 1 degree of freedom for each parameter estimated}
$$

- In the case of the sample variance $S^2$, we have to estimate one parameter (the population mean $\mu$). Hence
\begin{align*}
\text{degrees of freedom} & = \text{Sample size}-\text{No. of estimated parameters} \\
                          & = n-1
\end{align*}


- This is where the $n-1$ factor comes from!



## Notation {.smaller}


- The realization of a random sample $X_1,\ldots,X_n$ is denoted by
$$
x_1, \ldots, x_n
$$

- The realization of the sample mean $\overline{X}$ is denoted
$$
\overline{x} := \frac{1}{n} \sum_{i=1}^n x_i
$$

- The realization of the sample variance $S^2$ is denoted 
$$
s^2=\frac{\sum_{i=1}{n}(x_i-\overline{x})^2}{n-1}=\frac{\sum_{i=1}^n x_i^2-n{\overline{x}^2}}{n-1}
$$


- **Capital letters denote random variables, while lowercase letters denote specific values (realizations) of those variables**





## Example calculation {.smaller}

- Wage data on 10 Advertising Professionals Accountants

|**Professional**| $x_1$ | $x_2$| $x_3$ | $x_4$ | $x_5$ | $x_6$ | $x_7$ | $x_8$ | $x_9$ |$x_{10}$|
|:--------------:|:-----:|:----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:----:|:-----:|
|     **Wage**   |   36  |  40  |  46  |  54  |  57  |  58  |  59  |  60  |  62  |  63  |


<br>

- **Task**: Estimate **population mean** and **variance**




## Solution to the example {.smaller}

- **Number** of advertising professionals $n=10$

- **Sample Mean:**
$$
\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i = \frac{36+40+46+{\dots}+62+63}{10}=\frac{535}{10}=53.5
$$

- **Sample Variance**:
\begin{align*}
  s^2 &  = \frac{\sum_{i=1}^n  x_{i}^2 - n \overline{x}^2}{n-1} \\
  \sum_{i=1}^n x_i^2 & = 36^2+40^2+46^2+{\ldots}+62^2+63^2 = 29435 \\
  s^2 & = \frac{29435-10(53.5)^2}{9} = 90.2778
\end{align*}








# Part 5: <br>Chi-squared distribution {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Overview {.smaller}

Chi-squared distribution:

- defined in terms of squares of $N(0, 1)$ random variables
- designed to describe variance estimation
- used to define other members of the normal family
    * Student t-distribution
    * F-distribution





## Why the normal family is important {.smaller}

- Classical hypothesis testing and regression problems
- The same maths solves apparently unrelated problems
- Easy to compute
    * Statistics tables
    * Software
- Enables the development of approximate methods in more complex (and interesting) problems





## Reminder: Normal distribution {.smaller}


- $X$ has **normal distribution** with mean $\mu$ and variance $\sigma^2$ if pdf is
$$
f(x) := \frac{1}{\sqrt{2\pi\sigma^2}} \, \exp\left( -\frac{(x-\mu)^2}{2\sigma^2}\right) \,, \quad x \in \R
$$


- In this case we write
$$
X \sim N(\mu,\sigma^2)
$$ 

- The **standard normal distribution** is denoted $N(0,1)$




## Chi-squared distribution {.smaller}
### Definition

::: Definition

Let $Z_1,\ldots,Z_r$ be iid $N(0, 1)$ random variables. The **chi-squared distribution** with **$r$ degrees of freedom** is the distribution
$$
\chi^2_r \sim  Z^2_1+...+Z^2_r
$$

:::


## Chi-squared distribution {.smaller}
### Pdf characterization 


::: Theorem

The $\chi^2_r$ distribution is equivalent to a Gamma distribution
$$
\chi^2_r \sim \Gamma(r/2, 1/2)
$$
Therefore the pdf of $\chi^2_r$ can be written in closed form as
$$
f_{\chi^2_r}(x)=\frac{x^{(r/2)-1} \, e^{-x/2}}{\Gamma(r/2) 2^{r/2}} \,, \quad x>0
$$

:::





## Chi-squared distribution {.smaller}
### Plots of chi-squared pdf for different choices of r


```{r}
# Set the degrees of freedom
degree1 <- 1
degree2 <- 3
degree3 <- 6

# Generate values for the x-axis
x_values <- seq(0.1, 10, by = 0.1)

# Calculate the probability density function (PDF) values for each x for the 3 distributions
pdf_values1 <- dchisq(x_values, df = degree1)
pdf_values2 <- dchisq(x_values, df = degree2)
pdf_values3 <- dchisq(x_values, df = degree3)


# Plot the gamma PDFs
plot(x_values, 
    pdf_values1, 
    type = "l", 
    col = "blue", lwd = 2,
    xlab = "", 
    ylab = "",
    ylim = c(0, max(pdf_values2, pdf_values3) + 0.1))

lines(x_values, 
      pdf_values2, 
      col = "red", 
      lwd = 2)

lines(x_values, 
      pdf_values3, 
      col = "black", 
      lwd = 2)

mtext("x", side=1, line=3, cex=2)
mtext("pdf", side=2, line=2.5, cex=2)

# Add a legend
legend("topright", legend = c(paste("r =", degree1),
                              paste("r =", degree2),
                              paste("r =", degree3)),
       col = c("blue", "red", "black"), lty = 1, lwd = 2, cex = 1.5)

``` 



## Proof of Theorem -- Case $r =1$ {.smaller}

- We start with the case $r=1$
- Need to prove that
$$
\chi^2_1 \sim \Gamma(1/2, 1/2)
$$
- Therefore we need to show that the pdf of $\chi^2_1$ is
$$
f_{\chi^2_1}(x)=\frac{x^{-1/2} \, e^{-x/2}}{\Gamma(1/2) 2^{1/2}} \,, \quad x>0
$$


## Proof of Theorem -- Case $r =1$ {.smaller}

- To this end, notice that by definition
$$
\chi^2_1 \sim Z^2 \,, \qquad Z \sim N(0,1)
$$
- Hence, for $x>0$ we can compute **cdf** via
\begin{align*}
F_{\chi^2_1}(x) & = P(\chi^2_1 \leq x) \\
                & = P(Z^2 \leq x ) \\
                & = P(- \sqrt{x} \leq Z  \leq \sqrt{x} ) \\
                & = 2 P (0 \leq Z \leq \sqrt{x})
\end{align*}
where in the last equality we used symmetry of $Z$ around $x=0$



## Proof of Theorem -- Case $r =1$ {.smaller}

- Recalling the definition of **standard normal pdf** we get
\begin{align*}
F_{\chi^2_1}(x) & = 2 P (0 \leq Z \leq \sqrt{x}) \\
                & = 2 \frac{1}{\sqrt{2\pi}} 
                    \int_0^{\sqrt{x}} e^{-t^2/2} \, dt \\
                & = 2 \frac{1}{\sqrt{2\pi}} G( \sqrt{x} )
\end{align*}
where we set 
$$
G(x) := \int_0^{x} e^{-t^2/2} \, dt
$$



## Proof of Theorem -- Case $r =1$ {.smaller}

- We can now compute pdf of $\chi_1^2$ by differentiating the cdf

- By the Fundamental Theorem of Calculus we have
$$
G'(x) = \frac{d}{dx} \left( \int_0^{x} e^{-t^2/2} \, dt \right) = e^{-x^2/2} \quad \implies \quad 
G'(\sqrt{x}) = e^{-x/2}
$$

- Chain rule yields
\begin{align*}
f_{\chi^2_1}(x) & = \frac{d}{dx} F_{\chi^2_1}(x) =
\frac{d}{dx} \left(  2 \frac{1}{\sqrt{2\pi}} G( \sqrt{x} )  \right)  \\
& = 2 \frac{1}{\sqrt{2\pi}} G'( \sqrt{x} ) \frac{x^{-1/2}}{2} 
= \frac{x^{-1/2} e^{-x/2}}{2^{1/2} \sqrt{\pi}}
\end{align*}


## Proof of Theorem -- Case $r =1$ {.smaller}

- It is well known that 
$$
\Gamma(1/2) = \sqrt{\pi}
$$
- Hence, we conclude
$$
f_{\chi^2_1}(x) = \frac{x^{-1/2} e^{-x/2}}{2^{1/2} \sqrt{\pi}} = \frac{x^{-1/2} e^{-x/2}}{2^{1/2} \Gamma(1/2)}
$$
- This shows
$$
\chi_1^2 \sim \Gamma(1/2,1/2)
$$




## Proof of Theorem -- Case $r \geq 2$ {.smaller}

- We need to prove that $\chi^2_r \sim \Gamma(r/2, 1/2)$

- By definition
$$
\chi^2_r \sim Z^2_1 + \ldots + Z^2_r \,, \qquad 
Z_i \sim N(0,1) \quad \text{iid}
$$

- By the Theorem in Slide 46, we have 
$$
Z_1,\ldots,Z_r \,\,\, \text{iid} \quad \implies \quad
Z_1^2,\ldots,Z_r^2 \,\,\, \text{iid}
$$

- Moreover, by definition, $Z_i^2 \sim \chi_1^2$

- Therefore, we have
$$
\chi^2_r = \sum_{i=1}^r X_i, \qquad X_i \sim \chi^2_1 \quad \text{iid}
$$


## Proof of Theorem -- Case $r \geq 2$ {.smaller}

- We have just proven that
$$
\chi_1^2 \sim \Gamma (1/2,1/2)
$$

- Moreover, the Theorem in Slide 53 guarantees that
$$
Y_i \sim \Gamma(\alpha_i, \beta) \quad \text{independent} \quad \implies \quad
Y_1 + \ldots + Y_n \sim \Gamma(\alpha,\beta)
$$ 
where $\alpha = \alpha_1 + \ldots + \alpha_n$


- Therefore, we conclude that
$$
\chi^2_r = \sum_{i=1}^r X_i, \qquad X_i \sim \Gamma(1/2,1/2) \quad \text{iid} \quad \implies \quad 
\chi^2_r \sim \Gamma(r/2,1/2)
$$



# Part 6: <br>Sampling from normal distribution {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Sampling from Normal distribution {.smaller}


**Sample mean and variance**: For a random sample $X_1,\ldots,X_n$ defined by
$$
S^2 := \frac{1}{n-1} \sum_{i=1}^n \left( X_i - \overline{X}  \right)^2 \,, \qquad 
\overline{X} := \frac{1}{n} \sum_{i=1}^n X_i
$$


::: Question

Assume the sample is normal
$$
X_i \sim N(\mu,\sigma^2) \,, \quad \forall \, i = 1 , \ldots, n
$$
What are the distributions of $\overline{X}$ and $S^2$?

:::




## Properties of Sample Mean and Variance {.smaller}

::: Theorem

Let $X_1,\ldots,X_n$ be a random sample from $N(\mu,\sigma^2)$. Then

- $\overline{X}$ and $S^2$ are independent random variables
- $\overline{X}$ and $S^2$ are distributed as follows
$$
\overline{X} \sim  N(\mu,\sigma^2/n) \,, \qquad
\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$


:::



## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem

- To prove independence of $\overline{X}$ and $S^2$ we make use of the following Lemma
- Proof of this Lemma is technical and omitted
- For a proof see Lemma 5.3.3 in [@casella-berger]

::: Lemma

Let $X$ and $Y$ be normal random variables. Then
$$
X \text{ and } Y \text{ independent } \quad \iff \quad 
\Cov(X,Y) = 0 
$$

:::




## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem


- Note that $X_i - \overline{X}$ and $\overline{X}$ are 
normally distributed, being sums of iid normals

- Therefore, we can apply the Lemma to $X_i - \overline X$ and $\overline{X}$

- To this end, recall that $\Var[\overline X] = \sigma^2/n$

- Also note that, by independence of $X_1,\ldots,X_n$
$$
\Cov(X_i,X_j) = 
\begin{cases}
\Var[X_i] & \text{ if } \, i = j \\
0         & \text{ if } \, i \neq j \\
\end{cases}
$$



## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem

- Using bilinearity of covariance (i.e. linearity in both arguments)
\begin{align*}
\Cov(X_i - \overline X, \overline X) & = \Cov(X_i,\overline{X}) - \Cov(\overline X,\overline{X}) \\
& = \frac{1}{n} \sum_{j=1}^n \Cov(X_i,X_j) - \Var[\overline X] \\
& = \frac{1}{n} \Var[X_i] - \Var[\overline X] \\
& = \frac{1}{n} \sigma^2 - \frac{\sigma^2}{n} = 0
\end{align*}

- By the Lemma, we infer independence of $X_i - \overline X$ and $\overline X$




## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem

- We have shown 
$$
X_i - \overline X \quad \text{and} \quad  \overline X \quad 
\text{independent}
$$

- By the Theorem in Slide 46, we hence have 
$$
(X_i - \overline X)^2 \quad \text{and} \quad  \overline X \quad 
\text{independent}
$$


- By the same Theorem we also get
$$
\sum_{i=1}^n (X_i - \overline X)^2 = (n-1)S^2 \quad \text{and} \quad  \overline X \quad 
\text{independent}
$$

- Again the same Theorem, finally implies independence of
$S^2$ and $\overline X$





## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem


- We now want to show that $\overline{X} \sim  N(\mu,\sigma^2/n)$

- We are assuming that $X_1,\ldots,X_n$ are iid with 
$$
\Expect[X_i] = \mu \,, \qquad \Var[X_i] = \sigma^2
$$

- We have already seen in Slides 70 and 72 that, in this case,
$$
\Expect[\overline X] = \mu \,, \quad \Var[\overline{X}] = \frac{\sigma^2}{n}
$$

- Sum of independent normals is normal (see the Theorem in slide 50)

- Therefore $\overline{X}$ is normal, with mean $\mu$ and variance $\sigma^2/n$




## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem

- We are left to prove that 
$$
\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$
    * This is somewhat technical and we don't actually prove it 
    * For a proof see Theorem 5.3.1 in [@casella-berger]
    * We however want to provide some intuition on why it holds

- Recall that the chi-squared distribution with $r$ degrees of freedom is
$$
\chi_r^2 \sim Z_1^2 + \ldots + Z_r^2
$$
with $Z_i$ iid and $N(0,1)$




## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem

- By definition of $S^2$ we have
$$
\frac{(n-1)S^2}{\sigma^2}  = 
\sum_{i=1}^n \frac{(X_i - \overline X)^2}{\sigma^2}
$$

- If we replace the sample mean $\overline X$ with the actual mean $\mu$ we get the approximation
$$
\frac{(n-1)S^2}{\sigma^2}  = 
\sum_{i=1}^n \frac{(X_i - \overline X)^2}{\sigma^2} \approx
\sum_{i=1}^n \frac{(X_i - \mu)^2}{\sigma^2}
$$



## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem

- Since $X_i \sim N(\mu,\sigma^2)$, we have that
$$
Z_i := \frac{X_i - \mu}{\sigma} \sim N(0,1)
$$

- Therefore 
$$
\frac{(n-1)S^2}{\sigma^2}   \approx \sum_{i=1}^n \frac{(X_i - \mu)^2}{\sigma^2} = \sum_{i=1}^n Z_i^2 \sim \chi_n^2
$$

- The above is just an approximation:  
When replacing $\mu$ with $\overline X$, we **lose 1 degree of freedom**
$$
\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$






# Part 7: <br>t-distribution{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Estimating the Mean  {.smaller}

::: Problem

Estimate the mean $\mu$ of a **normal  population**

:::

**What to do?**

- We can collect normal samples $X_1, \ldots, X_n$ with
$X_i \sim N(\mu,\sigma^2)$

- We then compute the sample mean
$$
\overline X := \frac{1}{n} \sum_{i=1}^n X_i
$$
- We know that $\Expect[\overline X] = \mu$



## $\overline X$ approximates $\mu$  {.smaller}



::: Question

How good is this approximation? How to quantify it?

:::


**Answer**: We consider the **Test Statistic**
$$
T := \frac{\overline{X}-\mu}{\sigma/\sqrt{n}} \, \sim \,N(0,1)
$$

- This is because $\overline X \sim N(\mu,\sigma^2/n)$ -- see Slide 101


- If $\sigma$ is known, then the only unknown in $T$ is $\mu$ 

**$T$ can be used to estimate $\mu$ $\quad \implies \quad$ Hypothesis Testing**




## Hypothesis testing  {.smaller}

- Suppose that $\mu=\mu_0$ (this is called the **null hypothesis**)


- Using the data collected $\xx = (x_1,\ldots,x_n)$, we compute
$$
t := \frac{\overline{x}-\mu_0}{\sigma/\sqrt{n}} \,, \qquad 
\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i
$$

- When $\mu = \mu_0$, the number $t$ is a realization of the test statistic (random variable)
$$
T = \frac{\overline{X}-\mu_0}{\sigma/\sqrt{n}} \, \sim \,N(0,1)
$$

- Therefore, we can compute the probability of $T$ being close to $t$
$$
p := P(T \approx t)
$$





## Hypothesis testing  {.smaller}


Given the value $p := P(T \approx t)$ we have 2 cases:

- $p$ is small $\quad \implies \quad$  **reject the null hypothesis** $\mu = \mu_0$
    * $p$ small means it is unlikely to observe such value of $t$
    * Recall that $t$ depends only on the data $\xx$, and on our guess $\mu_0$
    * We conclude that our guess must be wrong $\quad \implies \quad \mu \neq \mu_0$

- $p$ is large $\quad \implies \quad$  **do not reject the null hypothesis** $\mu = \mu_0$
    * $p$ large means that $t$ occurs with reasonably high probability
    * There is no reason to believe our guess $\mu_0$ was wrong
    * But we also do not have sufficient reason to believe $\mu_0$ was correct





## Important Remark  {.smaller}


- The key step in **Hypothesis Testing** is computing
$$
p = P(T \approx t)
$$

- This is only possible if we know the distribution of 
$$
T = \frac{\overline{X}-\mu}{\sigma/\sqrt{n}}
$$

- If we assume that the variance $\sigma^2$ is known, then 
$$
T \sim N(0,1)
$$ 
and $p$ is easily computed




## Unknown variance  {.smaller}

::: Problem

In general, the population variance $\sigma^2$ is unknown. What to do?

:::


**Idea**: We can replace $\sigma^2$ with the **sample variance**
$$
S^2 = \frac{\sum_{i=1}^n X_i^2 - n \overline{X}^2}{n-1}
$$
The new test statistic is hence
$$
T := \frac{\overline{X}-\mu}{S/\sqrt{n}}
$$




## Distribution of the test statistic  {.smaller}

::: Question

What is the distribution of

$$
T := \frac{\overline{X}-\mu}{S/\sqrt{n}} \qquad ?
$$


:::


**Answer**: $T$ has t-distribution with $n-1$ degrees of freedom

- This is also known as **Student's t-distribution**
- **Student** was the pen name under which **W.S. Gosset** was publishing his research
- He was **head brewer** at Guinness, at the time the largest brewery in the world!
- Used t-distribution to study chemical properties of barley from **low samples** [@student] (see original [paper](https://www.york.ac.uk/depts/maths/histstat/student.pdf)
)





## t-distribution  {.smaller}

::: Definition

A random variable $T$ has **Student's t-distribution** with
**p degrees of freedom**, denoted by 
$$
T \sim t_p \,,
$$
if the pdf of $T$ is
$$
f_T(t) = \frac{\Gamma \left( \frac{p+1}{2} \right) }{\Gamma \left( \frac{p}{2} \right)} \, \frac{1}{(p\pi)^{1/2}} \,
\frac{ 1  }{ (1 + t^2/p)^{(p+1)/2} } \,, \qquad 
t \in \R 
$$

:::




## Characterization of the t-distribution  {.smaller}

::: Theorem

Let $U \sim N(0,1)$ and $V \sim \chi_p^2$ be independent random variables. Then
$$
T := \frac{U}{\sqrt{V/p}}  \, \sim  \, t_p \,,
$$
that is, $T$ has t-distribution with $p$ degrees of freedom.

:::


**Proof**: Given as exercise in Homework assignments




## Distribution of t-statistic  {.smaller}

As a consequence of the Theorem in previous slide we obtain:

::: Theorem

Let $X_1,\ldots,X_n$ be a random sample from $N(\mu,\sigma^2)$. Then the random variable 
$$
T = \frac{\overline{X}-\mu}{S/\sqrt{n}}
$$
has **t-distribution** with $n-1$ degrees of freedom, that is,
$$
T \sim t_{n-1}
$$
:::


## Distribution of t-statistic  {.smaller}
### Proof of Theorem

- Since $X_1,\ldots,X_n$ is random sample from $N(\mu,\sigma^2)$, we have that (see Slide 101)
$$
\overline{X} \sim N(\mu, \sigma^2/n)
$$

- Therefore, we can renormalize and obtain
$$
U := \frac{ \overline{X} - \mu }{ \sigma/\sqrt{n} } \sim N(0,1)
$$



## Distribution of t-statistic  {.smaller}
### Proof of Theorem

- We have also shown that
$$
V := \frac{ (n-1) S^2 }{ \sigma^2 } \sim \chi_{n-1}^2
$$

- Finally, we can rewrite $T$ as
$$
T = \frac{\overline{X}-\mu}{S/\sqrt{n}} = \frac{U}{ \sqrt{V/(n-1)} }
$$

- By the Theorem in Slide 118, we conclude that $T \sim t_{n-1}$





## Properties of t-distribution {.smaller}

::: Proposition
### Expectation and Variance of t-distribution

Suppose that $T \sim t_p$. We have: 

- If $p>1$ then $\Expect[T] = 0$
- If $p>2$ then $\Var[T] = \frac{p}{p-2}$

:::

**Notes:**

- We have to assume $p>1$, otherwise $\Expect[T] = \infty$ for $p=1$
- We have to assume $p>2$, otherwise $\Var[T] = \infty$ for $p=1,2$
- $\Expect[T] = 0$ follows trivially from symmetry of the pdf $f_T(t)$ around $t=0$
- Computing $\Var[T]$ is quite involved, and we skip it





## t-distribution {.smaller}
### Comparison with Standard Normal

The $t_p$ distribution **approximates** the standard normal $N(0,1)$:

- $t_p$ it is symmetric around zero and bell-shaped, like $N(0,1)$
- $t_p$ has **heavier tails** compared to $N(0,1)$
- While the variance of $N(0,1)$ is $1$, the variance of $t_p$ is $\frac{p}{p-2}$
- We have that 
$$
t_p \to N(0,1) \quad \text{as} \quad p \to \infty
$$





## Plot: Comparison with Standard Normal {.smaller}


```{r}

# Set the degrees of freedom for t-distribution
degree1 <- 3

# Generate values for the x-axis
x_values <- seq(-4, 4, by = 0.1)  # Adjusted for standard normal distribution

# Calculate the probability density function (PDF) values for each x for the 3 distributions
pdf_values1 <- dnorm(x_values)
pdf_values2 <- dt(x_values, df = degree1)

# Plot the standard normal and t-distributions
plot(x_values, 
     pdf_values1, 
     type = "l", 
     col = "blue", 
     lwd = 2,
     xlab = "", 
     ylab = "",
     ylim = c(0, max(pdf_values2, pdf_values3) + 0.1))

lines(x_values, 
      pdf_values2, 
      col = "red", 
      lwd = 2)


mtext("x", side = 1, line = 3, cex = 2)
mtext("pdf", side = 2, line = 2.5, cex = 2)

# Add a legend
legend("topright", 
       legend = c("Standard Normal N(0,1)", 
                  paste("t with", degree1, "deg of freedom")),
       col = c("blue", "red"), 
       lty = 1, 
       lwd = 2, 
       cex = 1.5)

```






## References


