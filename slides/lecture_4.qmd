---
title: "Statistical Models"
subtitle: "Lecture 4"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Lecture 4: <br>The variance ratio and <br> two-sample t-tests {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Outline of Lecture 4


1. One-sample variance ratio test
2. Worked example
3. One-sample variance ratio test in R
4. Two-sample hypothesis tests
5. Two-sample t-test
6. Two-sample t-test: Example






# Part 1: <br>One-sample variance <br> ratio test{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Task: Estimating mean and variance {.smaller}

- Assume the population has normal distribution $N(\mu,\sigma^2)$
    * Mean $\mu$ and variance $\sigma^2$ are **unknown**


- **Questions** about $\mu$ and $\sigma^2$
    1. What is my best guess of the value?
    2. How far away from the true value am I likely to be?


- **Answers:**
    * The one-sample **t-test** answers questions about $\mu$ (seen in Lecture 3)
    * The one-sample **variance ratio test** answers questions about $\sigma^2$





## Reminder {.smaller}

- The-one sample variance test uses **chi-squared distribution**

- **Recall:** Chi-squared distribution with $p$ degrees of freedom is
$$
\chi_p^2 = Z_1^2 + \ldots + Z_p^2
$$
where $Z_1, \ldots, Z_p$ are iid $N(0, 1)$





## One-sample one-sided variance ratio test {.smaller}

**Assumption:** Suppose given sample $X_1,\ldots, X_n$ iid from $N(\mu,\sigma^2)$


**Goal:** Estimate variance $\sigma^2$ of population


**Test:** 

- Suppose $\sigma_0$ is guess for $\sigma$ 

- The one-sided hypothesis test for $\sigma$ is
$$
H_0 \colon \sigma = \sigma_0 \qquad H_1 \colon \sigma > \sigma_0
$$



## What to do? {.smaller}

- Consider the sample variance
$$
S^2 = \frac{ \sum_{i=1}^n  X_i^2 - n \overline{X}^2 }{n-1}
$$

- Since we believe $H_0$, the variance is 
$$
\sigma = \sigma_0
$$

- $S^2$ cannot be too far from the true variance $\sigma$

- Therefore we **cannot** have that 
$$
S^2 \gg \sigma^2 = \sigma_0^2 
$$



## What to do? {.smaller}

- If we observe $S^2 \gg \sigma_0^2$ then our guess $\sigma_0$ is probably wrong

- Therefore we **reject** $H_0$ if
$$
S^2 \gg \sigma_0^2
$$


- The **rejection** condition $S^2 \gg \sigma_0^2$ is equivalent to 
$$
\frac{(n-1)S^2}{\sigma_0^2}  \gg 1
$$
where $n$ is the sample size



## What to do? {.smaller}

- We define our **test statistic** as
$$
\chi^2 := \frac{(n-1)S^2}{\sigma_0^2}
$$


- The **rejection** condition is hence
$$
\chi^2 \gg 1
$$



## What to do? {.smaller}

- In Lecture 2, we have proven that
$$
\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$


- Assuming $\sigma=\sigma_0$, we therefore have
$$
\chi^2 = \frac{(n-1)S^2}{\sigma_0^2} = \frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$



## Summary: Rejection condition {.smaller}

- We **reject** $H_0$ if 
$$
\chi^2 = \frac{(n-1)S^2}{\sigma_0^2} \gg 1 
$$

- This means we do not want $\chi^2$ to be too extreme to the right

- As $\chi^2 \sim \chi_{n-1}^2$, we decide to rejct $H_0$ if
$$
\chi^2 > \chi_{n-1}^2(0.05)   
$$


- By definition, the **critical value** $\chi_{n-1}^2(0.05)$ is such that
$$
P(\chi_{n-1}^2 > \chi_{n-1}^2(0.05) ) = 0.05
$$




## Critical values of chi-squared {.smaller}

- $x^* := \chi_{n-1}^2(0.05)$ is point on $x$-axis such that $P(\chi_{n-1}^2 > x^* ) = 0.05$

- Therefore, the semi-open interval $(x^*,+\infty)$ is the *rejection region*

- In the picture we have $n = 12$ and $\chi_{11}^2(0.05) = 19.68$

```{r}
# Degrees of freedom
df <- 11

# Values for x-axis
x <- seq(0, 30, length.out = 1000)  # Adjust the range according to chi-squared distribution

# Calculate PDF of chi-squared distribution
pdf <- dchisq(x, df)

# Plot PDF
plot(x, pdf, type = "l", col = "blue", lwd = 2, xlab = "x", ylab = "Density")


# Shade area where p-value > 0.95
x_fill_right <- x[x >= qchisq(0.95, df)]
y_fill_right <- pdf[x >= qchisq(0.95, df)]
polygon(c(x_fill_right, rev(x_fill_right)), c(y_fill_right, rep(0, length(y_fill_right))), col = "gray", border = NA)

# Add critical value for 0.95 quantile
x_star <- qchisq(0.95, df)
points(x_star, dchisq(x_star, df), col = "red", pch = 19, cex = 1.3)

# Add text above x_star containing its value
text(x_star * 1.05, dchisq(x_star, df) * 1.35, paste("x* =", round(x_star, 2)), pos = 3, col = "red", cex = 1.3)

# Add legend
legend("topright", legend = c("area of rejection region = 0.05"), fill = "gray", cex = 1.3)

```


## Critical values of chi-squared -- Tables {.smaller}

- Find Table 13.5 in this [file](files/Statistics_Tables.pdf)
- Look at the row with Degree of Freedom $n-1$ (or its closest value)
- Find **critical value** $\chi^2_{n-1}(0.05)$ in column $\alpha = 0.05$
- **Example**: $n=12$, DF $=11$, $\chi^2_{11}(0.05) = 19.68$

![](images/chi_squared_test_statistic_table.png){width=82%}




## The p-value {.smaller}

- Given the test statistic $\chi^2$, the **p-value** is defined as
$$
p := P( \chi_{n-1}^2 > \chi^2 )
$$


- Notice that 
$$
p < 0.05 \qquad \iff \qquad  \chi^2 > \chi_{n-1}^2(0.05)
$$

> This is because $\chi^2 > \chi_{n-1}^2(0.05)$ iff
> $$
> p = P(\chi_{n-1}^2 > \chi^2)
>  < P(\chi_{n-1}^2 > \chi_{n-1}^2(0.05) )
>  = 0.05
> $$




## One-sample one-sided variance ratio test {.smaller}
### The procedure

Suppose given

- Sample $x_1, \ldots, x_n$ of size $n$ from $N(\mu,\sigma^2)$
- Guess $\sigma_0$ for $\sigma$

The one-sided hypothesis test is
$$
H_0 \colon \sigma = \sigma_0 \qquad H_1 \colon \sigma > \sigma_0
$$
The **variance ratio test** consists of 3 steps



## One-sample one-sided variance ratio test {.smaller}
### The procedure

1. **Calculation**: Compute the chi-squared statistic
$$
\chi^2 = \frac{(n-1) s^2}{\sigma_0^2}
$$
where sample mean and variance are
$$
\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i \,, \qquad 
s^2 = \frac{\sum_{i=1}^n x_i^2 - n \overline{x}^2}{n-1}
$$



## One-sample one-sided variance ratio test {.smaller}
### The procedure

2. **Statistical Tables or R**: Find either
    * Critical value in [Table 13.5](files/Statistics_Tables.pdf)
    $$
    \chi_{n-1}^2(0.05)
    $$
    * p-value in R
    $$
    p := P( \chi_{n-1}^2 > \chi^2 )
    $$
    (more on this later)




## One-sample one-sided variance ratio test {.smaller}
### The procedure

3. **Interpretation**:
    * Reject $H_0$ if 
    $$
    \chi^2 > \chi_{n-1}^2(0.05)  \qquad \text{ or } \qquad p < 0.05
    $$
    * Do not reject $H_0$ if 
    $$
    \chi^2 \leq \chi_{n-1}^2(0.05) \qquad \text{ or } \qquad 
    p \geq 0.05
    $$






# Part 2: <br>Worked example {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## One-sample variance ratio test: Example {.smaller}


| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Cons. Expectation        |66 | 53| 62| 61| 78| 72| 65| 64| 61| 50| 55| 51|
| Cons. Spending           |72 | 55| 69| 65| 82| 77| 72| 78| 77| 75| 77| 77|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|


<br>

- **Data:** *Consumer Expectation* (CE) and *Consumer Spending* (CS) in 2011 (again!)
- **Assumption:** CE and CS are normally distributed





## One-sample variance ratio test: Example {.smaller}


| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Cons. Expectation        |66 | 53| 62| 61| 78| 72| 65| 64| 61| 50| 55| 51|
| Cons. Spending           |72 | 55| 69| 65| 82| 77| 72| 78| 77| 75| 77| 77|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|

<br>

- **Remark**: Monthly data on CE and CS can be matched 
    * Hence consider: $\quad$ Difference $=$ CE $-$ CS 
    * CE and CS normal $\quad \implies \quad$ Difference $\sim N(\mu,\sigma^2)$

- **Question:** Test the following hypothesis:
$$
H_0 \colon \sigma = 1 \qquad 
H_1 \colon \sigma > 1
$$




## Motivation of test {.smaller}

- If $X \sim N(\mu,\sigma^2)$ then
$$
P( \mu - 2 \sigma \leq X \leq \mu + 2\sigma ) \approx 0.95 
$$

- Recall: $\quad$ Difference $=$ (CE $-$ CS) $\sim N(\mu,\sigma^2)$

- Hence if $\sigma = 1$
$$
P( \mu - 2 \leq {\rm CE} - {\rm CS} \leq \mu + 2 ) \approx 0.95 
$$

- **Meaning of variance ratio test:** 
$$
\sigma=1
\quad \implies \quad 
\text{CS index is within } \pm{2} \text{ of CE index with probability } 0.95  
$$





## The variance ratio test by hand {.smaller}


| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|

<br>

1. **Calculations**: Using the above data, compute 

- Sample mean:
$$
\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n} =  \frac{-6-2-7-4- \ldots -22-26}{12} = -\frac{138}{12} = -11.5
$$




## The variance ratio test by hand {.smaller}

| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|

<br>

1. **Calculations**: Using the above data, compute 

- Sample variance:
\begin{align*}
\sum_{i=1}^{n} x_i^2 & = (-6)^2 + (-2)^2 + (-7)^2 + \ldots + (-22)^2 + (-26)^2 = 2432 \\
s^2 & = \frac{\sum_{i=1}^n x^2_i- n \bar{x}^2}{n-1}
      = \frac{2432-12(-11.5)^2}{11} = \frac{845}{11} = 76.8182
\end{align*}





## The variance ratio test by hand {.smaller}

| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|

<br>

1. **Calculations**: Using the above data, compute 

- Chi-squared statistic:
$$
\chi^2 = \frac{(n-1)s^2}{\sigma_0^2} 
       = \frac{11 \left(\frac{845}{11}\right) }{1} 
       = 845
$$





## The variance ratio test by hand {.smaller}

2. **Statistical Tables**:
    * Sample size is $n = 12$
    * In [Table 13.5](files/Statistics_Tables.pdf) find
    $$
    \chi_{11}^2(0.05) = 19.68
    $$

3. **Interpretation**:
    * Test statistic is $\chi^2 = 845$
    * We **reject** $H_0$ because
    $$
    \chi^2 = 845 > 19.68 = \chi_{n-1}^2(0.05)
    $$



## The variance ratio test by hand {.smaller}

4. **Conclusion**:
    * We accept $H_1$: The standard deviation satisfies $\sigma > 1$
    * A better estimate for $\sigma$ could be sample standard deviation
    $$
    s=\sqrt{\frac{845}{11}}=8.765
    $$
    * This suggests: With probability $0.95$
    $$
    \text{CS index is within } \pm{2 \times 8.765 = \pm 17.53 } \text{ of CE index}  
    $$




# Part 3: <br>One-sample variance <br> ratio test in R{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## The variance ratio test in R {.smaller}

**Goal**: Perform chi-squared variance ratio test in R

- For this, we need to compute p-value
$$
p = P(\chi_{n-1}^2 > \chi^2)
$$

- Thus, we need to compute probabilities for chi-squared distribution in R





## Probability Distributions in R {.smaller}

- R can natively do calculations with known probability distrubutions
- Example: Let $X$ be r.v. with $N(\mu,\sigma^2)$ distribution

| **R command**         |   **Computes**                                |
|:--------------------  |:------------------------------               |
| ``pnorm(x, mean = mu, sd = sig)`` |   $P(X \leq x)$                              |
| ``qnorm(p, mean = mu, sd = sig)`` |   $q$ such that $P(X \leq q) = p$                          |
| ``dnorm(x, mean = mu, sd = sig)`` |   $f(x)$, where $f$ is pdf of $X$    |
| ``rnorm(n, mean = mu, sd = sig)`` |   $n$ random samples from distr. of $X$|
: {tbl-colwidths="[50,50]"}


**Note**: Syntax of commands

**norm** $=$ normal $\qquad$ **p** $=$ probability $\qquad$ **q** $=$ quantile 

**d** $=$ density $\qquad \quad \,\,\,\,$  **r** $=$ random
  


## Example {.smaller}

- Suppose average height of women is normally distributed $N(\mu,\sigma^2)$
- Assume mean $\mu = 163$ cm and standard deviation $\sigma = 8$ cm

<br>

```{r}
#| echo: true

# Probability woman exceeds 180cm in height
# P(X > 180) = 1 - P(X <= 180)

1 - pnorm(180, mean = 163, sd = 8)
```


## {.smaller}

```{r}
#| echo: true

# The upper 10th percentile for women height, that is,
# height q such that P(X <= q) = 0.9

qnorm(0.90, mean = 163, sd = 8)
```

<br>



```{r}
#| echo: true

# Value of pdf at 163

dnorm(163, mean = 163, sd = 8)
```


<br>

```{r}
#| echo: true

# Generate random sample of size 5

rnorm(5, mean = 163, sd = 8)
```


## {.smaller}

**Question:** What is the height of the tallest woman, according to the model?


- The tallest woman could be found using quantiles
- There are roughly 3.5 billion women
- The tallest would be in the top 1/(3.5 billion) quantile

```{r}
#| echo: true

# Find the top 1/(3.5 billion) quantile

p = 1 - 1 / (3.5e9)
qnorm(p, mean = 163, sd = 8)
```


- The current (living) tallest woman is Rumeysa Gelci at $215$ cm ([Wikipedia Page](https://en.m.wikipedia.org/wiki/Rumeysa_Gelgi))




## Probability Distributions in R {.smaller}
### Chi-squared distribution

- Commands for chi-squared distrubution are similar
- ``df = n`` denotes $n$ degrees of feedom

| **R command**         |   **Computes**                                |
|:--------------------  |:------------------------------               |
| ``pchisq(x, df = n)`` |   $P(X \leq x)$                              |
| ``qchisq(p, df = n)`` |   $q$ such that $P(X \leq q) = p$                          |
| ``dchisq(x, df = n)``  |   $f(x)$, where $f$ is pdf of $X$    |
| ``rchisq(m, df = n)``  |   $m$ random samples from distr. of $X$|
: {tbl-colwidths="[40,60]"}



## Example 1 {.smaller}

- From Tables we found quantile $\chi_{11}^2 (0.05) = 19.68$
- **Question:** Compute such quantile in R





## Example 1 -- Solution {.smaller}

- From Tables we found quantile $\chi_{11}^2 (0.05) = 19.68$
- **Question:** Compute such quantile in R


<br>

```{r}
#| echo: true
# Compute 0.95 quantile for chi-squared with 11 degrees of freedom

quantile <- qchisq(0.95, df = 11)

cat("The 0.95 quantile for chi-squared with df = 11 is", quantile)
```


## Example 2 {.smaller}

- The $\chi^2$ statistic for variance ratio test has distribution $\chi_{n-1}^2$

- **Question:** Compute the p-value
$$
p := P(\chi_{n-1}^2 > \chi^2)
$$



## Example 2 -- Solution {.smaller}

- The $\chi^2$ statistic for variance ratio test has distribution $\chi_{n-1}^2$

- **Question:** Compute the p-value
$$
p := P(\chi_{n-1}^2 > \chi^2)
$$


- Observe that
$$
p := P(\chi_{n-1}^2 > \chi^2) = 1 - P(\chi_{n-1}^2  \leq  \chi^2)
$$

- The code is therefore

```r
# Compute p-value for chi^2 = chi_squared and df = n

p_value <- 1 - pchisq(chi_squared, df = n)
```




## The variance ratio test in R {.smaller}


| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Cons. Expectation        |66 | 53| 62| 61| 78| 72| 65| 64| 61| 50| 55| 51|
| Cons. Spending           |72 | 55| 69| 65| 82| 77| 72| 78| 77| 75| 77| 77|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|

<br>

- **Back to the Worked Example:** Monthly data on CE and CS

- **Question:** Test the following hypothesis:
$$
H_0 \colon \sigma = 1 \qquad 
H_1 \colon \sigma > 1
$$




## The variance ratio test in R {.smaller}

| Month                    | J | F | M | A | M | J | J | A | S | O | N | D |
|:------------------------ |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Cons. Expectation        |66 | 53| 62| 61| 78| 72| 65| 64| 61| 50| 55| 51|
| Cons. Spending           |72 | 55| 69| 65| 82| 77| 72| 78| 77| 75| 77| 77|
| Difference               |-6 | -2| -7| -4| -4| -5| -7|-14|-16|-25|-22|-26|

<br>

- Start by entering data into R

```r
# Enter Consumer Expectation and Consumer Spending data
CE <- c(66, 53, 62, 61, 78, 72, 65, 64, 61, 50, 55, 51)
CS <- c(72, 55, 69, 65, 82, 77, 72, 78, 77, 75, 77, 77)

# Compute difference
difference <- CE - CS
``` 



## The variance ratio test in R {.smaller}

- Compute chi-squared statistic
$$
\chi^2 = \frac{(n-1) s^2}{\sigma^2_0}
$$


```r
# Compute sample size
n <- length(difference)

# Enter null hypothesis
sigma_0 <- 1

# Compute sample variance
s <- var(difference)

# Compute chi-squared statistic
chi_squared <- (n - 1) * s / sigma_0 ^ 2
```



## The variance ratio test in R {.smaller}

- Compute the p-value, and print to screen
$$
p = P(\chi_{n-1}^2 > \chi^2) = 1 - P(\chi_{n-1}^2 \leq \chi^2)
$$


```r
# Compute p-value
p_value <- 1 - pchisq(chi_squared, df = n - 1)

# Print p-value
cat("The p-value for one-sided variance test is", p_value)
```

<br>

- The full code can be downloaded here [variance_ratio_test.R](codes/variance_ratio_test.R)



## Running the code {.smaller}

- Running [variance_ratio_test.R](codes/variance_ratio_test.R) gives the following output:
```{r}
# Enter Consumer Expectation and Consumer Spending data
CE <- c(66, 53, 62, 61, 78, 72, 65, 64, 61, 50, 55, 51)
CS <- c(72, 55, 69, 65, 82, 77, 72, 78, 77, 75, 77, 77)

# Compute difference
difference <- CE - CS

# Compute sample size
n <- length(difference)

# Enter null hypothesis
sigma_0 <- 1

# Compute sample variance
s <- var(difference)

# Compute chi-squared statistic
chi_squared <- (n - 1) * s / sigma_0 ^ 2

# Compute p-value
p_value <- 1 - pchisq(chi_squared, df = n - 1)

# Print p-value
cat("The p-value for one-sided variance test is", p_value)
```

<br>

- Since $p = 0 < 0.05$ we **reject** $H_0$
- Therefore the true variance seems to be $\sigma^2 > 1$





# Part 4: <br>Two-sample<br> hypothesis tests {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Overview {.smaller}

In this Lecture:

- We looked at data on CCI before and after the 2008 crash
- In this case data for each month is directly comparable
- Can then construct the difference between the 2007 and 2009 values
- Analysis reduces from a two-sample to a one-sample problem

::: Question

How do we analyze two samples that cannot be paired?

:::





## Problem statement {.smaller}

**Goal:** compare mean and variance of 2 independent **normal** samples

- First sample:
    * $X_1, \ldots, X_n$ from normal population $N(\mu_X,\sigma_X^2)$

- Second sample:
    * $Y_1, \ldots, Y_m$ from normal population $N(\mu_Y,\sigma_Y^2)$

- We may have $n \neq m$ 
    * Samples cannot be paired due to different size!

**Tests available:**

- Two-sample $t$-test to test for difference in means
- Two-sample $F$-test to test for difference in variances (next week)




## Why is this important? {.smaller}

- Hypothesis testing starts to get interesting with 2 or more samples

- t-test and F-test show the normal distribution family in action

- This is also the maths behind regression
    * Same methods apply to seemingly unrelated problems
    * Regression is a big subject in statistics




## Normal distribution family in action {.smaller}
### Two-sample t-test

- Want to compare the means of two independent samples
- At the same time population variances are unknown
- Therefore both variances are estimated with sample variances
- Test statistic is $t_k$-distributed with $k$ linked to the total number of observations




## Normal distribution family in action {.smaller}
### Two-sample F-test

- Want to compare the variance of two independent samples
- This can be done by studying the ratio of the sample variances
$$
S^2_X/S^2_Y
$$

- We have already shown that
$$
\frac{(n - 1) S^2_X}{\sigma^2_X} \sim \chi^2_{n - 1} \qquad 
\frac{(m - 1) S^2_Y}{\sigma^2_Y} \sim \chi^2_{m - 1}
$$



## Normal distribution family in action {.smaller}
### Two-sample F-test


- Hence we can study statistic
$$
F = \frac{S^2_X / \sigma_X^2}{S^2_Y / \sigma_Y^2}
$$

- We will see that $F$ has F-distribution (next week)






# Part 5: <br>Two-sample t-test {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## The two-sample t-test {.smaller}

**Assumptions**: Suppose given samples from 2 normal populations

- $X_1, \ldots ,X_n$ iid with distribution $N(\mu_X,\sigma_X^2)$
- $Y_1, \ldots ,Y_m$ iid with distribution $N(\mu_Y,\sigma_Y^2)$


**Further assumptions**:

- In general $n \neq m$, so that one-sample t-test cannot be applied
- The two populations have same variance 
$$
\sigma^2_X = \sigma^2_Y = \sigma^2
$$


**Note:** Assuming same variance is simplification. Removing it leads to *Welch t-test*





## The two-sample t-test {.smaller}


**Goal**: Compare means $\mu_X$ and $\mu_Y$

**Hypothesis set:** We test for a difference in means
$$
H_0 \colon \mu_X = \mu_Y \qquad  H_1 \colon \mu_X \neq \mu_Y
$$


**t-statistic**: The general form is
$$
T = \frac{\text{Estimate}-\text{Hypothesised value}}{\text{e.s.e.}}
$$



## The two-sample t-statistic {.smaller}

- Define the sample means
$$
\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i
\qquad \qquad
\overline{Y} = \frac{1}{m} \sum_{i=1}^m Y_i
$$

- Notice that
$$
\Expect[ \overline{X} ] = \mu_X   \qquad \qquad
\Expect[ \overline{Y} ] = \mu_Y
$$

- Therefore we can estimate $\mu_X - \mu_Y$ with the sample means, that is,
$$
\text{Estimate} = \overline{X} - \overline{Y}
$$




## The two-sample t-statistic {.smaller}

- Since we are testing for difference in mean, we have
$$
\text{Hypothesised value} = \mu_X - \mu_Y
$$

- The *Estimated Standard Error* is the standard deviation of estimator
$$
\text{e.s.e.} = \text{Standard Deviation of } \overline{X} -\overline{Y} 
$$


## The two-sample t-statistic {.smaller}

- Therefore the two-sample t-statistic is
$$
T = \frac{\overline{X} - \overline{Y} - (\mu_X - \mu_Y)}{\text{e.s.e.}}
$$

- Under the Null Hypothesis that $\mu_X = \mu_Y$, the t-statistic becomes
$$
T = \frac{\overline{X} - \overline{Y} }{\text{e.s.e.}}
$$




## A note on the degrees of freedom (df) {.smaller}

- The general rule is
$$
\text{df} = \text{Sample size} - \text{No. of estimated parameters}
$$

- Sample size in two-sample t-test:
    * $n$ in the first sample
    * $m$ in the second sample 
    * Hence total number of observations is $n + m$

- No. of estimated parameters is 2: Namely $\mu_X$ and $\mu_Y$

- Hence degree of freedoms in two-sample t-test is
$$
{\rm df} = n + m - 2
$$
(more on this later)





## The estimated standard error {.smaller}

- Recall: We are assuming populations have same variance
$$
\sigma^2_X = \sigma^2_Y = \sigma^2
$$

- We need to compute the estimated standard error
$$
\text{e.s.e.} = \text{Standard Deviation of } \ \overline{X} -\overline{Y} 
$$

- Variance of sample mean was computed in the Lemma in Slide 72 Lecture 2 

-  Since $\overline{X} \sim N(\mu_X,\sigma^2)$ and $\overline{Y} \sim N(\mu_Y,\sigma^2)$, by the Lemma we get
$$
\Var[\overline{X}] = \frac{\sigma^2}{n} \,, \qquad \quad
\Var[\overline{Y}] = \frac{\sigma^2}{m}
$$



## The estimated standard error {.smaller}

- Since $X_i$ and $Y_i$ are independent we get $\Cov(X_i,Y_j)=0$

- By bilinearity of covariance we infer
$$
\Cov ( \overline{X} , \overline{Y} ) = \frac{1}{n \cdot m} \sum_{i=1}^n \sum_{j=1}^m \Cov (X_i,Y_j) = 0
$$

- We can then compute
\begin{align*}
\Var[ \overline{X} - \overline{Y} ] & = \Var[ \overline{X} ] + \Var [ \overline{Y} ] -
                                        2 \Cov( \overline{X} , \overline{Y} ) \\
                                    & = \Var[ \overline{X} ] + \Var [ \overline{Y} ] \\
                                    & = \sigma^2 \left( \frac{1}{n} + \frac{1}{m}  \right)
\end{align*}





## The estimated standard error {.smaller}

- Taking the square root gives
$$
\text{S.D.}(\overline{X} - \overline{Y} )= \sigma \ \sqrt{\frac{1}{n}+\frac{1}{m}}
$$

- Therefore, the t-statistic is
$$
T = \frac{\overline{X} - \overline{Y} - (\mu_X - \mu_Y)}{\text{e.s.e.}} 
  = \frac{\overline{X} - \overline{Y} - (\mu_X - \mu_Y)}{\sigma \ \sqrt{\dfrac{1}{n}+\dfrac{1}{m}}}
$$







## Estimating the variance {.smaller}

The t-statistic is currently
$$
T = \frac{\overline{X} - \overline{Y} - (\mu_X - \mu_Y)}{\sigma \ \sqrt{\dfrac{1}{n}+\dfrac{1}{m}}}
$$

- Variance $\sigma^2$ is unknown: we need to **estimate it**!

- Define the sample variances

$$
S_X^2 = \frac{ \sum_{i=1}^n X_i^2 - n \overline{X}^2 }{n-1} \qquad \qquad
S_Y^2 = \frac{ \sum_{i=1}^m Y_i^2 - m \overline{Y}^2 }{m-1}
$$



## Estimating the variance {.smaller}

- Recall that 
$$
X_1, \ldots , X_n \sim N(\mu_X, \sigma^2) \qquad \qquad
Y_1, \ldots , Y_m \sim N(\mu_Y, \sigma^2)
$$

- From Lecture 2, we know that $S_X^2$ and $S_Y^2$ are unbiased estimators of $\sigma^2$, i.e.
$$
\Expect[ S_X^2 ] = \Expect[ S_Y^2 ] = \sigma^2
$$

- Therefore, both $S_X^2$ and $S_Y^2$ can be used to estimate $\sigma^2$



## Estimating the variance {.smaller}

- We can improve the estimate of $\sigma^2$ by combining  $S_X^2$ and $S_Y^2$

- We will consider a (convex) linear combination
$$
S^2 := \lambda_X S_X^2 + \lambda_Y S_Y^2  \,, \qquad  
\lambda_X + \lambda_Y = 1
$$


- $S^2$ is still an unbiased estimator of $\sigma^2$, since
\begin{align*}
\Expect[S^2]  & = \Expect[ \lambda_X S_X^2 + \lambda_Y S_Y^2 ] \\
              & = \lambda_X \Expect[S_X^2] + \lambda_Y \Expect[S_Y^2] \\
             & = (\lambda_X + \lambda_Y) \sigma^2 \\
             & = \sigma^2
\end{align*}



## Estimating the variance {.smaller}


We choose coefficients $\lambda_X$ and $\lambda_Y$ which reflect sample sizes
$$
\lambda_X := \frac{n - 1}{n + m - 2} \qquad 
\qquad
\lambda_Y := \frac{m - 1}{n + m - 2}
$$

**Notes**: 

- We have $\lambda_X + \lambda_Y = 1$

- Denominators in $\lambda_X$ and $\lambda_Y$ are degrees of freedom 
$${\rm df } = n + m - 2$$

- This choice is made so that $S^2$ has chi-squared distribution (more on this later) 




## Pooled estimator of variance {.smaller}

::: Definition

The **pooled estimator** of $\sigma^2$ is defined as
$$
S_p^2 := \lambda_X S_X^2 + \lambda_Y S_Y^2 
      = \frac{(n-1) S_X^2 + (m-1) S_Y^2}{n + m - 2}
$$

:::


**Note**: 

- $n=m$ implies $\lambda_X = \lambda_Y$
- In this case $S_X^2$ and $S_Y^2$ have same weight in $S_p^2$




## The two-sample t-statistic {.smaller}

- The t-statistic has currently the form
$$
T = \frac{\overline{X} - \overline{Y} - (\mu_X - \mu_Y)}{\sigma \ \sqrt{\dfrac{1}{n}+\dfrac{1}{m}}}
$$

- We replace $\sigma$ with the pooled estimator $S_p$


## The two-sample t-statistic {.smaller}

::: Definition

The two sample t-statistic is defined as
$$
T := \frac{\overline{X} - \overline{Y} - (\mu_X - \mu_Y)}{ S_p \ \sqrt{\dfrac{1}{n}+\dfrac{1}{m}}}  
$$
:::

**Note**: Under the Null Hypothesis that $\mu_X = \mu_Y$ this becomes
$$
T = \frac{\overline{X} - \overline{Y}}{ S_p \ \sqrt{\dfrac{1}{n}+\dfrac{1}{m}}} 
= \frac{\overline{X} - \overline{Y}}{ \sqrt{ \dfrac{ (n-1) S_X^2 + (m-1) S_Y^2 }{n + m - 2} } \ \sqrt{\dfrac{1}{n}+\dfrac{1}{m}}} 
$$



## Distribution of two-sample t-statistic {.smaller}

::: Theorem 

The two sample t-statistic has $t_{n+m-2}$ distribution
$$
T := \frac{\overline{X} - \overline{Y} - (\mu_X - \mu_Y)}{ S_p \ \sqrt{\dfrac{1}{n}+\dfrac{1}{m}}}  \sim t_{n + m - 2}
$$

:::



## Distribution of two-sample t-statistic {.smaller}
### Proof

- We have already seen that $\overline{X} - \overline{Y}$ is normal with
$$
\Expect[\overline{X} - \overline{Y}] = \mu_X - \mu_Y 
\qquad \qquad
\Var[\overline{X} - \overline{Y}] = \sigma^2 \left( \frac{1}{n} + \frac{1}{m} \right)
$$

- Therefore we can rescale $\overline{X} - \overline{Y}$ to get
$$
U := \frac{\overline{X} - \overline{Y} - (\mu_X - \mu_Y)}{ \sigma \sqrt{ \dfrac{1}{n} + \dfrac{1}{m}}} \sim N(0,1)
$$



## Distribution of two-sample t-statistic {.smaller}
### Proof


- We are assuming $X_1, \ldots, X_n$ iid $N(\mu_X,\sigma^2)$

- Therefore, as already shown, we have 
$$
\frac{ (n-1) S_X^2 }{ \sigma^2 } \sim \chi_{n-1}^2
$$

- Similarly, since $Y_1, \ldots, Y_m$ iid $N(\mu_Y,\sigma^2)$, we get
$$
\frac{ (m-1) S_Y^2 }{ \sigma^2 } \sim \chi_{m-1}^2
$$




## Distribution of two-sample t-statistic {.smaller}
### Proof


- Since $X_i$ and $Y_j$ are independent, we also have that
$$
\frac{ (n-1) S_X^2 }{ \sigma^2 }   \quad \text{ and } \quad 
\frac{ (m-1) S_Y^2 }{ \sigma^2 } \quad \text{ are independent}
$$


- In particular we obtain
$$
\frac{ (n-1) S_X^2 }{ \sigma^2 }  + \frac{ (m-1) S_Y^2 }{ \sigma^2 } \sim \chi_{n-1}^2 + \chi_{m-1}^2 \sim \chi_{m + n- 2}^2
$$



## Distribution of two-sample t-statistic {.smaller}
### Proof

- Recall the definition of $S_p^2$
$$
S_p^2 = \frac{(n-1) S_X^2 + (m-1) S_Y^2}{ n + m - 2 }
$$

- Therefore
$$
V := \frac{ (n+m-2) S_p^2 }{ \sigma^2 } 
= \frac{ (n - 1) S_X^2}{ \sigma^2}  + \frac{ (m-1) S_Y^2 }{ \sigma^2 } \sim \chi_{n + m - 2}^2 
$$



## Distribution of two-sample t-statistic {.smaller}
### Proof

- Rewrite $T$ as
\begin{align*}
T & = \frac{\overline{X} - \overline{Y} - (\mu_X - \mu_Y)}{ S_p \ \sqrt{\dfrac{1}{n}+\dfrac{1}{m}}} \\
  & =  \frac{\overline{X} - \overline{Y} - (\mu_X - \mu_Y)}{ \sigma \sqrt{ \dfrac{1}{n} + \dfrac{1}{m} }   } 
  \Bigg/  
  \sqrt{ 
    \frac{ (n + m - 2) S_p^2 \big/  \sigma^2}{ (n+ m - 2) } 
    } \\
  & = \frac{U}{\sqrt{V/(n+m-2)}}
\end{align*}



## Distribution of two-sample t-statistic {.smaller}
### Proof

- By construction $\overline{X}- \overline{Y}$ is independent of $S_X^2$ and $S_Y^2$

- Therefore $\overline{X}- \overline{Y}$ is independent of $S_p^2$

- We conclude that $U$ and $V$ are independent

- In conclusion, we have shown that
$$
T = \frac{U}{\sqrt{V/(n+m-2)}} \,, \qquad 
U \sim N(0,1) \,, \qquad V \sim \chi_{n + m - 2}^2
$$

- By the Theorem in Slide 118 of Lecture 2, we conclude that
$$
T \sim t_{n+m-2}
$$




## The two-sample t-test {.smaller}
### Procedure


Suppose given two independent samples

- Sample $x_1, \ldots, x_n$ from $N(\mu_X,\sigma^2)$ of size $n$
- Sample $y_1, \ldots, y_m$ from $N(\mu_Y,\sigma^2)$ of size $m$


The two-sided hypothesis test for **difference in means** is
$$
H_0 \colon \mu_X = \mu_Y \quad \qquad 
H_1 \colon \mu_X \neq \mu_Y
$$

The two-sample t-test consists of 3 steps




## The two-sample t-test {.smaller}
### Procedure

1. **Calculation**: Compute the two-sample t-statistic
$$
t = \frac{ \overline{x} - \overline{y}}{ s_p \ \sqrt{ \dfrac{1}{n} + \dfrac{1}{m} }}
$$
where sample means and pooled variance estimator are
$$
\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i  \qquad 
\overline{y} = \frac{1}{m} \sum_{i=1}^m y_i  \qquad
s_p^2 = \frac{ (n-1) s_X^2 + (m - 1) s_Y^2 }{ m + n - 2} 
$$
$$
s_X^2 = \frac{\sum_{i=1}^n x_i^2 - n \overline{x}^2}{n-1}
\qquad 
s_Y^2 = \frac{\sum_{i=1}^m y_i^2 - m \overline{y}^2}{m-1}
$$



## The two-sample t-test {.smaller}
### Procedure

2. **Statistical Tables or R**: Find either
    * Critical value in [Table 13.1](files/Statistics_Tables.pdf)
    $$
    t_{n + m - 2} (0.025)
    $$
    * p-value in R
    $$
    p := 2 P( t_{n + m -2} > |t| )
    $$




## The two-sample t-test {.smaller}
### Procedure

3. **Interpretation**:
    * Reject $H_0$ if 
    $$
    |t| > t_{n + m - 2} (0.025)  \qquad \text{ or } \qquad p < 0.05
    $$
    * Do not reject $H_0$ if 
    $$
    |t| \leq t_{n + m - 2} (0.025)  \qquad \text{ or } \qquad 
    p \geq 0.05
    $$





## The two-sample t-test in R {.smaller}
### General commands

1. Store the samples $x_1,\ldots,x_n$ and $y_1,\ldots,y_m$ in two R vectors
    * ``x_sample <- c(x1, ..., xn)``
    * ``y_sample <- c(y1, ..., ym)``

2. Perform a two-sided two-sample t-test on ``x_sample`` and ``y_sample``
    * ``t.test(x_sample, y_sample, var.equal = TRUE)``

3. Read output
    * Output is similar to one-sample t-test
    * The main quantity of interest is p-value




## Comments on command ``t.test(x, y)`` {.smaller}

1. R will perform a two-sample t-test on populations ``x`` and ``y``
2. R implicitly assumes the null hypothesis is 
$$
H_0 \colon \mu_X - \mu_Y = 0
$$

3. ``mu = mu0`` tells R to test null hypothesis:
    $$
    H_0 \colon \mu_X - \mu_Y = \mu_0
    $$ 


## Comments on command ``t.test(x, y)`` {.smaller}

4. One-sided two sample t-test can be performed by specifying
    * ``alternative = "greater"`` which tests 
    $$
    H_1 \colon \mu_X - \mu_Y > \mu_0
    $$ 
    * ``alternative = "smaller"`` which tests 
    $$
    H_1 \colon \mu_X - \mu_Y < \mu_0
    $$ 



## Comments on command ``t.test(x, y)`` {.smaller}

5. ``var.equal = TRUE`` tells R to assume that populations have same variance
$$
\sigma_X^2 = \sigma^2_Y
$$

6. In this case R computes the t-statistic with formula discussed earlier
    $$
    t = \frac{ \overline{x} - \overline{y} }{s_p \sqrt{ \dfrac{1}{n} + \dfrac{1}{m} }}
    $$





## Comments on command ``t.test(x, y)`` {.smaller}

**Warning**: If ``var.equal = TRUE`` is **not** specified then

- R assumes that populations have different variance
    $\sigma_X^2 \neq \sigma^2_Y$
- In this case the t-statistic 
  $$
    t = \frac{ \overline{x} - \overline{y} }{s_p \sqrt{ \dfrac{1}{n} + \dfrac{1}{m} }}
  $$
is **NOT** t-distributed

- R performs the **Welch t-test** instead of the **classic t-test** 



## Comments on command ``t.test(x, y)`` {.smaller}

- **Welch t-test** consists in computing the **Welch statistic**
$$
w = \frac{\overline{x} - \overline{y}}{ \sqrt{ \dfrac{s_X^2}{n} + \dfrac{s_Y^2}{m} } } 
$$
- If sample sizes $m,n > 5$, then $w$ is **approximately t-distributed**
    * Degrees of freedom are not integer, and depend on $S_X, S_Y, n, m$


**Bottom line:** 

- p-value from Welch t-test is **similar** to p-value from two-sample t-test
- Since p-values are similar, most times the 2 tests yield **same decision**







# Part 6: <br>Two-sample t-test<br>Example {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Two-sample t-test {.smaller}
### Example


- **Samples:** Wage data on 10 Mathematicians and 13 Accountants

- **Assumptions:** Wages are independent and normally distributed

- **Goal**: Compare mean wage for the 2 professions
    * Is there evidence of differences in average pay?


| Mathematicians |$x_1$|$x_2$|$x_3$|$x_4$|$x_5$|$x_6$|$x_7$|$x_8$|$x_9$|$x_{10}$|
|:-----------    |:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:------:|
|     Wages      | 36  |  40 | 46  | 54  |  57 | 58  |  59 |  60 |  62 |   63   |


| Accountants |$y_1$|$y_2$|$y_3$|$y_4$|$y_5$|$y_6$|$y_7$|$y_8$|$y_9$|$y_{10}$|$y_{11}$|$y_{12}$|$y_{13}$|
|:----------- |:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:------:|:------:|:------:|:------:|
|  Wages      | 37  | 37  | 42  |  44 |  46 |  48 |  54 |  56 |  59 | 60     |     60 |   64   |    64  |




## Calculations: First sample {.smaller}

- **Sample size:** $\ n =$ No. of Mathematicians $= 10$
- **Mean**:
$$
\bar{x} = \frac{\sum_{i=1}^n x_i}{n} 
        = \frac{36+40+46+ \ldots +62+63}{10}=\frac{535}{10}=53.5
$$

- **Variance:**
\begin{align*}
  s^2_X & = \frac{\sum_{i=1}^n x_i^2 - n \bar{x}^2}{n -1 } \\
  \sum_{i=1}^n x_i^2 & =  36^2+40^2+46^2+ \ldots +62^2+63^2 = 29435 \\
  s^2_X & =  \frac{29435-10(53.5)^2}{9} = 90.2778
\end{align*}




## Calculations: Second sample {.smaller}

- **Sample size:** $\ m =$ No. of Accountants $= 13$
- **Mean**:
$$
\bar{y} = \frac{37+37+42+ \dots +64+64}{13}
        = \frac{671}{13} = 51.6154
$$

- **Variance:**
\begin{align*}
  s^2_Y & = \frac{\sum_{i=1}^m y_i^2 - m \bar{y}^2}{m - 1} \\
  \sum_{i=1}^m y_i^2 & = 37^2+37^2+42^2+ \ldots +64^2+64^2 = 35783  \\
  s^2_Y & =  \frac{35783-13(51.6154)^2}{12} = 95.7547
\end{align*}




## Calculations: Pooled Variance {.smaller}

- **Pooled variance:**
\begin{align*}
s_p^2 & = \frac{(n-1) s_X^2 + (m-1) s_Y^2}{ n + m - 2}  \\
      & = \frac{(9) 90.2778 + (12) 95.7547 }{ 10 + 13 - 2} \\
      & = 93.40746
\end{align*}


- **Pooled standard deviation:**
$$
s_p = \sqrt{93.40746} = 9.6648
$$



## Calculations: t-statistic {.smaller}

1. **Calculation:** Compute the two-sample t-statistic

\begin{align*}
t & = \frac{\bar{x} - \bar{y} }{s_p \ \sqrt{\dfrac{1}{n}+\dfrac{1}{m}}} \\
  & = \frac{53.5 - 51.6154}{9.6648 \times \sqrt{\dfrac{1}{10}+\dfrac{1}{13}}} \\
  & = \frac{1.8846}{9.6648{\times}0.4206} \\
  & = 0.464 \,\, (3\ \text{d.p.})
\end{align*}




## Completing the t-test {.smaller}


2. **Referencing Tables**:
    * Degrees of freedom are
    $${\rm df} = n + m - 2 = 10 + 13 - 2 = 21$$
    * Find corresponding critical value in [Table 13.1](files/Statistics_Tables.pdf)
$$
t_{21}(0.025) = 2.08
$$




## Completing the t-test {.smaller} 

3. **Interpretation:**
    * We have that 
      $$
      | t | = 0.464 < 2.08 = t_{21}(0.025) 
      $$

    * Therefore the p-value satisfies $p>0.05$
    * There is no evidence ($p>0.05$) in favor of $H_1$
    * Hence we accept that $\mu_X = \mu_Y$

4. **Conclusion:** Average pay levels seem to be the same for both professions





## The two-sample t-test in R: Code {.smaller}


```r
# Enter Wages data in 2 vectors using function c()

mathematicians <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)
accountants <- c(37, 37, 42, 44, 46, 48, 54, 56, 59, 60, 60, 64, 64)


# Perform two-sample t-test with null hypothesis mu_X = mu_Y
# Specify that populations have same variance
# Store result of t.test in answer

answer <- t.test(mathematicians, accountants, var.equal = TRUE)


# Print answer
print(answer)
```

- Code can be downloaded here [two_sample_t_test.R](codes/two_sample_t_test.R)




## {.smaller}

```{r}
# Enter Wages data in 2 vectors using function c()

mathematicians <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)
accountants <- c(37, 37, 42, 44, 46, 48, 54, 56, 59, 60, 60, 64, 64)


# Perform two-sample t-test with null hypothesis mu_X = mu_Y
# Specify that populations have same variance
# Store result of t.test in answer

answer <- t.test(mathematicians, accountants, var.equal = TRUE)


# Print answer
print(answer)
```


**Comments on output**: 

1. First line: R tells us that a Two-Sample t-test is performed
2. Second line: Data for t-test is ``mathematicians`` and ``accountants``




## {.smaller}

```{r}
# Enter Wages data in 2 vectors using function c()

mathematicians <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)
accountants <- c(37, 37, 42, 44, 46, 48, 54, 56, 59, 60, 60, 64, 64)


# Perform two-sample t-test with null hypothesis mu_X = mu_Y
# Specify that populations have same variance
# Store result of t.test in answer

answer <- t.test(mathematicians, accountants, var.equal = TRUE)


# Print answer
print(answer)
```


**Comments on output**: 

3. Third line:
    * The t-statistic computed is $t = 0.46359$
    * **Note**: This coincides with the one computed by hand!
    * There are $21$ degrees of freedom
    * The p-values is $p = 0.6477$




##  {.smaller}

```{r}
# Enter Wages data in 2 vectors using function c()

mathematicians <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)
accountants <- c(37, 37, 42, 44, 46, 48, 54, 56, 59, 60, 60, 64, 64)


# Perform two-sample t-test with null hypothesis mu_X = mu_Y
# Specify that populations have same variance
# Store result of t.test in answer

answer <- t.test(mathematicians, accountants, var.equal = TRUE)


# Print answer
print(answer)
```


**Comments on output**: 

4. Fourth line: The alternative hypothesis is that the difference in means is not zero
    * This translates to $H_1 \colon \mu_X \neq \mu_Y$
    * **Warning**: This is not saying to reject $H_0$ -- R is just stating $H_1$





##  {.smaller}

```{r}
# Enter Wages data in 2 vectors using function c()

mathematicians <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)
accountants <- c(37, 37, 42, 44, 46, 48, 54, 56, 59, 60, 60, 64, 64)


# Perform two-sample t-test with null hypothesis mu_X = mu_Y
# Specify that populations have same variance
# Store result of t.test in answer

answer <- t.test(mathematicians, accountants, var.equal = TRUE)


# Print answer
print(answer)
```


**Comments on output**: 

5. Fifth line: R computes a $95 \%$ confidence interval for $\mu_X - \mu_Y$ 
$$
(\mu_X - \mu_Y) \in [-6.569496, 10.338727]
$$
    * **Interpretation:** If you repeat the experiment (on new data) over and over, the interval $[a,b]$ will contain $\mu_X - \mu_Y$ about $95\%$ of the times





##  {.smaller}

```{r}
# Enter Wages data in 2 vectors using function c()

mathematicians <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)
accountants <- c(37, 37, 42, 44, 46, 48, 54, 56, 59, 60, 60, 64, 64)


# Perform two-sample t-test with null hypothesis mu_X = mu_Y
# Specify that populations have same variance
# Store result of t.test in answer

answer <- t.test(mathematicians, accountants, var.equal = TRUE)


# Print answer
print(answer)
```


**Comments on output**: 

6. Seventh line: R computes sample mean for the two populations
    * Sample mean for ``mathematicians`` is $53.5$
    * Sample mean for ``accountants`` is $51.61538$





##  {.smaller}

```{r}
# Enter Wages data in 2 vectors using function c()

mathematicians <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)
accountants <- c(37, 37, 42, 44, 46, 48, 54, 56, 59, 60, 60, 64, 64)


# Perform two-sample t-test with null hypothesis mu_X = mu_Y
# Specify that populations have same variance
# Store result of t.test in answer

answer <- t.test(mathematicians, accountants, var.equal = TRUE)


# Print answer
print(answer)
```


**Conclusion**: The p-value is $p = 0.6477$

- Since $p > 0.05$ we do not reject $H_0$
- Hence $\mu_X$ and $\mu_Y$ appear to be similar
- Average pay levels seem to be the same for both professions





## t-test without assuming same variance {.smaller}

- In the previous t-tests we assumed the populations have same variance
$$
\sigma_X^2 = \sigma_Y^2
$$

- R can also perform t-test without assuming $\sigma_X^2 = \sigma_Y^2$
    * This is called **Welch Two-sample t-test**
    * It can be performed with ``t.test(x, y)``
    * Note that we are omitting the option ``var.equal = TRUE``
    * Equivalently, you may specify ``var.equal = FALSE``


## t-test without assuming same variance {.smaller}
### Warnings 

- The **Welch t-statistic** and **classic t-statistic** are different:
$$
w = \frac{\overline{x} - \overline{y}}{ \sqrt{ \dfrac{s_X^2}{n} + \dfrac{s_Y^2}{m} } }  \qquad \qquad 
t = \frac{ \overline{x} - \overline{y} }{s_p \sqrt{ \dfrac{1}{n} + \dfrac{1}{m} }}
$$

- The **Welch t-statistic** does **not** follow a t-distribution

- However, the p-value obtained with **Welch t-test** is often comparable to the one obtained with **classic t-test**

- This means the two tests will often give the same decision





## Welch two-sample t-test: Code {.smaller}

```r
# Enter Wages data in 2 vectors using function c()

mathematicians <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)
accountants <- c(37, 37, 42, 44, 46, 48, 54, 56, 59, 60, 60, 64, 64)


# Perform Welch two-sample t-test with null hypothesis mu_X = mu_Y
# Store result of t.test in answer

answer <- t.test(mathematicians, accountants)


# Print answer
print(answer)
```

- **Note**: 
    * This is almost the same code as in Slide 87
    * Only difference: we are omitting the option ``var.equal = TRUE`` in ``t.test``





## {.smaller}

```{r}
# Enter Wages data in 2 vectors using function c()

mathematicians <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)
accountants <- c(37, 37, 42, 44, 46, 48, 54, 56, 59, 60, 60, 64, 64)


# Perform Welch two-sample t-test with null hypothesis mu_X = mu_Y
# Store result of t.test in answer

answer <- t.test(mathematicians, accountants)


# Print answer
print(answer)
```

**Comments on output**: 

1. First line: R tells us that a Welch Two-Sample t-test is performed
    * The rest of the output is similar to classic t-test
    * Main difference is that p-value and t-statistic differ from classic t-test




## {.smaller}

```{r}
# Enter Wages data in 2 vectors using function c()

mathematicians <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)
accountants <- c(37, 37, 42, 44, 46, 48, 54, 56, 59, 60, 60, 64, 64)


# Perform Welch two-sample t-test with null hypothesis mu_X = mu_Y
# Store result of t.test in answer

answer <- t.test(mathematicians, accountants)


# Print answer
print(answer)
```

**Comments on output**: 

2. Third line: 
    * The Welch t-statistic is $w = 0.46546$ (standard t-test gave $t = 0.46359$)
    * Degrees of freedom are fractionary $\rm{df} = 19.795$ (standard t-test $\rm{df} = 21$) 
    * The Welch t-statistic is approximately t-distributed with $W \approx t_{19.795}$

3. Fifth line: The confidence interval for $\mu_X - \mu_Y$ is also different




##  {.smaller}

```{r}
# Enter Wages data in 2 vectors using function c()

mathematicians <- c(36, 40, 46, 54, 57, 58, 59, 60, 62, 63)
accountants <- c(37, 37, 42, 44, 46, 48, 54, 56, 59, 60, 60, 64, 64)


# Perform Welch two-sample t-test with null hypothesis mu_X = mu_Y
# Store result of t.test in answer

answer <- t.test(mathematicians, accountants)


# Print answer
print(answer)
```

**Conclusion**: The p-values obtained with the 2 tests are almost the same

- Welch t-test: p-value $= 0.6467$
- Classic t-test: p-value $= 0.6477$
- Both test: $p > 0.05$, and therefore do not reject $H_0$




